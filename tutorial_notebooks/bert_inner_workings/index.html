


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="George's Website along with a Collection of Useful Snippets for Python Machine Learning">
      
      
        <link rel="canonical" href="https://github.com/gmihaila/gmihaila.github.io/tutorial_notebooks/bert_inner_workings/">
      
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-5.1.1">
    
    
      
        <title>Bert Inner Workings - George Mihaila</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.a676eddb.min.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/palette.b302131d.min.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
      <link rel="manifest" href="../../manifest.webmanifest" crossorigin="use-credentials">
    
    
      <link rel="stylesheet" href="../../assets/pymdownx-extras/extra.css">
    
    
      
    
    
  </head>
  
  
    
    
    <body dir="ltr" data-md-color-primary="black" data-md-color-accent="black">
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#-bert-inner-workings" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="https://github.com/gmihaila/gmihaila.github.io" title="George Mihaila" class="md-header-nav__button md-logo" aria-label="George Mihaila">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,2L14,6.5V17.5L19,13V2M6.5,5C4.55,5 2.45,5.4 1,6.5V21.16C1,21.41 1.25,21.66 1.5,21.66C1.6,21.66 1.65,21.59 1.75,21.59C3.1,20.94 5.05,20.5 6.5,20.5C8.45,20.5 10.55,20.9 12,22C13.35,21.15 15.8,20.5 17.5,20.5C19.15,20.5 20.85,20.81 22.25,21.56C22.35,21.61 22.4,21.59 22.5,21.59C22.75,21.59 23,21.34 23,21.09V6.5C22.4,6.05 21.75,5.75 21,5.5V7.5L21,13V19C19.9,18.65 18.7,18.5 17.5,18.5C15.8,18.5 13.35,19.15 12,20V13L12,8.5V6.5C10.55,5.4 8.45,5 6.5,5V5Z" /></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,6H21V8H3V6M3,11H21V13H3V11M3,16H21V18H3V16Z" /></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            George Mihaila
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Bert Inner Workings
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,6.41L17.59,5L12,10.59L6.41,5L5,6.41L10.59,12L5,17.59L6.41,19L12,13.41L17.59,19L19,17.59L13.41,12L19,6.41Z" /></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/gmihaila/machine_learning_things/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Machine Learning Things
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="https://github.com/gmihaila/gmihaila.github.io" title="George Mihaila" class="md-nav__button md-logo" aria-label="George Mihaila">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,2L14,6.5V17.5L19,13V2M6.5,5C4.55,5 2.45,5.4 1,6.5V21.16C1,21.41 1.25,21.66 1.5,21.66C1.6,21.66 1.65,21.59 1.75,21.59C3.1,20.94 5.05,20.5 6.5,20.5C8.45,20.5 10.55,20.9 12,22C13.35,21.15 15.8,20.5 17.5,20.5C19.15,20.5 20.85,20.81 22.25,21.56C22.35,21.61 22.4,21.59 22.5,21.59C22.75,21.59 23,21.34 23,21.09V6.5C22.4,6.05 21.75,5.75 21,5.5V7.5L21,13V19C19.9,18.65 18.7,18.5 17.5,18.5C15.8,18.5 13.35,19.15 12,20V13L12,8.5V6.5C10.55,5.4 8.45,5 6.5,5V5Z" /></svg>

    </a>
    George Mihaila
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/gmihaila/machine_learning_things/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Machine Learning Things
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="About" class="md-nav__link">
      About
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../activities/activities/" title="Activities" class="md-nav__link">
      Activities
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../useful/useful/" title="Useful Code" class="md-nav__link">
      Useful Code
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      Tutorial Notebooks
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Tutorial Notebooks" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        Tutorial Notebooks
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Bert Inner Workings
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,9H17V7H3V9M3,13H17V11H3V13M3,17H17V15H3V17M19,17H21V15H19V17M19,7V9H21V7H19M19,13H21V11H19V13Z" /></svg>
        </span>
      </label>
    
    <a href="./" title="Bert Inner Workings" class="md-nav__link md-nav__link--active">
      Bert Inner Workings
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lets-look-at-how-an-input-flows-through-bert" class="md-nav__link">
    Let's look at how an input flows through Bert.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-should-i-know-for-this-notebook" class="md-nav__link">
    What should I know for this notebook?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-deep-are-we-going" class="md-nav__link">
    How deep are we going?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tutorial-structure" class="md-nav__link">
    Tutorial Structure
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#terminology" class="md-nav__link">
    Terminology
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-to-use-this-notebook" class="md-nav__link">
    How to use this notebook?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dataset" class="md-nav__link">
    Dataset
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coding" class="md-nav__link">
    Coding
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installs" class="md-nav__link">
    Installs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imports" class="md-nav__link">
    Imports
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#define-input" class="md-nav__link">
    Define Input
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bert-tokenizer" class="md-nav__link">
    Bert Tokenizer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bert-configuration" class="md-nav__link">
    Bert Configuration
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bert-for-sequence-classification" class="md-nav__link">
    Bert For Sequence Classification
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#class-call" class="md-nav__link">
    Class Call
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#class-components" class="md-nav__link">
    Class Components
  </a>
  
    <nav class="md-nav" aria-label="Class Components">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bertmodel" class="md-nav__link">
    BertModel
  </a>
  
    <nav class="md-nav" aria-label="BertModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bert-embeddings" class="md-nav__link">
    Bert Embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bert-encoder" class="md-nav__link">
    Bert Encoder
  </a>
  
    <nav class="md-nav" aria-label="Bert Encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bert-layer" class="md-nav__link">
    Bert Layer
  </a>
  
    <nav class="md-nav" aria-label="Bert Layer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bert-attention" class="md-nav__link">
    Bert Attention
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-bertselfattention" class="md-nav__link">
    # BertSelfAttention
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-bertselfoutput" class="md-nav__link">
    # BertSelfOutput
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-assemble-bertattention" class="md-nav__link">
    # Assemble BertAttention
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bertintermediate" class="md-nav__link">
    BertIntermediate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bertoutput" class="md-nav__link">
    BertOutput
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#assemble-bertlayer" class="md-nav__link">
    Assemble BertLayer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#assemble-bertencoder" class="md-nav__link">
    Assemble BertEncoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bertpooler" class="md-nav__link">
    BertPooler
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#assemble-bertmodel" class="md-nav__link">
    Assemble BertModel
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#assemble-components" class="md-nav__link">
    Assemble Components
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#complete-diagram" class="md-nav__link">
    Complete Diagram
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-note" class="md-nav__link">
    Final Note
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contact-" class="md-nav__link">
    Contact 🎣
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pytorchtext_bucketiterator/" title="PyTorchText BucketIterator" class="md-nav__link">
      PyTorchText BucketIterator
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../pretrain_transformers_pytorch/" title="Pretrain Transformers" class="md-nav__link">
      Pretrain Transformers
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../finetune_transformers_pytorch/" title="Finetune Transformers" class="md-nav__link">
      Finetune Transformers
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../gpt2_finetune_classification/" title="GPT2 Finetune Classification" class="md-nav__link">
      GPT2 Finetune Classification
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../resume/resume/" title="Resume" class="md-nav__link">
      Resume
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lets-look-at-how-an-input-flows-through-bert" class="md-nav__link">
    Let's look at how an input flows through Bert.
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-should-i-know-for-this-notebook" class="md-nav__link">
    What should I know for this notebook?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-deep-are-we-going" class="md-nav__link">
    How deep are we going?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tutorial-structure" class="md-nav__link">
    Tutorial Structure
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#terminology" class="md-nav__link">
    Terminology
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-to-use-this-notebook" class="md-nav__link">
    How to use this notebook?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dataset" class="md-nav__link">
    Dataset
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coding" class="md-nav__link">
    Coding
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installs" class="md-nav__link">
    Installs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imports" class="md-nav__link">
    Imports
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#define-input" class="md-nav__link">
    Define Input
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bert-tokenizer" class="md-nav__link">
    Bert Tokenizer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bert-configuration" class="md-nav__link">
    Bert Configuration
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bert-for-sequence-classification" class="md-nav__link">
    Bert For Sequence Classification
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#class-call" class="md-nav__link">
    Class Call
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#class-components" class="md-nav__link">
    Class Components
  </a>
  
    <nav class="md-nav" aria-label="Class Components">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bertmodel" class="md-nav__link">
    BertModel
  </a>
  
    <nav class="md-nav" aria-label="BertModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bert-embeddings" class="md-nav__link">
    Bert Embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bert-encoder" class="md-nav__link">
    Bert Encoder
  </a>
  
    <nav class="md-nav" aria-label="Bert Encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bert-layer" class="md-nav__link">
    Bert Layer
  </a>
  
    <nav class="md-nav" aria-label="Bert Layer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bert-attention" class="md-nav__link">
    Bert Attention
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-bertselfattention" class="md-nav__link">
    # BertSelfAttention
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-bertselfoutput" class="md-nav__link">
    # BertSelfOutput
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#-assemble-bertattention" class="md-nav__link">
    # Assemble BertAttention
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bertintermediate" class="md-nav__link">
    BertIntermediate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bertoutput" class="md-nav__link">
    BertOutput
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#assemble-bertlayer" class="md-nav__link">
    Assemble BertLayer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#assemble-bertencoder" class="md-nav__link">
    Assemble BertEncoder
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bertpooler" class="md-nav__link">
    BertPooler
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#assemble-bertmodel" class="md-nav__link">
    Assemble BertModel
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#assemble-components" class="md-nav__link">
    Assemble Components
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#complete-diagram" class="md-nav__link">
    Complete Diagram
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-note" class="md-nav__link">
    Final Note
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contact-" class="md-nav__link">
    Contact 🎣
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/gmihaila/machine_learning_things/tree/master/docs/src/markdown/tutorial_notebooks/bert_inner_workings.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71,7.04C21.1,6.65 21.1,6 20.71,5.63L18.37,3.29C18,2.9 17.35,2.9 16.96,3.29L15.12,5.12L18.87,8.87M3,17.25V21H6.75L17.81,9.93L14.06,6.18L3,17.25Z" /></svg>
                  </a>
                
                
                  
                
                
                <h1 id="-bert-inner-workings"><strong>⚙️ Bert Inner Workings</strong><a class="headerlink" href="#-bert-inner-workings" title="Permanent link"></a></h1>
<h2 id="lets-look-at-how-an-input-flows-through-bert"><strong>Let's look at how an input flows through Bert.</strong><a class="headerlink" href="#lets-look-at-how-an-input-flows-through-bert" title="Permanent link"></a></h2>
<p><br></p>
<p><a href="https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/gpt2_finetune_classification.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> &nbsp;
<a href="https://github.com/gmihaila/ml_things/blob/master/notebooks/pytorch/gpt2_finetune_classification.ipynb"><img alt="Generic badge" src="https://img.shields.io/badge/GitHub-Source-greensvg" /></a>
<a href="https://gmihaila.medium.com/gpt2-for-text-classification-using-hugging-face-transformers-574555451832"><img alt="Generic badge" src="https://img.shields.io/badge/Article-Medium-black.svg" /></a>
<a href="https://opensource.org/licenses/Apache-2.0"><img alt="License" src="https://img.shields.io/badge/License-Apache%202.0-blue.svg" /></a></p>
<p><br></p>
<p><strong>Disclaimer:</strong> <em>The format of this tutorial notebook is very similar to my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format.</em></p>
<p><br></p>
<p><strong>Main idea:</strong></p>
<p>I created this notebook to better understand the inner workings of Bert. I followed a lot of tutorials to try to understand the architecture, but I was never able to really understand what was happening under the hood. For me it always helps to see the actual code instead of just simple abstract diagrams that a lot of times don't match the actual implementation. If you're like me than this tutorial will help!</p>
<p>I went as deep as you can go with Deep Learning - all the way to the tensor level. For me it helps to see the code and how the tensors move between layers. I feel like this level of abstraction is close enough to the core of the model to perfectly understand the inner workings.</p>
<p>I will use the implementation of Bert from one of the best NLP library out there - <a href="https://huggingface.co">HuggingFace</a> <a href="https://github.com/huggingface/transformers">Transformers</a>. More specifically, I will show the inner working of <a href="https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification">Bert For Sequence Classification</a>.</p>
<p>The term <strong>forward pass</strong> is used in Neural Networks and it refers to the calculations involved from the input sequence all the way to output of the last layer. It's basically the flow of data from input to output.</p>
<p>I will follow the code from an example input sequence all the way to the final output prediction.</p>
<p><br></p>
<h2 id="what-should-i-know-for-this-notebook"><strong>What should I know for this notebook?</strong><a class="headerlink" href="#what-should-i-know-for-this-notebook" title="Permanent link"></a></h2>
<p>Some prior knowledge of Bert is needed. I won't go into any details of how Bert works. For this there is plenty of information out there.</p>
<p>Since I am using the PyTorch implementation of Bert any knowledge on PyTorch is very useful.</p>
<p>Knowing a little bit about the <a href="https://github.com/huggingface/transformers">transformers</a> library helps too.</p>
<p><br> </p>
<h2 id="how-deep-are-we-going"><strong>How deep are we going?</strong><a class="headerlink" href="#how-deep-are-we-going" title="Permanent link"></a></h2>
<p>I think the best way to understand such a complex model as Bert is to see the actual layer components that are used. I will dig in the code until I see the actual PyTorch layers used <code>torch.nn</code>. In my opinion there is no need to go deeper than the <code>torch.nn</code> layers. </p>
<p><br></p>
<h2 id="tutorial-structure"><strong>Tutorial Structure</strong><a class="headerlink" href="#tutorial-structure" title="Permanent link"></a></h2>
<p>Each section contains multiple subsections. </p>
<p>The order of each section matches the order of the model's layers from input to output.</p>
<p>At the beginning of each section of code I created a diagram to illustrate the flow of tensors of that particular code.</p>
<p>I created the diagrams following the model's implementation. </p>
<p>The major section <strong>Bert For Sequence Classification</strong> starts with the <strong>Class Call</strong> that shows how we normally create the Bert model for sequence classification and perform a forward pass. <strong>Class Components</strong> contains the components of <code>BertForSequenceClassification</code> implementation.</p>
<p>At the end of each major section, I assemble all components from that section and show the output and diagram.</p>
<p>At the end of the notebook, I have all the code parts and diagrams assembled. </p>
<p><br></p>
<h2 id="terminology"><strong>Terminology</strong><a class="headerlink" href="#terminology" title="Permanent link"></a></h2>
<p>I will use regular deep learning terminology found in most Bert tutorials. I'm using some terms in a slightly different way:</p>
<ul>
<li>
<p><strong>Layer</strong> and <strong>layers</strong>: In this tutorial when I mention layer it can be an abstraction of a group of layers or just a single layer. When I reach <code>torch.nn</code> you know I refer to a single layer.</p>
</li>
<li>
<p><code>torch.nn</code>: I'm referring to any PyTorch layer module. This is the deepest I will go in this tutorial.</p>
</li>
</ul>
<p><br></p>
<h2 id="how-to-use-this-notebook"><strong>How to use this notebook?</strong><a class="headerlink" href="#how-to-use-this-notebook" title="Permanent link"></a></h2>
<p>The purpose of this notebook is purely educational. This notebook is to be used to align known information on how Bert woks with the code implementation of Bert. I used the Bert implementation from <a href="https://github.com/huggingface/transformers">Transformers</a>. My contribution is on arranging the code implementation and creating associated diagrams.</p>
<p><br></p>
<h2 id="dataset"><strong>Dataset</strong><a class="headerlink" href="#dataset" title="Permanent link"></a></h2>
<p>For simplicity I will only use two sentences as our data input: <code>I love cats!</code> and <code>He hates pineapple pizza.</code>. I'll pretend to do binary sentiment classification on these two sentences.</p>
<p><br></p>
<h2 id="coding"><strong>Coding</strong><a class="headerlink" href="#coding" title="Permanent link"></a></h2>
<p>Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output.</p>
<p>I made this format to be easy to follow if you decide to run each code cell in your own python notebook.</p>
<p>When I learn from a tutorial, I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.</p>
<p><br></p>
<h2 id="installs"><strong>Installs</strong><a class="headerlink" href="#installs" title="Permanent link"></a></h2>
<ul>
<li><strong><a href="https://github.com/huggingface/transformers">transformers</a></strong> library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub.</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1"># install the transformers library</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">transformers</span><span class="o">.</span><span class="n">git</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Installing build dependencies ... done
Getting requirements to build wheel ... done
Preparing wheel metadata ... done
 |████████████████████████████████| 2.9MB 6.7MB/s 
 |████████████████████████████████| 890kB 48.9MB/s 
 |████████████████████████████████| 1.1MB 49.0MB/s 
Building wheel for transformers (PEP 517) ... done
Building wheel for sacremoses (setup.py) ... done
 |████████████████████████████████| 71kB 5.2MB/s
</code></pre></div>

<h2 id="imports"><strong>Imports</strong><a class="headerlink" href="#imports" title="Permanent link"></a></h2>
<p>Import all needed libraries for this notebook.</p>
<p>Declare parameters used for this notebook:</p>
<ul>
<li><code>set_seed(123)</code> - Always good to set a fixed seed for reproducibility.</li>
<li><code>n_labels</code> - How many labels are we using in this dataset. This is used to decide size of classification head.</li>
<li><code>ACT2FN</code> - Dictionary for special activation functions used in Bert. We'll only need the <code>gelu</code> activation function.</li>
<li><code>BertLayerNorm</code> - Shortcut for calling the PyTorch normalization layer <code>torch.nn.LayerNorm</code>.</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers.activations</span> <span class="kn">import</span> <span class="n">gelu</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span><span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertConfig</span><span class="p">,</span> 
                          <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">BertPreTrainedModel</span><span class="p">,</span> 
                          <span class="n">apply_chunking_to_forward</span><span class="p">,</span> <span class="n">set_seed</span><span class="p">,</span>
                          <span class="p">)</span>
<span class="kn">from</span> <span class="nn">transformers.modeling_outputs</span> <span class="kn">import</span> <span class="p">(</span><span class="n">BaseModelOutputWithPastAndCrossAttentions</span><span class="p">,</span> 
                                           <span class="n">BaseModelOutputWithPoolingAndCrossAttentions</span><span class="p">,</span> 
                                           <span class="n">SequenceClassifierOutput</span><span class="p">,</span>
                                           <span class="p">)</span>


<span class="c1"># Set seed for reproducibility.</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># How many labels are we using in training.</span>
<span class="c1"># This is used to decide size of classification head.</span>
<span class="n">n_labels</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># GELU Activation function.</span>
<span class="n">ACT2FN</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gelu&quot;</span><span class="p">:</span> <span class="n">gelu</span><span class="p">}</span>

<span class="c1"># Define BertLayerNorm.</span>
<span class="n">BertLayerNorm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span>
</code></pre></div>

<h2 id="define-input"><strong>Define Input</strong><a class="headerlink" href="#define-input" title="Permanent link"></a></h2>
<p>Let's define some text data on which we will use Bert to classify as positive or negative.</p>
<p>We encoded our positive and negative sentiments into:
* 0 - for negative sentiments.
* 1 - for positive sentiments.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># array of text we want to classify</span>
<span class="n">input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;I love cats!&#39;</span><span class="p">,</span>
              <span class="s2">&quot;He hates pineapple pizza.&quot;</span><span class="p">]</span>

<span class="c1"># senitmen labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div>

<h2 id="bert-tokenizer"><strong>Bert Tokenizer</strong><a class="headerlink" href="#bert-tokenizer" title="Permanent link"></a></h2>
<p>Creating the <code>tokenizer</code> is pretty standard when using the Transformers library.</p>
<p>Using our newly created <code>tokenizer</code> we'll use it on our two sentence dataset and create the <code>input_sequence</code> that will be used as input for our Bert model.</p>
<details>
<summary>Show Bert Tokenizer Diagram</summary>

<figure>
  <img src="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings_tokenizer.png" />
</figure>

</details>

<div class="highlight"><pre><span></span><code><span class="c1"># create BertTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-cased&#39;</span><span class="p">)</span>

<span class="c1"># create input sequence using tokenizer</span>
<span class="n">input_sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">input_texts</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>

<span class="c1"># since input_sequence is a dictionary we can also add the labels to it</span>
<span class="c1"># want to make sure all values ar tensors</span>
<span class="n">input_sequences</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;labels&#39;</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">labels</span><span class="p">)})</span>

<span class="c1"># the tokenizer will return a dictionary of three: input_ids, attention_mask and token_type_ids</span>
<span class="c1"># let&#39;s do a pretty print</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;PRETTY PRINT OF `input_sequences` UPDATED WITH `labels`:&#39;</span><span class="p">)</span>
<span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> : </span><span class="si">%s</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">))</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">input_sequences</span><span class="o">.</span><span class="n">items</span><span class="p">()];</span>

<span class="c1"># lets see how the text looks like after Bert Tokenizer</span>
<span class="c1"># we see the special tokens added</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ORIGINAL TEXT:&#39;</span><span class="p">)</span>
<span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="n">example</span><span class="p">)</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">input_texts</span><span class="p">];</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">TEXT AFTER USING `BertTokenizer`:&#39;</span><span class="p">)</span>
<span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">example</span><span class="p">))</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">input_sequences</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()];</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Downloading: 100% |████████████████████████████████| 213k/213k [00:00&lt;00:00, 278kB/s]

PRETTY PRINT OF `input_sequences` UPDATED WITH `labels`:
input_ids : tensor([[  101,   146,  1567, 11771,   106,   102,     0,     0,     0],
        [  101,  1124, 18457, 10194, 11478,  7136, 13473,   119,   102]])

token_type_ids : tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0]])

attention_mask : tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1]])

labels : tensor([1, 0])

ORIGINAL TEXT:
I love cats!
He hates pineapple pizza.

TEXT AFTER USING `BertTokenizer`:
[CLS] I love cats! [SEP] [PAD] [PAD] [PAD]
[CLS] He hates pineapple pizza. [SEP]
</code></pre></div>

<h2 id="bert-configuration"><strong>Bert Configuration</strong><a class="headerlink" href="#bert-configuration" title="Permanent link"></a></h2>
<p>Predefined values specific to Bert architecture already defined for us by Hugging Face.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Create the bert configuration.</span>
<span class="n">bert_configuraiton</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-cased&#39;</span><span class="p">)</span>

<span class="c1"># Let&#39;s see number of layers.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;NUMBER OF LAYERS:&#39;</span><span class="p">,</span> <span class="n">bert_configuraiton</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>

<span class="c1"># We can also see the size of embeddings inside Bert.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;EMBEDDING SIZE:&#39;</span><span class="p">,</span> <span class="n">bert_configuraiton</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

<span class="c1"># See which activation function used in hidden layers.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACTIVATIONS:&#39;</span><span class="p">,</span> <span class="n">bert_configuraiton</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Downloading: 100% |████████████████████████████████| 433/433 [00:00&lt;00:00, 15.5kB/s]

NUMBER OF LAYERS: 12
EMBEDDING SIZE: 768
ACTIVATIONS: gelu
</code></pre></div>

<h2 id="bert-for-sequence-classification"><strong>Bert For Sequence Classification</strong><a class="headerlink" href="#bert-for-sequence-classification" title="Permanent link"></a></h2>
<p>I will go over the Bert for Sequence Classification model. This is a Bert language model with a classification layer on top.</p>
<p>If you plan on looking at other transformers models his tutorial will be very similar.</p>
<h2 id="class-call"><strong>Class Call</strong><a class="headerlink" href="#class-call" title="Permanent link"></a></h2>
<p>Let's start with doing a forward pass using the whole model call from Hugging Face Transformer.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Let&#39; start with the final model how we normally use.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-cased&#39;</span><span class="p">)</span>

<span class="c1"># Perform a forward pass. We only care about the output and no gradients.</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">**</span><span class="n">input_sequences</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>

<span class="c1"># Let&#39;s check how a forward pass output looks like.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;FORWARD PASS OUTPUT:&#39;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Downloading: 100% |████████████████████████████████| 436M/436M [00:07&lt;00:00, 61.3MB/s]

Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;]
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

FORWARD PASS OUTPUT: SequenceClassifierOutput(loss=tensor(0.7454), logits=tensor([[ 0.2661, -0.1774],
        [ 0.2223, -0.0847]]), hidden_states=None, attentions=None)
</code></pre></div>

<h2 id="class-components"><strong>Class Components</strong><a class="headerlink" href="#class-components" title="Permanent link"></a></h2>
<p>Now let's look at the code implementation and break down each part of the model and check the outputs.</p>
<p>Start with the <code>BertForSequenceClassification</code> found in <a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L1449">transformers/src/transformers/models/bert/modeling_bert.py#L1449</a>.</p>
<p>The <code>forward</code> pass uses the following layers:</p>
<ul>
<li><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L815">BertModel</a> layer: </li>
</ul>
<p><code>self.bert = BertModel(config)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout">torch.nn.Dropout</a> layer for dropout:</li>
</ul>
<p><code>self.dropout = nn.Dropout(config.hidden_dropout_prob)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear">torch.nn.Linear</a> layer used for classification: </li>
</ul>
<p><code>self.classifier = nn.Linear(config.hidden_size, config.num_labels)</code></p>
<h3 id="bertmodel"><strong>BertModel</strong><a class="headerlink" href="#bertmodel" title="Permanent link"></a></h3>
<p>This is the core Bert model that can be found at: <a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L815">transformers/src/transformers/models/bert/modeling_bert.py#L815</a>.</p>
<p>Hugging Face was nice enough to mention a small summary: <em>The bare Bert Model transformer outputting raw hidden-states without any specific head on top.</em></p>
<p>The <code>forward</code> pass uses the following layers:</p>
<ul>
<li><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L165">BertEmbeddings</a> layer: </li>
</ul>
<p><code>self.embeddings = BertEmbeddings(config)</code></p>
<ul>
<li><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L512">BertEncoder</a> layer:</li>
</ul>
<p><code>self.encoder = BertEncoder(config)</code></p>
<ul>
<li><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L601">BertPooler</a> layer:</li>
</ul>
<p><code>self.pooler = BertPooler(config)</code></p>
<h4 id="bert-embeddings"><strong>Bert Embeddings</strong><a class="headerlink" href="#bert-embeddings" title="Permanent link"></a></h4>
<p>This is where we feed the <code>input_sequences</code> created under <strong>Bert Tokenizer</strong> and get our first embeddings.</p>
<p>Implementation can be found at: <a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L165">transformers/src/transformers/models/bert/modeling_bert.py#L165</a>.</p>
<p>This layer contains actual PyTorch layers. I won't go into farther details since this is how far we need to go.</p>
<p>The <code>forward</code> pass uses following layers:</p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#embedding">torch.nn.Embedding</a> layer for word embeddings:</li>
</ul>
<p><code>self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#embedding">torch.nn.Embedding</a> layer for position embeddings:</li>
</ul>
<p><code>self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#embedding">torch.nn.Embedding</a> for token type embeddings:</li>
</ul>
<p><code>self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#layernorm">torch.nn.LayerNorm</a> layer for normalization:</li>
</ul>
<p><code>self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout">torch.nn.Dropout</a> layer for dropout:</li>
</ul>
<p><code>self.dropout = nn.Dropout(config.hidden_dropout_prob)</code></p>
<details>

<summary>Show Bert Embeddings Diagram</summary>

<figure>
  <img src="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings_embeddings.png"/>  
</figure>

</details>

<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BertEmbeddings</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_type_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">type_vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>

        <span class="c1"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span>
        <span class="c1"># any TensorFlow checkpoint file</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>

        <span class="c1"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;position_ids&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding_type</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;position_embedding_type&quot;</span><span class="p">,</span> <span class="s2">&quot;absolute&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values_length</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_ids</span><span class="p">[:,</span> <span class="n">past_key_values_length</span> <span class="p">:</span> <span class="n">seq_length</span> <span class="o">+</span> <span class="n">past_key_values_length</span><span class="p">]</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Created Tokens Positions IDs:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">)</span>
        

        <span class="k">if</span> <span class="n">token_type_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">position_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">token_type_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_type_embeddings</span><span class="p">(</span><span class="n">token_type_ids</span><span class="p">)</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Tokens IDs:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Tokens Type IDs:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Word Embeddings:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">inputs_embeds</span> <span class="o">+</span> <span class="n">token_type_embeddings</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding_type</span> <span class="o">==</span> <span class="s2">&quot;absolute&quot;</span><span class="p">:</span>
            <span class="n">position_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span><span class="p">(</span><span class="n">position_ids</span><span class="p">)</span>

            <span class="c1"># ADDED</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Position Embeddings:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">position_embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

            <span class="n">embeddings</span> <span class="o">+=</span> <span class="n">position_embeddings</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Token Types Embeddings:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">token_type_embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Sum Up All Embeddings:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Embeddings Layer Nromalization:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Embeddings Dropout Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">embeddings</span>


<span class="c1"># Create Bert embedding layer.</span>
<span class="n">bert_embeddings_block</span> <span class="o">=</span> <span class="n">BertEmbeddings</span><span class="p">(</span><span class="n">bert_configuraiton</span><span class="p">)</span>

<span class="c1"># Perform a forward pass.</span>
<span class="n">embedding_output</span> <span class="o">=</span> <span class="n">bert_embeddings_block</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_sequences</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">],</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">input_sequences</span><span class="p">[</span><span class="s1">&#39;token_type_ids&#39;</span><span class="p">])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Created Tokens Positions IDs:
 tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]])

Tokens IDs:
 torch.Size([2, 9])

Tokens Type IDs:
 torch.Size([2, 9])

Word Embeddings:
 torch.Size([2, 9, 768])

Position Embeddings:
 torch.Size([1, 9, 768])

Token Types Embeddings:
 torch.Size([2, 9, 768])

Sum Up All Embeddings:
 torch.Size([2, 9, 768])

Embeddings Layer Nromalization:
 torch.Size([2, 9, 768])

Embeddings Dropout Layer:
 torch.Size([2, 9, 768])
</code></pre></div>

<h4 id="bert-encoder"><strong>Bert Encoder</strong><a class="headerlink" href="#bert-encoder" title="Permanent link"></a></h4>
<p>This layer contains the core of the bert model where the self-attention happens. </p>
<p>The implementation can be found at: <a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L512">transformers/src/transformers/models/bert/modeling_bert.py#L512</a>.</p>
<p>The <code>forward</code> pass uses:</p>
<ul>
<li>12 of the <a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L429">BertLayer</a> layers ( in this setup <code>config.num_hidden_layers=12</code>):</li>
</ul>
<p><code>self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])</code></p>
<h5 id="bert-layer"><strong>Bert Layer</strong><a class="headerlink" href="#bert-layer" title="Permanent link"></a></h5>
<p>This layer contains basic components of the self-attention implementation.</p>
<p>Implementation can be found at <a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L429">transformers/src/transformers/models/bert/modeling_bert.py#L429</a>.</p>
<p>The <code>forward</code> pass uses:</p>
<ul>
<li><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L351">BertAttention</a> layer:</li>
</ul>
<p><code>self.attention = BertAttention(config)</code></p>
<ul>
<li><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L400">BertIntermediate</a> layer:</li>
</ul>
<p><code>self.intermediate = BertIntermediate(config)</code></p>
<ul>
<li><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L415">BertOutput</a> layer:</li>
</ul>
<p><code>self.output = BertOutput(config)</code></p>
<h6 id="bert-attention"><strong>Bert Attention</strong><a class="headerlink" href="#bert-attention" title="Permanent link"></a></h6>
<p>This layer contains basic components of the self-attention implementation.</p>
<p>Implementation can be found at <a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L351">transformers/src/transformers/models/bert/modeling_bert.py#L351</a>.</p>
<p>The <code>forward</code> pass uses:</p>
<ul>
<li><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L212">BertSelfAttention</a> layer:</li>
</ul>
<p><code>self.self = BertSelfAttention(config)</code></p>
<ul>
<li><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L337">BertSelfOutput</a> layer:</li>
</ul>
<p><code>self.output = BertSelfOutput(config)</code></p>
<h6 id="-bertselfattention"># <strong>BertSelfAttention</strong><a class="headerlink" href="#-bertselfattention" title="Permanent link"></a></h6>
<p>This layer contains the <code>torch.nn</code> basic components of the self-attention implementation.</p>
<p>Implementation can be found at <a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L212">transformers/src/transformers/models/bert/modeling_bert.py#L212</a>.</p>
<p>The <code>forward</code> pass uses:</p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear">torch.nn.Linear</a> used for the Query layer: </li>
</ul>
<p><code>self.query = nn.Linear(config.hidden_size, self.all_head_size)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear">torch.nn.Linear</a> used for the Key layer: </li>
</ul>
<p><code>self.key = nn.Linear(config.hidden_size, self.all_head_size)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear">torch.nn.Linear</a> used for the Value layer: </li>
</ul>
<p><code>self.value = nn.Linear(config.hidden_size, self.all_head_size)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout">torch.nn.Dropout</a> layer for dropout:</li>
</ul>
<p><code>self.dropout = nn.Dropout(config.attention_probs_dropout_prob)</code></p>
<details>
<summary>Show BertSelfAttention Diagram</summary>

<figure>
  <img src="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings_selfattention.png" />
</figure>

</details>

<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BertSelfAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">%</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;embedding_size&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The hidden size (</span><span class="si">%d</span><span class="s2">) is not a multiple of the number of attention &quot;</span>
                <span class="s2">&quot;heads (</span><span class="si">%d</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">/</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Attention Head Size:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Combined Attentions Head Size:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">attention_probs_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding_type</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;position_embedding_type&quot;</span><span class="p">,</span> <span class="s2">&quot;absolute&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding_type</span> <span class="o">==</span> <span class="s2">&quot;relative_key&quot;</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding_type</span> <span class="o">==</span> <span class="s2">&quot;relative_key_query&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">distance_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span>

    <span class="k">def</span> <span class="nf">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">new_x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">new_x_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Hidden States:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">mixed_query_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># If this is instantiated as a cross-attention module, the keys</span>
        <span class="c1"># and values come from an encoder; the attention mask needs to be</span>
        <span class="c1"># such that the encoder&#39;s padding tokens are not attended to.</span>
        <span class="n">is_cross_attention</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">is_cross_attention</span> <span class="ow">and</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

            <span class="c1"># ADDED</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Query Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">mixed_query_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Key Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Value Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

            <span class="c1"># reuse k,v, cross_attentions</span>
            <span class="n">key_layer</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">value_layer</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">encoder_attention_mask</span>
        <span class="k">elif</span> <span class="n">is_cross_attention</span><span class="p">:</span>

            <span class="c1"># ADDED</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Query Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">mixed_query_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Key Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Value Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

            <span class="n">key_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">))</span>
            <span class="n">value_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">))</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">encoder_attention_mask</span>
        <span class="k">elif</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

            <span class="c1"># ADDED</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Query Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">mixed_query_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Key Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Value Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

            <span class="n">key_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
            <span class="n">value_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
            <span class="n">key_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">key_layer</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">value_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_key_value</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">value_layer</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>

            <span class="c1"># ADDED</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Query Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">mixed_query_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Key Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Value Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

            <span class="n">key_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
            <span class="n">value_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>

        
        

        <span class="n">query_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_for_scores</span><span class="p">(</span><span class="n">mixed_query_layer</span><span class="p">)</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Query:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">query_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Key:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">key_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Value:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">value_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
            <span class="c1"># if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.</span>
            <span class="c1"># Further calls to cross_attention layer can then reuse all cross-attention</span>
            <span class="c1"># key/value_states (first &quot;if&quot; case)</span>
            <span class="c1"># if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of</span>
            <span class="c1"># all previous decoder key/value_states. Further calls to uni-directional self-attention</span>
            <span class="c1"># can concat previous decoder key/value_states to current projected key/value_states (third &quot;elif&quot; case)</span>
            <span class="c1"># if encoder bi-directional self-attention `past_key_value` is always `None`</span>
            <span class="n">past_key_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">key_layer</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">)</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Key Transposed:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">key_layer</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw attention scores.</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query_layer</span><span class="p">,</span> <span class="n">key_layer</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Attention Scores:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding_type</span> <span class="o">==</span> <span class="s2">&quot;relative_key&quot;</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding_type</span> <span class="o">==</span> <span class="s2">&quot;relative_key_query&quot;</span><span class="p">:</span>
            <span class="n">seq_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">position_ids_l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">position_ids_r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">distance</span> <span class="o">=</span> <span class="n">position_ids_l</span> <span class="o">-</span> <span class="n">position_ids_r</span>
            <span class="n">positional_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance_embedding</span><span class="p">(</span><span class="n">distance</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">positional_embedding</span> <span class="o">=</span> <span class="n">positional_embedding</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">query_layer</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># fp16 compatibility</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding_type</span> <span class="o">==</span> <span class="s2">&quot;relative_key&quot;</span><span class="p">:</span>
                <span class="n">relative_position_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhld,lrd-&gt;bhlr&quot;</span><span class="p">,</span> <span class="n">query_layer</span><span class="p">,</span> <span class="n">positional_embedding</span><span class="p">)</span>
                <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span> <span class="o">+</span> <span class="n">relative_position_scores</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding_type</span> <span class="o">==</span> <span class="s2">&quot;relative_key_query&quot;</span><span class="p">:</span>
                <span class="n">relative_position_scores_query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhld,lrd-&gt;bhlr&quot;</span><span class="p">,</span> <span class="n">query_layer</span><span class="p">,</span> <span class="n">positional_embedding</span><span class="p">)</span>
                <span class="n">relative_position_scores_key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhrd,lrd-&gt;bhlr&quot;</span><span class="p">,</span> <span class="n">key_layer</span><span class="p">,</span> <span class="n">positional_embedding</span><span class="p">)</span>
                <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span> <span class="o">+</span> <span class="n">relative_position_scores_query</span> <span class="o">+</span> <span class="n">relative_position_scores_key</span>

        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">)</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Attention Scores Divided by Scalar:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Apply the attention mask is (precomputed for all layers in BertModel forward() function)</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span> <span class="o">+</span> <span class="n">attention_mask</span>

        <span class="c1"># Normalize the attention scores to probabilities.</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span><span class="n">attention_scores</span><span class="p">)</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Attention Probabilities Softmax Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">attention_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># This is actually dropping out entire tokens to attend to, which might</span>
        <span class="c1"># seem a bit unusual, but is taken from the original Transformer paper.</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">)</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Attention Probabilities Dropout Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">attention_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># Mask heads if we want to</span>
        <span class="k">if</span> <span class="n">head_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">attention_probs</span> <span class="o">*</span> <span class="n">head_mask</span>

        <span class="n">context_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value_layer</span><span class="p">)</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Context:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">context_layer</span> <span class="o">=</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Context Permute:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">new_context_layer_shape</span> <span class="o">=</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_head_size</span><span class="p">,)</span>
        <span class="n">context_layer</span> <span class="o">=</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">new_context_layer_shape</span><span class="p">)</span>

        <span class="c1"># ADDED</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Context Reshaped:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">context_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">context_layer</span><span class="p">,</span> <span class="n">attention_probs</span><span class="p">)</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="p">(</span><span class="n">context_layer</span><span class="p">,)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">+</span> <span class="p">(</span><span class="n">past_key_value</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">outputs</span>

<span class="c1"># Create bert self attention layer.</span>
<span class="n">bert_selfattention_block</span> <span class="o">=</span> <span class="n">BertSelfAttention</span><span class="p">(</span><span class="n">bert_configuraiton</span><span class="p">)</span>

<span class="c1"># Perform a forward pass.</span>
<span class="n">context_embedding</span> <span class="o">=</span> <span class="n">bert_selfattention_block</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">embedding_output</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Attention Head Size:
 64

Combined Attentions Head Size:
 768

Hidden States:
 torch.Size([2, 9, 768])

Query Linear Layer:
 torch.Size([2, 9, 768])

Key Linear Layer:
 torch.Size([2, 9, 768])

Value Linear Layer:
 torch.Size([2, 9, 768])

Query:
 torch.Size([2, 12, 9, 64])

Key:
 torch.Size([2, 12, 9, 64])

Value:
 torch.Size([2, 12, 9, 64])

Key Transposed:
 torch.Size([2, 12, 64, 9])

Attention Scores:
 torch.Size([2, 12, 9, 9])

Attention Scores Divided by Scalar:
 torch.Size([2, 12, 9, 9])

Attention Probabilities Softmax Layer:
 torch.Size([2, 12, 9, 9])

Attention Probabilities Dropout Layer:
 torch.Size([2, 12, 9, 9])

Context:
 torch.Size([2, 12, 9, 64])

Context Permute:
 torch.Size([2, 9, 12, 64])

Context Reshaped:
 torch.Size([2, 9, 768])
</code></pre></div>

<h6 id="-bertselfoutput"># <strong>BertSelfOutput</strong><a class="headerlink" href="#-bertselfoutput" title="Permanent link"></a></h6>
<p>This layer contains the <code>torch.nn</code> basic components of the self-attention implementation.</p>
<p>Implementation can be found at <a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L337">transformers/src/transformers/models/bert/modeling_bert.py#L337</a>.</p>
<p>The <code>forward</code> pass uses:</p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear">torch.nn.Linear</a> layer: </li>
</ul>
<p><code>self.dense = nn.Linear(config.hidden_size, config.hidden_size)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#layernorm">torch.nn.LayerNorm</a> layer for normalization:</li>
</ul>
<p><code>self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout">torch.nn.Dropout</a> layer for dropout:</li>
</ul>
<p><code>self.dropout = nn.Dropout(config.hidden_dropout_prob)</code></p>
<details>
<summary>Show BertSelfOutput Diagram</summary>

<figure>
  <img src="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings_selfoutput.png"  />
</figure>

</details>

<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BertSelfOutput</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span> <span class="o">=</span> <span class="n">BertLayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Hidden States:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Hidden States Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Hidden States Dropout Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_states</span> <span class="o">+</span> <span class="n">input_tensor</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Hidden States Normalization Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span>


<span class="c1"># Create Bert self output layer.</span>
<span class="n">bert_selfoutput_block</span> <span class="o">=</span> <span class="n">BertSelfOutput</span><span class="p">(</span><span class="n">bert_configuraiton</span><span class="p">)</span>

<span class="c1"># Perform a forward pass - context_embedding[0] because we have tuple.</span>
<span class="n">attention_output</span> <span class="o">=</span> <span class="n">bert_selfoutput_block</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">context_embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_tensor</span><span class="o">=</span><span class="n">embedding_output</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Hidden States:
 torch.Size([2, 9, 768])

Hidden States Linear Layer:
 torch.Size([2, 9, 768])

Hidden States Dropout Layer:
 torch.Size([2, 9, 768])

Hidden States Normalization Layer:
 torch.Size([2, 9, 768])
</code></pre></div>

<h6 id="-assemble-bertattention"># <strong>Assemble BertAttention</strong><a class="headerlink" href="#-assemble-bertattention" title="Permanent link"></a></h6>
<p>Put together <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L212">BertSelfAttention</a></strong> layer and <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L337">BertSelfOutput</a></strong> layer to create the <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L351">BertAttention</a> layer</strong>.</p>
<p>Now perform a <code>forward</code> pass using previous output layer as input.</p>
<details>
<summary>Show BertAttention Diagram</summary>

<figure>
  <img src="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings_attention.png" />
</figure>

</details>

<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BertAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self</span> <span class="o">=</span> <span class="n">BertSelfAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">BertSelfOutput</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">heads</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">find_pruneable_heads_and_indices</span><span class="p">(</span>
            <span class="n">heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">attention_head_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span>
        <span class="p">)</span>

        <span class="c1"># Prune linear layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">query</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">prune_linear_layer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">dense</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Update hyper params and store pruned heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">all_head_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">attention_head_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pruned_heads</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">self_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_value</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">self_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">attention_output</span><span class="p">,)</span> <span class="o">+</span> <span class="n">self_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>  <span class="c1"># add attentions if we output them</span>
        <span class="k">return</span> <span class="n">outputs</span>

<span class="c1"># Create attention assembled layer.</span>
<span class="n">bert_attention_block</span> <span class="o">=</span> <span class="n">BertAttention</span><span class="p">(</span><span class="n">bert_configuraiton</span><span class="p">)</span>

<span class="c1"># Perform a forward pass to wholte Bert Attention layer.</span>
<span class="n">attention_output</span> <span class="o">=</span> <span class="n">bert_attention_block</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">embedding_output</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Attention Head Size:
 64

Combined Attentions Head Size:
 768

Hidden States:
 torch.Size([2, 9, 768])

Query Linear Layer:
 torch.Size([2, 9, 768])

Key Linear Layer:
 torch.Size([2, 9, 768])

Value Linear Layer:
 torch.Size([2, 9, 768])

Query:
 torch.Size([2, 12, 9, 64])

Key:
 torch.Size([2, 12, 9, 64])

Value:
 torch.Size([2, 12, 9, 64])

Key Transposed:
 torch.Size([2, 12, 64, 9])

Attention Scores:
 torch.Size([2, 12, 9, 9])

Attention Scores Divided by Scalar:
 torch.Size([2, 12, 9, 9])

Attention Probabilities Softmax Layer:
 torch.Size([2, 12, 9, 9])

Attention Probabilities Dropout Layer:
 torch.Size([2, 12, 9, 9])

Context:
 torch.Size([2, 12, 9, 64])

Context Permute:
 torch.Size([2, 9, 12, 64])

Context Reshaped:
 torch.Size([2, 9, 768])
Hidden States:
 torch.Size([2, 9, 768])

Hidden States Linear Layer:
 torch.Size([2, 9, 768])

Hidden States Dropout Layer:
 torch.Size([2, 9, 768])

Hidden States Normalization Layer:
 torch.Size([2, 9, 768])
</code></pre></div>

<h6 id="bertintermediate"><strong>BertIntermediate</strong><a class="headerlink" href="#bertintermediate" title="Permanent link"></a></h6>
<p>This layer contains the <code>torch.nn</code> basic components of the Bert model implementation.</p>
<p>Implementation can be found at <a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L400">transformers/src/transformers/models/bert/modeling_bert.py#L400</a>.</p>
<p>The <code>forward</code> pass uses:</p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear">torch.nn.Linear</a> layer: </li>
</ul>
<p><code>self.dense = nn.Linear(config.hidden_size, config.intermediate_size)</code></p>
<details>
<summary>Show BertIntermediate Diagram</summary>

<figure>
  <img src="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings_intermediate.png" />
</figure>

</details>

<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BertIntermediate</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_act_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_act_fn</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Hidden States:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Hidden States Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_act_fn</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Hidden States Gelu Activation Function:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span>


<span class="c1"># Create bert intermediate layer.</span>
<span class="n">bert_intermediate_block</span> <span class="o">=</span> <span class="n">BertIntermediate</span><span class="p">(</span><span class="n">bert_configuraiton</span><span class="p">)</span>

<span class="c1"># Perform a forward pass - attention_output[0] because we have tuple.</span>
<span class="n">intermediate_output</span> <span class="o">=</span> <span class="n">bert_intermediate_block</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">attention_output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Hidden States:
 torch.Size([2, 9, 768])

Hidden States Linear Layer:
 torch.Size([2, 9, 3072])

Hidden States Gelu Activation Function:
 torch.Size([2, 9, 3072])
</code></pre></div>

<h6 id="bertoutput"><strong>BertOutput</strong><a class="headerlink" href="#bertoutput" title="Permanent link"></a></h6>
<p>This layer contains the <code>torch.nn</code> basic components of the Bert model implementation.</p>
<p>Implementation can be found at <a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L415">transformers/src/transformers/models/bert/modeling_bert.py#L415</a>.</p>
<p>The <code>forward</code> pass uses:</p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear">torch.nn.Linear</a> layer: </li>
</ul>
<p><code>self.dense = nn.Linear(config.intermediate_size, config.hidden_size)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#layernorm">torch.nn.LayerNorm</a> layer for normalization:</li>
</ul>
<p><code>self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout">torch.nn.Dropout</a> layer for dropout:</li>
</ul>
<p><code>self.dropout = nn.Dropout(config.hidden_dropout_prob)</code></p>
<details>
<summary>Show BertOutput Diagram</summary>

<figure>
  <img src="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings_output.png" />
</figure>

</details>

<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BertOutput</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span> <span class="o">=</span> <span class="n">BertLayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Hidden States:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Hidden States Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Hidden States Dropout Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">hidden_states</span> <span class="o">+</span> <span class="n">input_tensor</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Hidden States Layer Normalization:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden_states</span>


<span class="c1"># Create bert output layer.</span>
<span class="n">bert_output_block</span> <span class="o">=</span> <span class="n">BertOutput</span><span class="p">(</span><span class="n">bert_configuraiton</span><span class="p">)</span>

<span class="c1"># Perform forward pass - attention_output[0] dealing with tuple.</span>
<span class="n">layer_output</span> <span class="o">=</span> <span class="n">bert_output_block</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">intermediate_output</span><span class="p">,</span> <span class="n">input_tensor</span><span class="o">=</span><span class="n">attention_output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Hidden States:
 torch.Size([2, 9, 3072])

Hidden States Linear Layer:
 torch.Size([2, 9, 768])

Hidden States Dropout Layer:
 torch.Size([2, 9, 768])

Hidden States Layer Normalization:
 torch.Size([2, 9, 768])
</code></pre></div>

<h6 id="assemble-bertlayer"><strong>Assemble BertLayer</strong><a class="headerlink" href="#assemble-bertlayer" title="Permanent link"></a></h6>
<p>Put together <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L351">BertAttention</a></strong> layer, <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L400">BertIntermediate</a></strong> layer and <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L415">BertOutput</a></strong> layer to create the <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L429">BertLayer</a> layer</strong>.</p>
<p>Now perform a <code>forward</code> pass using previous output layer as input.</p>
<details>
<summary>Show BertLayer Diagram</summary>

<figure>
  <img src="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings_layer.png" />
</figure>

</details>

<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BertLayer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size_feed_forward</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">chunk_size_feed_forward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_len_dim</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">BertAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_cross_attention</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">add_cross_attention</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_cross_attention</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> should be used as a decoder model if cross attention is added&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">crossattention</span> <span class="o">=</span> <span class="n">BertAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate</span> <span class="o">=</span> <span class="n">BertIntermediate</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">BertOutput</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># decoder uni-directional self-attention cached key/values tuple is at positions 1,2</span>
        <span class="n">self_attn_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">self_attention_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">past_key_value</span><span class="o">=</span><span class="n">self_attn_past_key_value</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">self_attention_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># if decoder, the last output is tuple of self-attn cache</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">self_attention_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">present_key_value</span> <span class="o">=</span> <span class="n">self_attention_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">self_attention_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>  <span class="c1"># add self attentions if we output attention weights</span>

        <span class="n">cross_attn_present_key_value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span> <span class="ow">and</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;crossattention&quot;</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;If `encoder_hidden_states` are passed, </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`&quot;</span>

            <span class="c1"># cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple</span>
            <span class="n">cross_attn_past_key_value</span> <span class="o">=</span> <span class="n">past_key_value</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span> <span class="k">if</span> <span class="n">past_key_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">cross_attention_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">crossattention</span><span class="p">(</span>
                <span class="n">attention_output</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">head_mask</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="n">cross_attn_past_key_value</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">attention_output</span> <span class="o">=</span> <span class="n">cross_attention_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">+</span> <span class="n">cross_attention_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># add cross attentions if we output attention weights</span>

            <span class="c1"># add cross-attn cache to positions 3,4 of present_key_value tuple</span>
            <span class="n">cross_attn_present_key_value</span> <span class="o">=</span> <span class="n">cross_attention_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">present_key_value</span> <span class="o">=</span> <span class="n">present_key_value</span> <span class="o">+</span> <span class="n">cross_attn_present_key_value</span>

        <span class="n">layer_output</span> <span class="o">=</span> <span class="n">apply_chunking_to_forward</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward_chunk</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_size_feed_forward</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len_dim</span><span class="p">,</span> <span class="n">attention_output</span>
        <span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">layer_output</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span>

        <span class="c1"># if decoder, return the attn key/values as the last output</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">+</span> <span class="p">(</span><span class="n">present_key_value</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">outputs</span>

    <span class="k">def</span> <span class="nf">feed_forward_chunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_output</span><span class="p">):</span>
        <span class="n">intermediate_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
        <span class="n">layer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">intermediate_output</span><span class="p">,</span> <span class="n">attention_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">layer_output</span>



<span class="c1"># Assemble block to create Bert Layer.</span>
<span class="n">bert_layer_block</span> <span class="o">=</span> <span class="n">BertLayer</span><span class="p">(</span><span class="n">bert_configuraiton</span><span class="p">)</span>

<span class="c1"># Perform feed forward on a whole Bert Layer.</span>
<span class="n">layer_output</span> <span class="o">=</span> <span class="n">bert_layer_block</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">embedding_output</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Attention Head Size:
 64

Combined Attentions Head Size:
 768

Hidden States:
 torch.Size([2, 9, 768])

Query Linear Layer:
 torch.Size([2, 9, 768])

Key Linear Layer:
 torch.Size([2, 9, 768])

Value Linear Layer:
 torch.Size([2, 9, 768])

Query:
 torch.Size([2, 12, 9, 64])

Key:
 torch.Size([2, 12, 9, 64])

Value:
 torch.Size([2, 12, 9, 64])

Key Transposed:
 torch.Size([2, 12, 64, 9])

Attention Scores:
 torch.Size([2, 12, 9, 9])

Attention Scores Divided by Scalar:
 torch.Size([2, 12, 9, 9])

Attention Probabilities Softmax Layer:
 torch.Size([2, 12, 9, 9])

Attention Probabilities Dropout Layer:
 torch.Size([2, 12, 9, 9])

Context:
 torch.Size([2, 12, 9, 64])

Context Permute:
 torch.Size([2, 9, 12, 64])

Context Reshaped:
 torch.Size([2, 9, 768])
Hidden States:
 torch.Size([2, 9, 768])

Hidden States Linear Layer:
 torch.Size([2, 9, 768])

Hidden States Dropout Layer:
 torch.Size([2, 9, 768])

Hidden States Normalization Layer:
 torch.Size([2, 9, 768])

Hidden States:
 torch.Size([2, 9, 768])

Hidden States Linear Layer:
 torch.Size([2, 9, 3072])

Hidden States Gelu Activation Function:
 torch.Size([2, 9, 3072])

Hidden States:
 torch.Size([2, 9, 3072])

Hidden States Linear Layer:
 torch.Size([2, 9, 768])

Hidden States Dropout Layer:
 torch.Size([2, 9, 768])

Hidden States Layer Normalization:
 torch.Size([2, 9, 768])
</code></pre></div>

<h5 id="assemble-bertencoder"><strong>Assemble BertEncoder</strong><a class="headerlink" href="#assemble-bertencoder" title="Permanent link"></a></h5>
<p>Put together 12 of the <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L429">BertLayer</a></strong> layers ( in this setup <code>config.num_hidden_layers=12</code>) to create the <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L512">BertEncoder</a></strong> layer.</p>
<p>Now perform a <code>forward</code> pass using previous output layer as input.</p>
<details>
<summary>Show BertEncoder Diagram</summary>

<figure>
  <img src="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings_encoder.png" />
</figure>


</details>

<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BertEncoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">BertLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_self_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_cross_attentions</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">add_cross_attention</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="n">next_decoder_cache</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer_module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">):</span>

            <span class="c1"># ADDED</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">----------------- BERT LAYER </span><span class="si">%d</span><span class="s1"> -----------------&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="n">all_hidden_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

            <span class="n">layer_head_mask</span> <span class="o">=</span> <span class="n">head_mask</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">head_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">past_key_value</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;gradient_checkpointing&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>

                <span class="k">def</span> <span class="nf">create_custom_forward</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
                    <span class="k">def</span> <span class="nf">custom_forward</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
                        <span class="k">return</span> <span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">past_key_value</span><span class="p">,</span> <span class="n">output_attentions</span><span class="p">)</span>

                    <span class="k">return</span> <span class="n">custom_forward</span>

                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span>
                    <span class="n">create_custom_forward</span><span class="p">(</span><span class="n">layer_module</span><span class="p">),</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">layer_head_mask</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">layer_module</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">attention_mask</span><span class="p">,</span>
                    <span class="n">layer_head_mask</span><span class="p">,</span>
                    <span class="n">encoder_hidden_states</span><span class="p">,</span>
                    <span class="n">encoder_attention_mask</span><span class="p">,</span>
                    <span class="n">past_key_value</span><span class="p">,</span>
                    <span class="n">output_attentions</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
                <span class="n">next_decoder_cache</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],)</span>
            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_self_attentions</span> <span class="o">=</span> <span class="n">all_self_attentions</span> <span class="o">+</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">add_cross_attention</span><span class="p">:</span>
                    <span class="n">all_cross_attentions</span> <span class="o">=</span> <span class="n">all_cross_attentions</span> <span class="o">+</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">],)</span>

        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="n">all_hidden_states</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span>
                <span class="n">v</span>
                <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">next_decoder_cache</span><span class="p">,</span>
                    <span class="n">all_hidden_states</span><span class="p">,</span>
                    <span class="n">all_self_attentions</span><span class="p">,</span>
                    <span class="n">all_cross_attentions</span><span class="p">,</span>
                <span class="p">]</span>
                <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">BaseModelOutputWithPastAndCrossAttentions</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">next_decoder_cache</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">all_hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">all_self_attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">all_cross_attentions</span><span class="p">,</span>
        <span class="p">)</span>

<span class="c1"># create bert encoder block by stacking 12 layers</span>
<span class="n">bert_encoder_block</span> <span class="o">=</span> <span class="n">BertEncoder</span><span class="p">(</span><span class="n">bert_configuraiton</span><span class="p">)</span>

<span class="c1"># perform forward pass on entire Bert Encoder</span>
<span class="n">encoder_embedding</span> <span class="o">=</span> <span class="n">bert_encoder_block</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">embedding_output</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768

----------------- BERT LAYER 1 -----------------

Hidden States:
 torch.Size([2, 9, 768])

Query Linear Layer:
 torch.Size([2, 9, 768])

Key Linear Layer:
 torch.Size([2, 9, 768])

Value Linear Layer:
 torch.Size([2, 9, 768])

Query:
 torch.Size([2, 12, 9, 64])

Key:
 torch.Size([2, 12, 9, 64])

Value:
 torch.Size([2, 12, 9, 64])

Key Transposed:
 torch.Size([2, 12, 64, 9])

Attention Scores:
 torch.Size([2, 12, 9, 9])

Attention Scores Divided by Scalar:
 torch.Size([2, 12, 9, 9])

Attention Probabilities Softmax Layer:
 torch.Size([2, 12, 9, 9])

Attention Probabilities Dropout Layer:
 torch.Size([2, 12, 9, 9])

Context:
 torch.Size([2, 12, 9, 64])

Context Permute:
 torch.Size([2, 9, 12, 64])

Context Reshaped:
 torch.Size([2, 9, 768])
Hidden States:
 torch.Size([2, 9, 768])

Hidden States Linear Layer:
 torch.Size([2, 9, 768])

Hidden States Dropout Layer:
 torch.Size([2, 9, 768])

Hidden States Normalization Layer:
 torch.Size([2, 9, 768])

Hidden States:
 torch.Size([2, 9, 768])

Hidden States Linear Layer:
 torch.Size([2, 9, 3072])

Hidden States Gelu Activation Function:
 torch.Size([2, 9, 3072])

Hidden States:
 torch.Size([2, 9, 3072])

Hidden States Linear Layer:
 torch.Size([2, 9, 768])

Hidden States Dropout Layer:
 torch.Size([2, 9, 768])

Hidden States Layer Normalization:
 torch.Size([2, 9, 768])

----------------- BERT LAYER 2 -----------------

...

----------------- BERT LAYER 12 -----------------

...
</code></pre></div>

<h4 id="bertpooler"><strong>BertPooler</strong><a class="headerlink" href="#bertpooler" title="Permanent link"></a></h4>
<p>This layer contains the core of the bert model where the self-attention happens. </p>
<p>The implementation can be found at: <a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L601">transformers/src/transformers/models/bert/modeling_bert.py#L601</a>.</p>
<p>The <code>forward</code> pass uses:</p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear">torch.nn.Linear</a> layer: </li>
</ul>
<p><code>self.dense = torch.nn.Linear(config.hidden_size, config.hidden_size)</code></p>
<ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#tanh">torch.nn.Tanh</a> activation function layer: </li>
</ul>
<p><code>self.activation = torch.nn.Tanh()</code></p>
<details>
<summary>Show BertPooler Diagram</summary>

<figure>
  <img src="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings_pooler.png" />
</figure>

</details>

<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BertPooler</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="c1"># We &quot;pool&quot; the model by simply taking the hidden state corresponding</span>
        <span class="c1"># to the first token.</span>
       
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Hidden States:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">first_token_tensor</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">First Token [CLS]:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">first_token_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">pooled_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">first_token_tensor</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">First Token [CLS] Linear Layer:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">pooled_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">pooled_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">pooled_output</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">First Token [CLS] Tanh Activation Function:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">pooled_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pooled_output</span>


<span class="c1"># Create bert pooler block.</span>
<span class="n">bert_pooler_block</span> <span class="o">=</span> <span class="n">BertPooler</span><span class="p">(</span><span class="n">bert_configuraiton</span><span class="p">)</span>

<span class="c1"># Perform forward pass - encoder_embedding[0] because it is a tuple.</span>
<span class="n">pooled_output</span> <span class="o">=</span> <span class="n">bert_pooler_block</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Hidden States:
 torch.Size([2, 9, 768])

First Token [CLS]:
 torch.Size([2, 768])

First Token [CLS] Linear Layer:
 torch.Size([2, 768])

First Token [CLS] Tanh Activation Function:
 torch.Size([2, 768])
</code></pre></div>

<h4 id="assemble-bertmodel"><strong>Assemble BertModel</strong><a class="headerlink" href="#assemble-bertmodel" title="Permanent link"></a></h4>
<p>Put together <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L165">BertEmbeddings</a></strong> layer, <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L512">BertEncoder</a></strong> layer and <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L601">BertPooler</a></strong> layer to create the <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L815">BertModel</a></strong> layer.</p>
<p>Now perform a <code>forward</code> pass using previous output layer as input.</p>
<details>
<summary>Show BertPooler Diagram</summary>

<figure>
  <img src="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings_model.png" />
</figure>


</details>

<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BertModel</span><span class="p">(</span><span class="n">BertPreTrainedModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of</span>
<span class="sd">    cross-attention is added between the self-attention layers, following the architecture described in `Attention is</span>
<span class="sd">    all you need &lt;https://arxiv.org/abs/1706.03762&gt;`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,</span>
<span class="sd">    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.</span>
<span class="sd">    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration</span>
<span class="sd">    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`</span>
<span class="sd">    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an</span>
<span class="sd">    input to the forward pass.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">add_pooling_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">BertEmbeddings</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">BertEncoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pooler</span> <span class="o">=</span> <span class="n">BertPooler</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">if</span> <span class="n">add_pooling_layer</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_prune_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heads_to_prune</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base</span>
<span class="sd">        class PreTrainedModel</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">heads</span> <span class="ow">in</span> <span class="n">heads_to_prune</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">layer</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">prune_heads</span><span class="p">(</span><span class="n">heads</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">use_cache</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):</span>
<span class="sd">            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if</span>
<span class="sd">            the model is configured as a decoder.</span>
<span class="sd">        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):</span>
<span class="sd">            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in</span>
<span class="sd">            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:</span>
<span class="sd">            - 1 for tokens that are **not masked**,</span>
<span class="sd">            - 0 for tokens that are **masked**.</span>
<span class="sd">        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):</span>
<span class="sd">            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</span>
<span class="sd">            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`</span>
<span class="sd">            (those that don&#39;t have their past key value states given to this model) of shape :obj:`(batch_size, 1)`</span>
<span class="sd">            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.</span>
<span class="sd">        use_cache (:obj:`bool`, `optional`):</span>
<span class="sd">            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up</span>
<span class="sd">            decoding (see :obj:`past_key_values`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span><span class="p">:</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span> <span class="k">if</span> <span class="n">use_cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You cannot specify both input_ids and inputs_embeds at the same time&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="k">elif</span> <span class="n">inputs_embeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">size</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You have to specify either input_ids or inputs_embeds&quot;</span><span class="p">)</span>

        <span class="n">device</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="n">input_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">inputs_embeds</span><span class="o">.</span><span class="n">device</span>

        <span class="c1"># past_key_values_length</span>
        <span class="n">past_key_values_length</span> <span class="o">=</span> <span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">+</span> <span class="n">past_key_values_length</span><span class="p">)),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">token_type_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]</span>
        <span class="c1"># ourselves in which case we just need to make it broadcastable to all heads.</span>
        <span class="n">extended_attention_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_extended_attention_mask</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

        <span class="c1"># If a 2D or 3D attention mask is provided for the cross-attention</span>
        <span class="c1"># we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_decoder</span> <span class="ow">and</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">encoder_batch_size</span><span class="p">,</span> <span class="n">encoder_sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">encoder_hidden_states</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
            <span class="n">encoder_hidden_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">encoder_batch_size</span><span class="p">,</span> <span class="n">encoder_sequence_length</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">encoder_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">encoder_hidden_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">encoder_extended_attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">invert_attention_mask</span><span class="p">(</span><span class="n">encoder_attention_mask</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encoder_extended_attention_mask</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Prepare head mask if needed</span>
        <span class="c1"># 1.0 in head_mask indicate we keep the head</span>
        <span class="c1"># attention_probs has shape bsz x n_heads x N x N</span>
        <span class="c1"># input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]</span>
        <span class="c1"># and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]</span>
        <span class="n">head_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_head_mask</span><span class="p">(</span><span class="n">head_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>

        <span class="n">embedding_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">past_key_values_length</span><span class="o">=</span><span class="n">past_key_values_length</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span>
            <span class="n">embedding_output</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">extended_attention_mask</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="o">=</span><span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="o">=</span><span class="n">encoder_extended_attention_mask</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">pooled_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooler</span><span class="p">(</span><span class="n">sequence_output</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">sequence_output</span><span class="p">,</span> <span class="n">pooled_output</span><span class="p">)</span> <span class="o">+</span> <span class="n">encoder_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="n">BaseModelOutputWithPoolingAndCrossAttentions</span><span class="p">(</span>
            <span class="n">last_hidden_state</span><span class="o">=</span><span class="n">sequence_output</span><span class="p">,</span>
            <span class="n">pooler_output</span><span class="o">=</span><span class="n">pooled_output</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
            <span class="n">cross_attentions</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">cross_attentions</span><span class="p">,</span>
        <span class="p">)</span>


<span class="c1"># Create bert model.</span>
<span class="n">bert_model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">(</span><span class="n">bert_configuraiton</span><span class="p">)</span>

<span class="c1"># Perform forward pass on entire model.</span>
<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">bert_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_sequences</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">],</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_sequences</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">],</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">input_sequences</span><span class="p">[</span><span class="s1">&#39;token_type_ids&#39;</span><span class="p">])</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Created Tokens Positions IDs:
 tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]])

Tokens IDs:
 torch.Size([2, 9])

Tokens Type IDs:
 torch.Size([2, 9])

Word Embeddings:
 torch.Size([2, 9, 768])

Position Embeddings:
 torch.Size([1, 9, 768])

Token Types Embeddings:
 torch.Size([2, 9, 768])

Sum Up All Embeddings:
 torch.Size([2, 9, 768])

Embeddings Layer Nromalization:
 torch.Size([2, 9, 768])

Embeddings Dropout Layer:
 torch.Size([2, 9, 768])

----------------- BERT LAYER 1 -----------------

...

----------------- BERT LAYER 12 -----------------
</code></pre></div>

<p>&hellip;</p>
<div class="highlight"><pre><span></span><code>Hidden States:
 torch.Size([2, 9, 768])

First Token [CLS]:
 torch.Size([2, 768])

First Token [CLS] Linear Layer:
 torch.Size([2, 768])

First Token [CLS] Tanh Activation Function:
 torch.Size([2, 768])
</code></pre></div>

<h3 id="assemble-components">Assemble Components<a class="headerlink" href="#assemble-components" title="Permanent link"></a></h3>
<p>Put together <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L815">BertModel</a></strong> layer, <strong><a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#dropout">torch.nn.Dropout</a></strong> layer and <strong><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear">torch.nn.Linear</a></strong> layer  to create the <strong><a href="https://github.com/huggingface/transformers/blob/d944966b19a4d6860bddc7cdc1ba928ca8a0da91/src/transformers/models/bert/modeling_bert.py#L1449">BertForSequenceClassification</a></strong> model.</p>
<p>Now perform a <code>forward</code> pass using previous output layer as input.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BertForSequenceClassification</span><span class="p">(</span><span class="n">BertPreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bert</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_dropout_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_labels</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">head_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">inputs_embeds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):</span>
<span class="sd">            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,</span>
<span class="sd">            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),</span>
<span class="sd">            If :obj:`config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_return_dict</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert</span><span class="p">(</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">head_mask</span><span class="o">=</span><span class="n">head_mask</span><span class="p">,</span>
            <span class="n">inputs_embeds</span><span class="o">=</span><span class="n">inputs_embeds</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="n">return_dict</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">pooled_output</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">pooled_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">pooled_output</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">pooled_output</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1">#  We are doing regression</span>
                <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_labels</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span><span class="p">,)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
            <span class="k">return</span> <span class="p">((</span><span class="n">loss</span><span class="p">,)</span> <span class="o">+</span> <span class="n">output</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output</span>

        <span class="k">return</span> <span class="n">SequenceClassifierOutput</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attentions</span><span class="o">=</span><span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">,</span>
        <span class="p">)</span>


<span class="c1"># create Bert model with classification layer - BertForSequenceClassificatin</span>
<span class="n">bert_for_sequence_classification_model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="p">(</span><span class="n">bert_configuraiton</span><span class="p">)</span>

<span class="c1"># perform forward pass on entire model</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">bert_for_sequence_classification_model</span><span class="p">(</span><span class="o">**</span><span class="n">input_sequences</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Attention Head Size:
 64

Combined Attentions Head Size:
 768
Created Tokens Positions IDs:
 tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]])

Tokens IDs:
 torch.Size([2, 9])

Tokens Type IDs:
 torch.Size([2, 9])

Word Embeddings:
 torch.Size([2, 9, 768])

Position Embeddings:
 torch.Size([1, 9, 768])

Token Types Embeddings:
 torch.Size([2, 9, 768])

Sum Up All Embeddings:
 torch.Size([2, 9, 768])

Embeddings Layer Nromalization:
 torch.Size([2, 9, 768])

Embeddings Dropout Layer:
 torch.Size([2, 9, 768])

----------------- BERT LAYER 1 -----------------

...

----------------- BERT LAYER 12 -----------------

...

Hidden States:
 torch.Size([2, 9, 768])

First Token [CLS]:
 torch.Size([2, 768])

First Token [CLS] Linear Layer:
 torch.Size([2, 768])

First Token [CLS] Tanh Activation Function:
 torch.Size([2, 768])
</code></pre></div>

<h3 id="complete-diagram">Complete Diagram<a class="headerlink" href="#complete-diagram" title="Permanent link"></a></h3>
<ul>
<li>
<p>If you want a <strong>.pdf</strong> version of this diagram: <a href="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings.pdf">bert_inner_workings.pdf</a>.</p>
</li>
<li>
<p>If you want a <strong>.png</strong> version of this diagram: <a href="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings.png">bert_inner_workings.png</a>.</p>
</li>
</ul>
<p><br></p>
<p><img alt="bert_inner_workings" src="https://github.com/gmihaila/ml_things/raw/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings.png" /></p>
<h2 id="final-note"><strong>Final Note</strong><a class="headerlink" href="#final-note" title="Permanent link"></a></h2>
<p>If you made it this far <strong>Congrats!</strong> 🎊 and <strong>Thank you!</strong> 🙏 for your interest in my tutorial!</p>
<p>I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow.</p>
<p>Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials!</p>
<p>If you see something wrong please let me know by opening an issue on my <a href="https://github.com/gmihaila/ml_things/issues">ml_things GitHub repository</a>!</p>
<p>A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.</p>
<h2 id="contact-"><strong>Contact</strong> 🎣<a class="headerlink" href="#contact-" title="Permanent link"></a></h2>
<p>🦊 GitHub: <a href="https://github.com/gmihaila">gmihaila</a></p>
<p>🌐 Website: <a href="https://gmihaila.github.io/">gmihaila.github.io</a></p>
<p>👔 LinkedIn: <a href="https://medium.com/r/?url=https%3A%2F%2Fwww.linkedin.com%2Fin%2Fmihailageorge">mihailageorge</a></p>
<p>📬 Email: <a href="mailto:georgemihaila@my.unt.edu.com?subject=GitHub%20Website">georgemihaila@my.unt.edu.com</a></p>
<p><br></p>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../../useful/useful/" title="Useful Code" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Useful Code
              </div>
            </div>
          </a>
        
        
          <a href="../pytorchtext_bucketiterator/" title="PyTorchText BucketIterator" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                PyTorchText BucketIterator
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4,11V13H16L10.5,18.5L11.92,19.92L19.84,12L11.92,4.08L10.5,5.5L16,11H4Z" /></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          Copyright &copy; 2015 - 2020 <a href="https://github.com/gmihaila"  target="_blank" rel="noopener">George Mihaila</a>
<br>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        with emoji by
        <a href="https://github.com/twitter/twemoji" target="_blank" rel="noopener">
          Twemoji
        </a>
      </div>
      
  <div class="md-footer-social">
    
      
      
      <a href="https://github.com/gmihaila" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
      <a href="https://www.linkedin.com/in/mihailageorge" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/vendor.c51dfa35.min.js"></script>
      <script src="../../assets/javascripts/bundle.eaaa3931.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "../..",
          features: ["instant"],
          search: Object.assign({
            worker: "../../assets/javascripts/worker/search.58d22e8e.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="../../assets/pymdownx-extras/extra.js"></script>
      
    
  </body>
</html>