
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Text classification is a very common problem that needs solving when dealing with text data. We’ve all seen and know how to use Encoder Transformer models like Bert and RoBerta for text classification but did you know you can use a Decoder Transformer model like GPT2 for text classification? In this tutorial, I will walk you through on how to use GPT2 from HuggingFace for text classification.">
      
      
      
        <link rel="canonical" href="https://github.com/gmihaila/gmihaila.github.io/activities/nlp_transformers/slides/">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-8.5.9">
    
    
      
        <title>Natural Language Processing and Transformers - George Mihaila</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.20d9efc8.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.08040f6c.min.css">
        
          
          
          <meta name="theme-color" content="#000000">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="black">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#natural-language-processing-and-transformers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="George Mihaila" class="md-header__button md-logo" aria-label="George Mihaila" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M8.75 8a.75.75 0 0 0-.75.75v6.5c0 .414.336.75.75.75h6.5a.75.75 0 0 0 .75-.75v-6.5a.75.75 0 0 0-.75-.75h-6.5zm.75 6.5v-5h5v5h-5z"/><path fill-rule="evenodd" d="M15.25 1a.75.75 0 0 1 .75.75V4h2.25c.966 0 1.75.784 1.75 1.75V8h2.25a.75.75 0 0 1 0 1.5H20v5h2.25a.75.75 0 0 1 0 1.5H20v2.25A1.75 1.75 0 0 1 18.25 20H16v2.25a.75.75 0 0 1-1.5 0V20h-5v2.25a.75.75 0 0 1-1.5 0V20H5.75A1.75 1.75 0 0 1 4 18.25V16H1.75a.75.75 0 0 1 0-1.5H4v-5H1.75a.75.75 0 0 1 0-1.5H4V5.75C4 4.784 4.784 4 5.75 4H8V1.75a.75.75 0 0 1 1.5 0V4h5V1.75a.75.75 0 0 1 .75-.75zm3 17.5a.25.25 0 0 0 .25-.25V5.75a.25.25 0 0 0-.25-.25H5.75a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h12.5z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            George Mihaila
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Natural Language Processing and Transformers
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/gmihaila/machine_learning_things" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    Machine Learning Things
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="George Mihaila" class="md-nav__button md-logo" aria-label="George Mihaila" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M8.75 8a.75.75 0 0 0-.75.75v6.5c0 .414.336.75.75.75h6.5a.75.75 0 0 0 .75-.75v-6.5a.75.75 0 0 0-.75-.75h-6.5zm.75 6.5v-5h5v5h-5z"/><path fill-rule="evenodd" d="M15.25 1a.75.75 0 0 1 .75.75V4h2.25c.966 0 1.75.784 1.75 1.75V8h2.25a.75.75 0 0 1 0 1.5H20v5h2.25a.75.75 0 0 1 0 1.5H20v2.25A1.75 1.75 0 0 1 18.25 20H16v2.25a.75.75 0 0 1-1.5 0V20h-5v2.25a.75.75 0 0 1-1.5 0V20H5.75A1.75 1.75 0 0 1 4 18.25V16H1.75a.75.75 0 0 1 0-1.5H4v-5H1.75a.75.75 0 0 1 0-1.5H4V5.75C4 4.784 4.784 4 5.75 4H8V1.75a.75.75 0 0 1 1.5 0V4h5V1.75a.75.75 0 0 1 .75-.75zm3 17.5a.25.25 0 0 0 .25-.25V5.75a.25.25 0 0 0-.25-.25H5.75a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25h12.5z"/></svg>

    </a>
    George Mihaila
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/gmihaila/machine_learning_things" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    Machine Learning Things
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        About
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../activities/" class="md-nav__link">
        Activities
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../useful/useful/" class="md-nav__link">
        Useful Code
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Tutorial Notebooks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tutorial Notebooks" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Tutorial Notebooks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorial_notebooks/bert_inner_workings/" class="md-nav__link">
        Bert Inner Workings
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorial_notebooks/pytorchtext_bucketiterator/" class="md-nav__link">
        PyTorchText BucketIterator
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorial_notebooks/pretrain_transformers_pytorch/" class="md-nav__link">
        Pretrain Transformers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorial_notebooks/finetune_transformers_pytorch/" class="md-nav__link">
        Finetune Transformers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../tutorial_notebooks/gpt2_finetune_classification/" class="md-nav__link">
        GPT2 Finetune Classification
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../resume/resume/" class="md-nav__link">
        Resume
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <a href="https://github.com/gmihaila/machine_learning_things/tree/master/docs/src/markdown/activities/nlp_transformers/slides.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

<h1 id="natural-language-processing-and-transformers"> Natural Language Processing and Transformers<a class="headerlink" href="#natural-language-processing-and-transformers" title="Permanent link"></a></h1>
<p><br/><br/></p>
<p><strong>George Mihaila</strong></p>
<p><em>PhD Candidate Computer Science</em>
<em>University of North Texas</em></p>
<p><br/></p>
<hr />
<h1 id="about-me">About me<a class="headerlink" href="#about-me" title="Permanent link"></a></h1>
<ul>
<li>Research Scientist at Amazon.</li>
<li>PhD Candidate Computer Science at University of North Texas.</li>
<li>Over 5 years of experience in the field of Deep Learning, Machine Learning and Natural Language Processing (NLP).</li>
<li>Over 3 years experience in industry as Machine Leaning Engineer, Data Scientist and Research Scientist.</li>
<li>Technical reviewer for 2 NLP Transformers models books: <code>Transformers for Natural Language Processing</code> (<code>1st</code> &amp; <code>2nd</code> edition).</li>
<li>In my free time I like traveling, cooking and small projects like writing tutorials on NLP/Deep Learning.</li>
</ul>
<hr />
<h1 id="agenda">Agenda<a class="headerlink" href="#agenda" title="Permanent link"></a></h1>
<ul>
<li>Intro</li>
<li>Understanding Natural Language Processing</li>
<li>Word embeddings</li>
<li>The Transformer model</li>
<li>BERT model</li>
<li>Architecture</li>
<li>Inner Workings</li>
<li>Embeddings</li>
<li>Sentiment Analysis</li>
<li>Conclusions</li>
<li>Q&amp;A</li>
</ul>
<p><br></p>
<hr />
<h1 id="intro">Intro<a class="headerlink" href="#intro" title="Permanent link"></a></h1>
<h2 id="wikipedia">Wikipedia<a class="headerlink" href="#wikipedia" title="Permanent link"></a></h2>
<p><strong>Natural language processing (NLP)</strong> is a subfield of linguistics, computer science, and artificial intelligence <strong>concerned with the interactions between computers and human language</strong> &hellip;</p>
<p>&hellip; The result is a computer capable of <strong>"understanding"</strong> the contents of documents, including the contextual nuances of the language within them.</p>
<hr />
<h1 id="understanding-natural-language-processing">Understanding Natural Language Processing<a class="headerlink" href="#understanding-natural-language-processing" title="Permanent link"></a></h1>
<hr />
<h1 id="understanding-natural-language-processing_1">Understanding Natural Language Processing<a class="headerlink" href="#understanding-natural-language-processing_1" title="Permanent link"></a></h1>
<div class="highlight"><pre><span></span><code><span class="n">example</span> <span class="o">=</span> <span class="s2">&quot;I love cats! Cats are very funny.&quot;</span>
</code></pre></div>
<hr />
<h1 id="understanding-natural-language-processing_2">Understanding Natural Language Processing<a class="headerlink" href="#understanding-natural-language-processing_2" title="Permanent link"></a></h1>
<div class="highlight"><pre><span></span><code><span class="n">example</span> <span class="o">=</span> <span class="s2">&quot;I love cats! Cats are very funny.&quot;</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="s1">&#39;love&#39;</span><span class="p">,</span> <span class="s1">&#39;cats&#39;</span><span class="p">,</span> <span class="s1">&#39;are&#39;</span><span class="p">,</span> <span class="s1">&#39;funny&#39;</span><span class="p">]</span>
</code></pre></div>
<hr />
<h1 id="understanding-natural-language-processing_3">Understanding Natural Language Processing<a class="headerlink" href="#understanding-natural-language-processing_3" title="Permanent link"></a></h1>
<div class="highlight"><pre><span></span><code><span class="n">example</span> <span class="o">=</span> <span class="s2">&quot;I love cats! Cats are very funny.&quot;</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="s1">&#39;love&#39;</span><span class="p">,</span> <span class="s1">&#39;cats&#39;</span><span class="p">,</span> <span class="s1">&#39;are&#39;</span><span class="p">,</span> <span class="s1">&#39;funny&#39;</span><span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">word_id</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;i&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;love&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;cats&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;are&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;funny&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
</code></pre></div>
<hr />
<h1 id="understanding-natural-language-processing_4">Understanding Natural Language Processing<a class="headerlink" href="#understanding-natural-language-processing_4" title="Permanent link"></a></h1>
<div class="highlight"><pre><span></span><code><span class="n">example</span> <span class="o">=</span> <span class="s2">&quot;I love cats! Cats are very funny.&quot;</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="s1">&#39;love&#39;</span><span class="p">,</span> <span class="s1">&#39;cats&#39;</span><span class="p">,</span> <span class="s1">&#39;are&#39;</span><span class="p">,</span> <span class="s1">&#39;funny&#39;</span><span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">word_id</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;i&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;love&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;cats&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;are&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;funny&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
</code></pre></div>
<p>Encode text into numbers.</p>
<hr />
<h1 id="word-embeddings">Word embeddings<a class="headerlink" href="#word-embeddings" title="Permanent link"></a></h1>
<hr />
<p><img alt="bg vertical right fit:30% 60%" src="../word_ids.png" /></p>
<h1 id="word-embeddings_1">Word embeddings<a class="headerlink" href="#word-embeddings_1" title="Permanent link"></a></h1>
<ul>
<li>Use numbers to represent words:
  <div class="highlight"><pre><span></span><code><span class="s1">&#39;love&#39;</span> <span class="p">:</span> <span class="mi">1</span>
<span class="s1">&#39;cats&#39;</span> <span class="p">:</span> <span class="mi">2</span>
<span class="s1">&#39;funny&#39;</span><span class="p">:</span> <span class="mi">4</span>
</code></pre></div></li>
</ul>
<hr />
<p><img alt="bg vertical right fit:30% 60%" src="../word_ids.png" />
<img alt="bg fit" src="../word_vectors.png" /></p>
<h1 id="word-embeddings_2">Word embeddings<a class="headerlink" href="#word-embeddings_2" title="Permanent link"></a></h1>
<ul>
<li>
<p>Use numbers to represent words:
  <div class="highlight"><pre><span></span><code><span class="s1">&#39;love&#39;</span> <span class="p">:</span> <span class="mi">1</span>
<span class="s1">&#39;cats&#39;</span> <span class="p">:</span> <span class="mi">2</span>
<span class="s1">&#39;funny&#39;</span><span class="p">:</span> <span class="mi">4</span>
</code></pre></div></p>
</li>
<li>
<p>User vectors instead of numbers</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="s1">&#39;love&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.90</span><span class="p">,</span> <span class="mf">3.10</span><span class="p">]</span>
<span class="s1">&#39;cats&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mf">3.40</span><span class="p">,</span> <span class="mf">3.20</span><span class="p">]</span>
<span class="s1">&#39;funny&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.45</span><span class="p">,</span> <span class="mf">1.88</span><span class="p">]</span>
</code></pre></div>
<hr />
<p><img alt="bg vertical right fit:30% 60%" src="../word_ids.png" />
<img alt="bg fit" src="../word_vectors.png" /></p>
<h1 id="word-embeddings_3">Word embeddings<a class="headerlink" href="#word-embeddings_3" title="Permanent link"></a></h1>
<ul>
<li>
<p>Use numbers to represent words:
  <div class="highlight"><pre><span></span><code><span class="s1">&#39;love&#39;</span> <span class="p">:</span> <span class="mi">1</span>
<span class="s1">&#39;cats&#39;</span> <span class="p">:</span> <span class="mi">2</span>
<span class="s1">&#39;funny&#39;</span><span class="p">:</span> <span class="mi">4</span>
</code></pre></div></p>
</li>
<li>
<p>User vectors instead of numbers
  <div class="highlight"><pre><span></span><code><span class="s1">&#39;love&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.90</span><span class="p">,</span> <span class="mf">3.10</span><span class="p">]</span>
<span class="s1">&#39;cats&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mf">3.40</span><span class="p">,</span> <span class="mf">3.20</span><span class="p">]</span>
<span class="s1">&#39;funny&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.45</span><span class="p">,</span> <span class="mf">1.88</span><span class="p">]</span>
</code></pre></div></p>
</li>
<li>
<p><strong>Word embeddings</strong> are a type of word <strong>representation</strong> that allows words with similar meaning to have a similar representation.</p>
</li>
</ul>
<hr />
<h1 id="the-transformer">The Transformer<a class="headerlink" href="#the-transformer" title="Permanent link"></a></h1>
<ul>
<li>Is a <strong>deep neural network architecture</strong> for <strong>transforming one sequence into another one</strong> with the help of two parts (<strong>Encoder</strong> and <strong>Decoder</strong>).</li>
<li>Was first introduced by Google in 2017 in the paper <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a>.</li>
<li>Is based solely on attention mechanisms.</li>
<li>It brought <strong><em>"the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search."</em></strong> for Google.</li>
</ul>
<hr />
<h1 id="the-transformer_1">The Transformer<a class="headerlink" href="#the-transformer_1" title="Permanent link"></a></h1>
<ul>
<li>Is a <strong>deep neural network architecture</strong> for <strong>transforming one sequence into another one</strong> with the help of two parts (<strong>Encoder</strong> and <strong>Decoder</strong>).</li>
<li>It can <strong><em>transform</em></strong> an article to a summary or translate english to another language, etc.</li>
</ul>
<p><img alt="bg vertical right fit" src="../transformers.png" /></p>
<hr />
<h1 id="the-transformer_2">The Transformer<a class="headerlink" href="#the-transformer_2" title="Permanent link"></a></h1>
<ul>
<li>Is based solely on attention mechanisms.</li>
</ul>
<p><img alt="bg vertical right fit" src="../self_attention.png" /></p>
<ul>
<li><strong>Disclaimer:</strong></li>
<li>I will not cover attention since this is not the intent of this presentation.</li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> by <a href="http://jalammar.github.io">Jay Alammar</a> is great resource!</li>
</ul>
<hr />
<h1 id="bert">BERT<a class="headerlink" href="#bert" title="Permanent link"></a></h1>
<h2 id="wikipedia_1">Wikipedia<a class="headerlink" href="#wikipedia_1" title="Permanent link"></a></h2>
<ul>
<li>Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google.</li>
<li>BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. </li>
<li>In 2019, Google announced that it had begun leveraging BERT in its search engine, and by late 2020 it was using BERT in almost every English-language query. </li>
<li>A 2020 literature survey concluded that "in a little over a year, BERT has become a ubiquitous baseline in NLP experiments".</li>
</ul>
<hr />
<h1 id="architecture">Architecture<a class="headerlink" href="#architecture" title="Permanent link"></a></h1>
<ul>
<li>We only have the encoder side of Transformer:</li>
</ul>
<p><img alt="" src="../transformers.png" />
<img alt="bg vertical right fit 60% 60%" src="../transformers_encoder_model.png" /></p>
<hr />
<h1 id="inner-workings">Inner Workings<a class="headerlink" href="#inner-workings" title="Permanent link"></a></h1>
<ul>
<li><a href="https://raw.githubusercontent.com/gmihaila/ml_things/master/notebooks/pytorch/bert_inner_workings/bert_inner_workings.png">Detailed diagram</a></li>
</ul>
<hr />
<p><img alt="bg vertical right fit 60% 60%" src="../bert_input_sequence.png" /></p>
<h1 id="bert-embeddings">BERT Embeddings<a class="headerlink" href="#bert-embeddings" title="Permanent link"></a></h1>
<ul>
<li><code>"I love cats"</code> is passed to the BERT model.</li>
<li>Output word embeddings for each word in the <strong>Word Sequence</strong>.</li>
<li>Special symbol word at the beginning of any <strong>Word Sequence</strong>.</li>
<li><code>[CLS]</code> used to represent meaning of the <strong>Word Sequence</strong>.</li>
<li>Context specific embeddings.</li>
</ul>
<hr />
<h1 id="movie-review-sentiment-analysis">Movie Review Sentiment Analysis<a class="headerlink" href="#movie-review-sentiment-analysis" title="Permanent link"></a></h1>
<ul>
<li>
<p>We will target a particular Natural Language Processing (NLP) problem -  Sentiment Analysis.</p>
</li>
<li>
<p>IMDB movie reviews sentiment dataset:</p>
</li>
<li>This is a dataset for binary sentiment classification containing a set of 25,000 highly popular movie reviews for training, and 25,000 for testing.</li>
</ul>
<hr />
<p><img alt="bg vertical right fit 60% 60%" src="../moview_review_representaiton_using_bert.png" /></p>
<h1 id="sentiment-analysis-with-bert-embeddings">Sentiment Analysis with BERT Embeddings<a class="headerlink" href="#sentiment-analysis-with-bert-embeddings" title="Permanent link"></a></h1>
<ul>
<li>Took <code>2,000</code> random movie reviews from the <strong>IMDB movie reviews sentiment dataset</strong>.</li>
<li>Compressed each of the <code>768</code> embeddings representation in <code>2</code> components using <strong>PCA</strong>.</li>
<li>Plot each movie review:
  <code>orange</code> - <strong>positive</strong>
  <code>blue</code> - <strong>negative</strong></li>
</ul>
<hr />
<h1 id="sentiment-analysis-with-bert-embeddings_1">Sentiment Analysis with BERT Embeddings<a class="headerlink" href="#sentiment-analysis-with-bert-embeddings_1" title="Permanent link"></a></h1>
<p><img alt="" src="../moview_review_representaiton_using_bert.png" /></p>
<hr />
<h1 id="sentiment-analysis-with-bert-embeddings_2">Sentiment Analysis with BERT Embeddings<a class="headerlink" href="#sentiment-analysis-with-bert-embeddings_2" title="Permanent link"></a></h1>
<p><img alt="" src="../moview_review_representaiton_using_bert_outlier.png" /></p>
<hr />
<h1 id="sentiment-analysis-with-bert-embeddings_3">Sentiment Analysis with BERT Embeddings<a class="headerlink" href="#sentiment-analysis-with-bert-embeddings_3" title="Permanent link"></a></h1>
<p><img alt="bg vertical right fit 60% 60%" src="../moview_review_representaiton_using_bert_outlier.png" /></p>
<p><code>I could not agree less with the rating that was given to this movie, and I believe this is a sample of how short minded 
most of spectators are all over the world. Really... Are you forgetting that Cinema used to be a kind of art before...</code></p>
<ul>
<li>BERT model had some trouble understanding this review ?!</li>
</ul>
<hr />
<h1 id="k-means-with-bert-embeddings">K-means with BERT embeddings<a class="headerlink" href="#k-means-with-bert-embeddings" title="Permanent link"></a></h1>
<ul>
<li>Use K-means clustering with BERT embeddings.</li>
<li>
<p>We know we have two sentiments - number of clusters 2.</p>
</li>
<li>
<p><img alt="" src="../kmeans_2_bert_embeddings.png" /></p>
</li>
</ul>
<hr />
<p><img alt="bg left-bottom fit 100% 100%" src="../moview_review_representaiton_using_bert.png" /></p>
<p><img alt="bg right-bottom fit 100% 100%" src="../kmeans_2_bert_embeddings.png" /></p>
<hr />
<h1 id="fine-grained-sentiment-analysis">Fine-grained Sentiment Analysis<a class="headerlink" href="#fine-grained-sentiment-analysis" title="Permanent link"></a></h1>
<ul>
<li>Sentiment classifiers are used in binary classification (just positive or just negative sentiment).</li>
<li>Fine-grained sentiment classification is a significantly more challenging task!</li>
<li>Typical breakdown of fine-grained sentiment:
<img alt="" src="https://camo.githubusercontent.com/925adaa041274057eeec6b903761e0f70701e4cca8937e280e65935fd4d4db24/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f3730352f312a7567386b7971556c6e7145756f334c6848664e7970672e706e67" /></li>
</ul>
<hr />
<h1 id="k-means-with-bert-embeddings_1">K-means with BERT embeddings<a class="headerlink" href="#k-means-with-bert-embeddings_1" title="Permanent link"></a></h1>
<ul>
<li>Use K-means clustering with BERT embeddings.</li>
<li>Try to find 3 sentiments (positive, neutral, negative) - number of clusters 3.</li>
<li><img alt="" src="../kmeans_3_bert_embeddings.png" /></li>
</ul>
<hr />
<p><img alt="bg left-bottom fit 100% 100%" src="../moview_review_representaiton_using_bert.png" /></p>
<p><img alt="bg right-bottom fit 100% 100%" src="../kmeans_3_bert_embeddings.png" /></p>
<hr />
<h1 id="k-means-with-bert-embeddings_2">K-means with BERT embeddings<a class="headerlink" href="#k-means-with-bert-embeddings_2" title="Permanent link"></a></h1>
<ul>
<li>Use K-means clustering with BERT embeddings.</li>
<li>Try to find 5 sentiments (positive, weakly_positive, neutral, weakly_negative, negative) - number of clusters 5.</li>
<li><img alt="" src="../kmeans_5_bert_embeddings.png" /></li>
</ul>
<hr />
<p><img alt="bg left-bottom fit 100% 100%" src="../moview_review_representaiton_using_bert.png" /></p>
<p><img alt="bg right-bottom fit 100% 100%" src="../kmeans_5_bert_embeddings.png" /></p>
<hr />
<h1 id="try-it-out-yourself">Try it out yourself<a class="headerlink" href="#try-it-out-yourself" title="Permanent link"></a></h1>
<p><a href="https://colab.research.google.com/github/gmihaila/gmihaila.github.io/blob/master/docs/markdown/activities/nlp_transformers/nlp_with_transformers_sentiment_analysis.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> &nbsp;
<a href="https://github.com/gmihaila/gmihaila.github.io/blob/master/docs/markdown/activities/nlp_transformers/nlp_with_transformers_sentiment_analysis.ipynb"><img alt="Generic badge" src="https://img.shields.io/badge/GitHub-Source-greensvg" /></a></p>
<hr />
<h1 id="conclusions">Conclusions<a class="headerlink" href="#conclusions" title="Permanent link"></a></h1>
<ul>
<li>I showed you:</li>
<li>What word embeddings are.</li>
<li>What are Transformers and BERT models.</li>
<li>How BERT embeddings work.</li>
<li>We did some sentiment analysis on a movie review dataset and how to find more sentiments.</li>
</ul>
<hr />
<h1 id="check-out-my-notebooks-tutorials">Check out my Notebooks Tutorials<a class="headerlink" href="#check-out-my-notebooks-tutorials" title="Permanent link"></a></h1>
<table>
<thead>
<tr>
<th align="left">Name</th>
<th align="left">Description</th>
<th align="left">Links</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><strong><img alt="🍇" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/1f347.svg" title=":grapes:" /> Better Batches with PyTorchText BucketIterator</strong></td>
<td align="left"><em>How to use PyTorchText BucketIterator to sort text data for better batching.</em></td>
<td align="left"><a href="https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/pytorchtext_bucketiterator.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> <a href="https://github.com/gmihaila/ml_things/blob/master/notebooks/pytorch/pytorchtext_bucketiterator.ipynb"><img alt="Generic badge" src="https://img.shields.io/badge/GitHub-Source-greensvg" /></a> <a href="https://www.dropbox.com/s/7gyq6qup6y43z9b/pytorchtext_bucketiterator.ipynb?dl=1"><img alt="Generic badge" src="https://img.shields.io/badge/Download-Notebook-red.svg" /></a> <a href="https://gmihaila.medium.com/better-batches-with-pytorchtext-bucketiterator-12804a545e2a"><img alt="Generic badge" src="https://img.shields.io/badge/Article-Medium-black.svg" /></a> <a href="https://gmihaila.github.io/tutorial_notebooks/pytorchtext_bucketiterator/"><img alt="Generic badge" src="https://img.shields.io/badge/Blog-Post-blue.svg" /></a></td>
</tr>
<tr>
<td align="left"><strong><img alt="🐶" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/1f436.svg" title=":dog:" /> Pretrain Transformers Models in PyTorch using Hugging Face Transformers</strong></td>
<td align="left"><em>Pretrain 67 transformers models on your custom dataset.</em></td>
<td align="left"><a href="https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/pretrain_transformers_pytorch.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> <a href="https://github.com/gmihaila/ml_things/blob/master/notebooks/pytorch/pretrain_transformers_pytorch.ipynb"><img alt="Generic badge" src="https://img.shields.io/badge/GitHub-Source-greensvg" /></a> <a href="https://www.dropbox.com/s/rkq79hwzhqa6x8k/pretrain_transformers_pytorch.ipynb?dl=1"><img alt="Generic badge" src="https://img.shields.io/badge/Download-Notebook-red.svg" /></a> <a href="https://gmihaila.medium.com/pretrain-transformers-models-in-pytorch-using-transformers-ecaaec00fbaa"><img alt="Generic badge" src="https://img.shields.io/badge/Article-Medium-black.svg" /></a> <a href="https://gmihaila.github.io/tutorial_notebooks/pretrain_transformers_pytorch/"><img alt="Generic badge" src="https://img.shields.io/badge/Blog-Post-blue.svg" /></a></td>
</tr>
<tr>
<td align="left"><strong><img alt="🎻" class="twemoji" src="https://twemoji.maxcdn.com/v/latest/svg/1f3bb.svg" title=":violin:" /> Fine-tune Transformers in PyTorch using Hugging Face Transformers</strong></td>
<td align="left"><em>Complete tutorial on how to fine-tune 73 transformer models for text classification — no code changes necessary!</em></td>
<td align="left"><a href="https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/finetune_transformers_pytorch.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> <a href="https://github.com/gmihaila/ml_things/blob/master/notebooks/pytorch/finetune_transformers_pytorch.ipynb"><img alt="Generic badge" src="https://img.shields.io/badge/GitHub-Source-greensvg" /></a> <a href="https://www.dropbox.com/s/tsqicfqgt8v87ae/finetune_transformers_pytorch.ipynb?dl=1"><img alt="Generic badge" src="https://img.shields.io/badge/Download-Notebook-red.svg" /></a> <a href="https://medium.com/@gmihaila/fine-tune-transformers-in-pytorch-using-transformers-57b40450635"><img alt="Generic badge" src="https://img.shields.io/badge/Article-Medium-black.svg" /></a> <a href="https://gmihaila.github.io/tutorial_notebooks/finetune_transformers_pytorch/"><img alt="Generic badge" src="https://img.shields.io/badge/Blog-Post-blue.svg" /></a></td>
</tr>
<tr>
<td align="left"><strong>⚙️ Bert Inner Workings in PyTorch using Hugging Face Transformers</strong></td>
<td align="left"><em>Complete tutorial on how an input flows through Bert.</em></td>
<td align="left"><a href="https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/bert_inner_workings.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> <a href="https://github.com/gmihaila/ml_things/blob/master/notebooks/pytorch/bert_inner_workings.ipynb"><img alt="Generic badge" src="https://img.shields.io/badge/GitHub-Source-greensvg" /></a> <a href="https://www.dropbox.com/s/jeftyo6cebfkma2/bert_inner_workings.ipynb?dl=1"><img alt="Generic badge" src="https://img.shields.io/badge/Download-Notebook-red.svg" /></a> <a href="https://gmihaila.medium.com/%EF%B8%8F-bert-inner-workings-1c3054cd1591"><img alt="Generic badge" src="https://img.shields.io/badge/Article-Medium-black.svg" /></a> <a href="https://gmihaila.github.io/tutorial_notebooks/bert_inner_workings/"><img alt="Generic badge" src="https://img.shields.io/badge/Blog-Post-blue.svg" /></a></td>
</tr>
<tr>
<td align="left"><strong>🎱 GPT2 For Text Classification using Hugging Face 🤗 Transformers</strong></td>
<td align="left"><em>Complete tutorial on how to use GPT2 for text classification.</em></td>
<td align="left"><a href="https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/gpt2_finetune_classification.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> <a href="https://github.com/gmihaila/ml_things/blob/master/notebooks/pytorch/gpt2_finetune_classification.ipynb"><img alt="Generic badge" src="https://img.shields.io/badge/GitHub-Source-greensvg" /></a> <a href="https://www.dropbox.com/s/6t6kvlewoabwxqw/gpt2_finetune_classification.ipynb?dl=1"><img alt="Generic badge" src="https://img.shields.io/badge/Download-Notebook-red.svg" /></a> <a href="https://gmihaila.medium.com/gpt2-for-text-classification-using-hugging-face-transformers-574555451832"><img alt="Generic badge" src="https://img.shields.io/badge/Article-Medium-black.svg" /></a> <a href="https://gmihaila.github.io/tutorial_notebooks/gpt2_finetune_classification/"><img alt="Generic badge" src="https://img.shields.io/badge/Blog-Post-blue.svg" /></a></td>
</tr>
</tbody>
</table>
<hr />
<h1 id="qa">Q&amp;A<a class="headerlink" href="#qa" title="Permanent link"></a></h1>
<p>Thank you!</p>
<hr />
<h1 id="contact-">Contact 🎣<a class="headerlink" href="#contact-" title="Permanent link"></a></h1>
<p>Let's stay in touch!</p>
<p>🦊 GitHub: <a href="https://github.com/gmihaila">gmihaila</a></p>
<p>🌐 Website: <a href="https://gmihaila.github.io/">gmihaila.github.io</a></p>
<p>👔 LinkedIn: <a href="https://www.linkedin.com/in/mihailageorge/">mihailageorge</a></p>
<p>📓 Medium: <a href="https://gmihaila.medium.com">@gmihaila</a></p>
<p>📬 Email: <a href="mailto:georgemihaila@my.unt.edu.com?subject=GitHub%20Website">georgemihaila@my.unt.edu.com</a></p>
<hr />
<h1 id="resources">Resources<a class="headerlink" href="#resources" title="Permanent link"></a></h1>
<ul>
<li><a href="https://gmihaila.medium.com">@gmihaila</a></li>
<li><a href="https://monkeylearn.com/text-classification/">Text Classification</a></li>
<li><a href="https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04">What is a transformer</a></li>
<li><a href="https://whatsnewinpublishing.com/google-has-made-one-of-the-biggest-leaps-forward-in-the-history-of-search-what-it-means-for-publishers/">Google's Search Engine biggest leap</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/">Understand transformers</a></li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2015 - 2020 <a href="https://github.com/gmihaila"  target="_blank" rel="noopener">George Mihaila</a>

    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    <a href="https://github.com/gmihaila" target="_blank" rel="noopener" title="gmihaila" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    <a href="https://gmihaila.medium.com/" target="_blank" rel="noopener" title="gmihaila" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262Zm288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095Zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271Z"/></svg>
    </a>
  
    
    
    <a href="https://www.linkedin.com/in/mihailageorge" target="_blank" rel="noopener" title="mihailageorge" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["instant", "header.autohide", "search.highlight", "search.share", "search.suggest", "navigation.top", "navigation.expand"], "search": "../../../assets/javascripts/workers/search.16e2a7d4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.d6c3db9e.min.js"></script>
      
    
  </body>
</html>