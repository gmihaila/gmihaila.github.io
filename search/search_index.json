{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About me George is a PhD candidate at University of North Texas, in the Department of Computer Science. He received his Bachelors Degree in Electrical Engineering in his home-country, Romania, and he got his Masters Degree in Computer Science in the United States. He worked three summers for State Farm as a Data Scientist (DS) and Machine Learning (ML) Engineer, and he has more than 4 years of experience in Natural Language Processing (NLP), Computer Vision (CV), Deep Learning (DL) and Reinforcement Learning (RL). He is currently working towards his doctoral thesis in casual dialog generation with persona. Competencies: data science, machine learning, deep learning, high performance computing, Tensorflow2.0, PyTorch, Python, R. Current Position Teaching Assistant Computer Science | University of North Texas August 2020 \u2013 Present Artificial Intelligence \u2013 Machine Learning Researcher University of North Texas (UNT) | Department of Computer Science Jan 2017 - Present Reading How Not to Be Wrong: The Power of Mathematical Thinking by Jordan Ellenberg Contact GitHub: gmihaila LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila","title":"About"},{"location":"#about-me","text":"George is a PhD candidate at University of North Texas, in the Department of Computer Science. He received his Bachelors Degree in Electrical Engineering in his home-country, Romania, and he got his Masters Degree in Computer Science in the United States. He worked three summers for State Farm as a Data Scientist (DS) and Machine Learning (ML) Engineer, and he has more than 4 years of experience in Natural Language Processing (NLP), Computer Vision (CV), Deep Learning (DL) and Reinforcement Learning (RL). He is currently working towards his doctoral thesis in casual dialog generation with persona. Competencies: data science, machine learning, deep learning, high performance computing, Tensorflow2.0, PyTorch, Python, R.","title":"About me"},{"location":"#current-position","text":"","title":"Current Position"},{"location":"#teaching-assistant","text":"Computer Science | University of North Texas August 2020 \u2013 Present","title":"Teaching Assistant"},{"location":"#artificial-intelligence--machine-learning-researcher","text":"University of North Texas (UNT) | Department of Computer Science Jan 2017 - Present","title":"Artificial Intelligence \u2013 Machine Learning Researcher"},{"location":"#reading","text":"How Not to Be Wrong: The Power of Mathematical Thinking by Jordan Ellenberg","title":"Reading"},{"location":"#contact","text":"GitHub: gmihaila LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila","title":"Contact"},{"location":"stuff/","text":"Welcome to Talon Hight-Performance Computing Documentation BTW this is mobile friendly and easy to deploy on GitHub Pages http://UNT-RITS.github.io/talon_sop/_build/html/index.html Task List Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Bundle code together Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } } Python Starter Content import tensorflow as tf import os print ( \"Hello world\" ) import tensorflow as tf import os print ( \"Hello world\" ) Formulas \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons \u2013 that's not all, we can also use GitHub's Octicons Foot Note Lorem ipsum 1 dolor sit amet R Starter Content Job Submit Content Tables Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Welcome to Talon Hight-Performance Computing Documentation"},{"location":"stuff/#welcome-to-talon-hight-performance-computing-documentation","text":"BTW this is mobile friendly and easy to deploy on GitHub Pages http://UNT-RITS.github.io/talon_sop/_build/html/index.html","title":"Welcome to Talon Hight-Performance Computing Documentation"},{"location":"stuff/#task-list","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Task List"},{"location":"stuff/#bundle-code-together","text":"Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } }","title":"Bundle code together"},{"location":"stuff/#python-starter","text":"","title":"Python Starter"},{"location":"stuff/#content","text":"import tensorflow as tf import os print ( \"Hello world\" ) import tensorflow as tf import os print ( \"Hello world\" )","title":"Content"},{"location":"stuff/#formulas","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons \u2013 that's not all, we can also use GitHub's Octicons","title":"Formulas"},{"location":"stuff/#foot-note","text":"Lorem ipsum 1 dolor sit amet","title":"Foot Note"},{"location":"stuff/#r-starter","text":"","title":"R Starter"},{"location":"stuff/#content_1","text":"","title":"Content"},{"location":"stuff/#job-submit","text":"","title":"Job Submit"},{"location":"stuff/#content_2","text":"","title":"Content"},{"location":"stuff/#tables","text":"Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Tables"},{"location":"activities/activities/","text":"Activities GPU Technology Conference (GTC) Code | Slides march 20-22, 2020 | San Jose California GPUs in Natural Language Processing Invited to participate in conference Instructor-Led Training at the GPU Technology Conference (GTC). Title: T22128: GPUs in Natural Language Processing Session Type: Instructor-Led Training Duration: 1 Hour 45 Minutes Find New Sentiments in Text Data Code | Slides march 3, 2020 | University of North Texas DP F223 Using text embedding and clustering methods to find new sentiments in text data. Use imdb movie reviews dataset and see how positive-negative sentiments can be used to find other sentiments. Use Tensorflow 2, Word Embeddings, Contextual Embeddings. We'll do clustering and find best number of cluster in data we don't know anything about. We'll do pretty plots to interpret results! We'll do all this on our UNT Talon Supercomputer! Duration: 1 Hour 15 Minutes Intro to using Git Lab Info | Slides may 3, 2019 | University of North Texas DP F285 How to use UNT HPC GitLab server Ever wondered what is Git and why every software company uses it? We'll teach some basic commands in Git Lab (platform used in almost every software company). And Yes, UNT has a Git Lab! Duration: 1 Hour 15 Minutes Intro to Word Embeddings - NLP Tools on Talon Info | Slides may 2, 2019 | University of North Texas DP F223 Natural Language Processing on Talon HPC Ever wondered or curious about natural language processing? How do you load a text file, how do you 'clean' it, how do you make use of it? We will learn all that and at the end you will build a Neural Network model that can predict a sentiment of a Movie Review. All this on Talon! Duration: 1 Hour 15 Minutes Using Python and Jupyter Notebooks on Talon Info | Slides may 1, 2019 | University of North Texas DP F223 Hands on work on Talon UNT HPC This workshop will go over some basic instruction about Python and Jupyter Notebooks. We will learn how to see what libraries we have available and how to install your own.Learn how to create a virtual environment, and most exciting build a simple Neural Network model to predict diabetes! All this on Talon! Duration: 1 Hour 15 Minutes Machine Learning - Neural Networks on Talon Info | Slides november 30, 2018 | University of North Texas GAB 535 What are Neural Networks, and why everyone is talking about them? This workshop is meant to give a general overview of why Neural Networks and Deep Learning are getting so much attention and help shed some light on how they work and what applications are possible because of them. Duration: 1 Hour 15 Minutes","title":"Activities"},{"location":"activities/activities/#activities","text":"","title":"Activities"},{"location":"activities/activities/#gpu-technology-conference-gtc","text":"Code | Slides march 20-22, 2020 | San Jose California GPUs in Natural Language Processing Invited to participate in conference Instructor-Led Training at the GPU Technology Conference (GTC). Title: T22128: GPUs in Natural Language Processing Session Type: Instructor-Led Training Duration: 1 Hour 45 Minutes","title":"GPU Technology Conference (GTC)"},{"location":"activities/activities/#find-new-sentiments-in-text-data","text":"Code | Slides march 3, 2020 | University of North Texas DP F223 Using text embedding and clustering methods to find new sentiments in text data. Use imdb movie reviews dataset and see how positive-negative sentiments can be used to find other sentiments. Use Tensorflow 2, Word Embeddings, Contextual Embeddings. We'll do clustering and find best number of cluster in data we don't know anything about. We'll do pretty plots to interpret results! We'll do all this on our UNT Talon Supercomputer! Duration: 1 Hour 15 Minutes","title":"Find New Sentiments in Text Data"},{"location":"activities/activities/#intro-to-using-git-lab","text":"Info | Slides may 3, 2019 | University of North Texas DP F285 How to use UNT HPC GitLab server Ever wondered what is Git and why every software company uses it? We'll teach some basic commands in Git Lab (platform used in almost every software company). And Yes, UNT has a Git Lab! Duration: 1 Hour 15 Minutes","title":"Intro to using Git Lab"},{"location":"activities/activities/#intro-to-word-embeddings---nlp-tools-on-talon","text":"Info | Slides may 2, 2019 | University of North Texas DP F223 Natural Language Processing on Talon HPC Ever wondered or curious about natural language processing? How do you load a text file, how do you 'clean' it, how do you make use of it? We will learn all that and at the end you will build a Neural Network model that can predict a sentiment of a Movie Review. All this on Talon! Duration: 1 Hour 15 Minutes","title":"Intro to Word Embeddings - NLP Tools on Talon"},{"location":"activities/activities/#using-python-and-jupyter-notebooks-on-talon","text":"Info | Slides may 1, 2019 | University of North Texas DP F223 Hands on work on Talon UNT HPC This workshop will go over some basic instruction about Python and Jupyter Notebooks. We will learn how to see what libraries we have available and how to install your own.Learn how to create a virtual environment, and most exciting build a simple Neural Network model to predict diabetes! All this on Talon! Duration: 1 Hour 15 Minutes","title":"Using Python and Jupyter Notebooks on Talon"},{"location":"activities/activities/#machine-learning---neural-networks-on-talon","text":"Info | Slides november 30, 2018 | University of North Texas GAB 535 What are Neural Networks, and why everyone is talking about them? This workshop is meant to give a general overview of why Neural Networks and Deep Learning are getting so much attention and help shed some light on how they work and what applications are possible because of them. Duration: 1 Hour 15 Minutes","title":"Machine Learning - Neural Networks on Talon"},{"location":"resume/resume/","text":"Resume pdf Summary Interest areas: Reinforcement Learning, Computer Vision, Big Data, Scaling Machine Learning, Parallel Processing. Main Research areas: Deep Learning, Natural Language Processing Thesis project: Casual Dialog Generation with different personas . For example, imagine a chatbot from the TV Series Friends that talks like your favorite character (Joey, Chandler etc.) Key Assets: Being a PhD student set me up to work with little guidance and being self-taught . How I like to takle problems: always start small and with good understating of the problem, build something simple that works and keep adding and testing on that until reaching the goal. Experience Data Scientist Intern State Farm | Enterprise Data & Analytics Align machine learning and artificial inteligence research to bussiness needs. Created a Reinforcement Learning (RL) proof of concept application. Built a framework around RL that makes it expandable into other business areas. Created custom data environments to easily train and deploy RL models. May 2020 \u2013 July 2020 Data Scientist \u2013 Machine Learning Engineer University of North Texas (UNT) | Research Information Technology Supported research project data science and Machine Learning (ML) needs. Maintain latest ML frameworks for UNT\u2019s High Performance Computing (HPC). Created workshops and tutorials about cloud computing HPC services. Sep 2018 - May 2020 Machine Learning Engineer Intern State Farm | Enterprise Data & Analytics Automated model risk validation reducing deployment timeline by 90%. Deployed Python package for automated model building and data analysis. Built a user interface for automated model comparison with CI/CD pipeline. May 2019 \u2013 July 2019 Data Scientist Intern State Farm | Enterprise &Data Analytics Introduced GPUs in ML tutorials that speed up training by over 50%. Increased model performance used in claims process by 5% using text data. Built Deep Learning Optical Character Recognition with over 75% accuracy. May 2018 \u2013 August 2018 Teaching Assistant \u2013 Computer Science University of North Texas (UNT) | Department of Computer Science Built and debugged C/C++ coding assignments and exams. Assisted Computer Science students as a peer mentor and grader. Sep 2017 \u2013 May 2018 Artificial Intelligence \u2013 Machine Learning Researcher University of North Texas (UNT) | Department of Computer Science Built state of the art Deep Learning models in Natural Language Processing. Implement concepts from research papers related to state of the art chatbots. Jan 2017 - Present Education PhD in Computer Science University of North Texas (UNT) | Department of Computer Science Focus on Natural Language Processing, Deep Generative Modeling, Companionable Robots, Persona Detection. May 2019 - Present | GPA 4.0 Masters in Computer Science University of North Texas (UNT) | Department of Computer Science Specialized in Machine Learning, Natural Language Processing, Big Data, Data Mining, Computer Vision and Deep Reinforcement Learning. May 2019 | GPA 3.9 Skills Reference Dr. Rodney D. Nielsen Associate Professor - Director HiLT Lab University of North Texas \u2013 Computer Science Engineer Email: Rodney.Nielsen@UNT.edu","title":"Resume"},{"location":"resume/resume/#resume--pdf","text":"","title":"Resume  pdf"},{"location":"resume/resume/#summary","text":"Interest areas: Reinforcement Learning, Computer Vision, Big Data, Scaling Machine Learning, Parallel Processing. Main Research areas: Deep Learning, Natural Language Processing Thesis project: Casual Dialog Generation with different personas . For example, imagine a chatbot from the TV Series Friends that talks like your favorite character (Joey, Chandler etc.) Key Assets: Being a PhD student set me up to work with little guidance and being self-taught . How I like to takle problems: always start small and with good understating of the problem, build something simple that works and keep adding and testing on that until reaching the goal.","title":"Summary"},{"location":"resume/resume/#experience","text":"","title":"Experience"},{"location":"resume/resume/#data-scientist-intern","text":"State Farm | Enterprise Data & Analytics Align machine learning and artificial inteligence research to bussiness needs. Created a Reinforcement Learning (RL) proof of concept application. Built a framework around RL that makes it expandable into other business areas. Created custom data environments to easily train and deploy RL models. May 2020 \u2013 July 2020","title":"Data Scientist Intern"},{"location":"resume/resume/#data-scientist--machine-learning-engineer","text":"University of North Texas (UNT) | Research Information Technology Supported research project data science and Machine Learning (ML) needs. Maintain latest ML frameworks for UNT\u2019s High Performance Computing (HPC). Created workshops and tutorials about cloud computing HPC services. Sep 2018 - May 2020","title":"Data Scientist \u2013 Machine Learning Engineer"},{"location":"resume/resume/#machine-learning-engineer-intern","text":"State Farm | Enterprise Data & Analytics Automated model risk validation reducing deployment timeline by 90%. Deployed Python package for automated model building and data analysis. Built a user interface for automated model comparison with CI/CD pipeline. May 2019 \u2013 July 2019","title":"Machine Learning Engineer Intern"},{"location":"resume/resume/#data-scientist-intern_1","text":"State Farm | Enterprise &Data Analytics Introduced GPUs in ML tutorials that speed up training by over 50%. Increased model performance used in claims process by 5% using text data. Built Deep Learning Optical Character Recognition with over 75% accuracy. May 2018 \u2013 August 2018","title":"Data Scientist Intern"},{"location":"resume/resume/#teaching-assistant--computer-science","text":"University of North Texas (UNT) | Department of Computer Science Built and debugged C/C++ coding assignments and exams. Assisted Computer Science students as a peer mentor and grader. Sep 2017 \u2013 May 2018","title":"Teaching Assistant \u2013 Computer Science"},{"location":"resume/resume/#artificial-intelligence--machine-learning-researcher","text":"University of North Texas (UNT) | Department of Computer Science Built state of the art Deep Learning models in Natural Language Processing. Implement concepts from research papers related to state of the art chatbots. Jan 2017 - Present","title":"Artificial Intelligence \u2013 Machine Learning Researcher"},{"location":"resume/resume/#education","text":"","title":"Education"},{"location":"resume/resume/#phd-in-computer-science","text":"University of North Texas (UNT) | Department of Computer Science Focus on Natural Language Processing, Deep Generative Modeling, Companionable Robots, Persona Detection. May 2019 - Present | GPA 4.0","title":"PhD in Computer Science"},{"location":"resume/resume/#masters-in-computer-science","text":"University of North Texas (UNT) | Department of Computer Science Specialized in Machine Learning, Natural Language Processing, Big Data, Data Mining, Computer Vision and Deep Reinforcement Learning. May 2019 | GPA 3.9","title":"Masters in Computer Science"},{"location":"resume/resume/#skills","text":"","title":"Skills"},{"location":"resume/resume/#reference","text":"","title":"Reference"},{"location":"resume/resume/#dr-rodney-d-nielsen","text":"Associate Professor - Director HiLT Lab University of North Texas \u2013 Computer Science Engineer Email: Rodney.Nielsen@UNT.edu","title":"Dr. Rodney D. Nielsen"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/","text":"Fine-tune Transformers in PyTorch using Hugging Face Transformers Info This notebook is designed to use a pretrained transformers model and fine-tune it on a classification task. The focus of this tutorial will be on the code itself and how to adjust it to your needs. This notebook is using the AutoClasses from transformer by Hugging Face functionality. This functionality can guess a model's configuration, tokenizer and architecture just by passing in the model's name. This allows for code reusability on a large number of transformers models! What should I know for this notebook? I provided enough instructions and comments to be able to follow along with minimum Python coding knowledge. Since I am using PyTorch to fine-tune our transformers models any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too. How to use this notebook? I built this notebook with reusability in mind. The way I load the dataset into the PyTorch Dataset class is pretty standard and can be easily reused for any other dataset. The only modifications needed to use your own dataset will be in reading in the dataset inside the MovieReviewsDataset class which uses PyTorch Dataset . The DataLoader will return a dictionary of batch inputs format so that it can be fed straight to the model using the statement: outputs = model(**batch) . As long as this statement holds, the rest of the code will work! What transformers models work with this notebook? There are rare cases where I use a different model than Bert when dealing with classification from text data. When there is a need to run a different transformer model architecture, which one would work with this code? Since the name of the notebooks is finetune_transformers it should work with more than one type of transformers. I ran this notebook across all the pretrained models found on Hugging Face Transformer. This way you know ahead of time if the model you plan to use works with this code without any modifications. The list of pretrained transformers models that work with this notebook can be found here . There are 73 models that worked and 33 models that failed to work with this notebook. Dataset This notebook will cover fine-tune transformers for binary classification task. I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with. Coding Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations. Downloads Download the Large Movie Review Dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Installs transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. Give it a try! Code Cell: # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Output: Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .9MB 6 .7MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 890kB 48 .9MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 .1MB 49 .0MB/s Building wheel for transformers ( PEP 517 ) ... done Building wheel for sacremoses ( setup.py ) ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 5 .2MB/s Building wheel for ml-things ( setup.py ) ... done Building wheel for ftfy ( setup.py ) ... done Imports Import all needed libraries for this notebook. Declare parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. epochs - Number of training epochs (authors recommend between 2 and 4). batches - Number of batches - depending on the max sequence length and GPU memory. For 512 sequence length a batch of 10 USUALY works without cuda memory issues. For small sequence length can try batch of 32 or higher. max_length - Pad or truncate text sequences to a specific length. I will set it to 60 to speed up training. device - Look for gpu to use. Will use cpu by default if no gpu found. model_name_or_path - Name of transformers model - will use already pretrained model. Path of transformer model - will load your own model from local disk. I always like to start off with bert-base-cased : 12-layer, 768-hidden, 12-heads, 109M parameters. Trained on cased English text. labels_ids - Dictionary of labels and their id - this will be used to convert string labels to numbers. n_labels - How many labels are we using in this dataset. This is used to decide size of classification head. Code Cell: import io import os import torch from tqdm.notebook import tqdm from torch.utils.data import Dataset , DataLoader from ml_things import plot_dict , plot_confusion_matrix , fix_text from sklearn.metrics import classification_report , accuracy_score from transformers import ( AutoConfig , AutoModelForSequenceClassification , AutoTokenizer , AdamW , get_linear_schedule_with_warmup , set_seed , ) # Set seed for reproducibility, set_seed ( 123 ) # Number of training epochs (authors recommend between 2 and 4) epochs = 4 # Number of batches - depending on the max sequence length and GPU memory. # For 512 sequence length batch of 10 works without cuda memory issues. # For small sequence length can try batch of 32 or higher. batches = 32 # Pad or truncate text sequences to a specific length # if `None` it will use maximum sequence of word piece tokens allowed by model. max_length = 60 # Look for gpu to use. Will use `cpu` by default if no gpu found. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) # Name of transformers model - will use already pretrained model. # Path of transformer model - will load your own model from local disk. model_name_or_path = 'bert-base-cased' # Dicitonary of labels and their id - this will be used to convert. # String labels to number ids. labels_ids = { 'neg' : 0 , 'pos' : 1 } # How many labels are we using in training. # This is used to decide size of classification head. n_labels = len ( labels_ids ) Helper Functions I like to keep all Classes and functions that will be used in this notebook under this section to help maintain a clean look of the notebook: MovieReviewsDataset(Dataset) If you worked with PyTorch before this is pretty standard. We need this class to read in our dataset, parse it, use tokenizer that transforms text into numbers and get it into a nice format to be fed to the model. Lucky for use, Hugging Face thought of everything and made the tokenizer do all the heavy lifting (split text into tokens, padding, truncating, encode text into numbers) and is very easy to use! In this class I only need to read in the content of each file, use fix_text to fix any Unicode problems and keep track of positive and negative sentiments. I will append all texts and labels in lists that later I will feed to the tokenizer and to the label ids to transform everything into numbers. There are three main parts of this PyTorch Dataset class: init () where we read in the dataset and transform text and labels into numbers. len () where we need to return the number of examples we read in. This is used when calling len(MovieReviewsDataset()) . getitem() always takes as an input an int value that represents which example from our examples to return from our dataset. If a value of 3 is passed, we will return the example form our dataset at position 3. It needs to return an object with the format that can be fed to our model. Luckily our tokenizer does that for us and returns a dictionary of variables ready to be fed to the model in this way: model(**inputs) . Code Cell: class MovieReviewsDataset ( Dataset ): r \"\"\"PyTorch Dataset class for loading data. This is where the data parsing happens and where the text gets encoded using loaded tokenizer. This class is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: path (:obj:`str`): Path to the data partition. use_tokenizer (:obj:`transformers.tokenization_?`): Transformer type tokenizer used to process raw text into numbers. labels_ids (:obj:`dict`): Dictionary to encode any labels names into numbers. Keys map to labels names and Values map to number associated to those labels. max_sequence_len (:obj:`int`, `optional`) Value to indicate the maximum desired sequence to truncate or pad text sequences. If no value is passed it will used maximum sequence size supported by the tokenizer and model. \"\"\" def __init__ ( self , path , use_tokenizer , labels_ids , max_sequence_len = None ): # Check if path exists. if not os . path . isdir ( path ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) # Check max sequence length. max_sequence_len = use_tokenizer . max_len if max_sequence_len is None else max_sequence_len texts = [] labels = [] print ( 'Reading partitions...' ) # Since the labels are defined by folders with data we loop # through each label. for label , label_id , in tqdm ( labels_ids . items ()): sentiment_path = os . path . join ( path , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:10] # Sample for debugging. print ( 'Reading %s files...' % label ) # Go through each file and read its content. for file_name in tqdm ( files_names ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. texts . append ( content ) # Save encode labels. labels . append ( label_id ) # Number of exmaples. self . n_examples = len ( labels ) # Use tokenizer on texts. This can take a while. print ( 'Using tokenizer on all texts. This can take a while...' ) self . inputs = use_tokenizer ( texts , add_special_tokens = True , truncation = True , padding = True , return_tensors = 'pt' , max_length = max_sequence_len ) # Get maximum sequence length. self . sequence_len = self . inputs [ 'input_ids' ] . shape [ - 1 ] print ( 'Texts padded or truncated to %d length!' % self . sequence_len ) # Add labels. self . inputs . update ({ 'labels' : torch . tensor ( labels )}) print ( 'Finished! \\n ' ) return def __len__ ( self ): r \"\"\"When used `len` return the number of examples. \"\"\" return self . n_examples def __getitem__ ( self , item ): r \"\"\"Given an index return an example from the position. Arguments: item (:obj:`int`): Index position to pick an example to return. Returns: :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model. It holddes the statement `model(**Returned Dictionary)`. \"\"\" return { key : self . inputs [ key ][ item ] for key in self . inputs . keys ()} train(dataloader, optimizer_, scheduler_, device_) I created this function to perform a full pass through the DataLoader object (the DataLoader object is created from our Dataset* type object using the **MovieReviewsDataset class). This is basically one epoch train through the entire dataset. The dataloader is created from PyTorch DataLoader which takes the object created from MovieReviewsDataset class and puts each example in batches. This way we can feed our model batches of data! The optimizer_ and scheduler_ are very common in PyTorch. They are required to update the parameters of our model and update our learning rate during training. There is a lot more than that but I won't go into details. This can actually be a huge rabbit hole since A LOT happens behind these functions that we don't need to worry. Thank you PyTorch! In the process we keep track of the actual labels and the predicted labels along with the loss. def train ( dataloader , optimizer_ , scheduler_ , device_ ): r \"\"\" Train pytorch model on a single pass through the data loader. It will use the global variable `model` which is the transformer model loaded on `_device` that we want to train on. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. optimizer_ (:obj:`transformers.optimization.AdamW`): Optimizer used for training. scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`): PyTorch scheduler. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss]. \"\"\" # Use global variable for model. global model # Tracking variables. predictions_labels = [] true_labels = [] # Total loss for this epoch. total_loss = 0 # Put the model into training mode. model . train () # For each batch of training data... for batch in tqdm ( dataloader , total = len ( dataloader )): # Add original labels - use later for evaluation. true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Always clear any previously calculated gradients before performing a # backward pass. model . zero_grad () # Perform a forward pass (evaluate the model on this training batch). # This will return the loss (rather than the model output) because we # have provided the `labels`. # The documentation for this a bert model function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to calculate training accuracy. loss , logits = outputs [: 2 ] # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # Perform a backward pass to calculate the gradients. loss . backward () # Clip the norm of the gradients to 1.0. # This is to help prevent the \"exploding gradients\" problem. torch . nn . utils . clip_grad_norm_ ( model . parameters (), 1.0 ) # Update parameters and take a step using the computed gradient. # The optimizer dictates the \"update rule\"--how the parameters are # modified based on their gradients, the learning rate, etc. optimizer . step () # Update the learning rate. scheduler . step () # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Convert these logits to list of predicted labels values. predictions_labels += logits . argmax ( axis =- 1 ) . flatten () . tolist () # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediction for future evaluations. return true_labels , predictions_labels , avg_epoch_loss validation(dataloader, device_) I implemented this function in a very similar way as train but without the parameters update, backward pass and gradient decent part. We don't need to do all of those VERY computationally intensive tasks because we only care about our model's predictions. I use the DataLoader in a similar way as in train to get out batches to feed to our model. In the process I keep track of the actual labels and the predicted labels along with the loss. def validation ( dataloader , device_ ): r \"\"\"Validation function to evaluate model performance on a separate set of data. This function will return the true and predicted labels so we can use later to evaluate the model's performance. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss] \"\"\" # Use global variable for model. global model # Tracking variables predictions_labels = [] true_labels = [] #total loss for this epoch. total_loss = 0 # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model . eval () # Evaluate data for one epoch for batch in tqdm ( dataloader , total = len ( dataloader )): # add original labels true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Telling the model not to compute or store gradients, saving memory and # speeding up validation with torch . no_grad (): # Forward pass, calculate logit predictions. # This will return the logits rather than the loss because we have # not provided labels. # token_type_ids is the same as the \"segment ids\", which # differentiates sentence 1 and 2 in 2-sentence tasks. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to to calculate training accuracy. loss , logits = outputs [: 2 ] # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # get predicitons to list predict_content = logits . argmax ( axis =- 1 ) . flatten () . tolist () # update list predictions_labels += predict_content # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediciton for future evaluations. return true_labels , predictions_labels , avg_epoch_loss Load Model and Tokenizer Loading the three essential parts of the pretrained transformers: configuration , tokenizer and model . I also need to load the model on the device I'm planning to use (GPU / CPU). Since I use the AutoClass functionality from Hugging Face I only need to worry about the model's name as input and the rest is handled by the transformers library. Code Cell: # Get model configuration. print ( 'Loading configuraiton...' ) model_config = AutoConfig . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , num_labels = n_labels ) # Get model's tokenizer. print ( 'Loading tokenizer...' ) tokenizer = AutoTokenizer . from_pretrained ( pretrained_model_name_or_path = model_name_or_path ) # Get the actual model. print ( 'Loading model...' ) model = AutoModelForSequenceClassification . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , config = model_config ) # Load model to defined device. model . to ( device ) print ( 'Model loaded to ` %s `' % device ) Output: Loading configuraiton... Loading tokenizer... Loading model... Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: [ 'cls.predictions.bias' , 'cls.predictions.transform.dense.weight' , 'cls.predictions.transform.dense.bias' , 'cls.predictions.decoder.weight' , 'cls.seq_relationship.weight' , 'cls.seq_relationship.bias' , 'cls.predictions.transform.LayerNorm.weight' , 'cls.predictions.transform.LayerNorm.bias' ] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture ( e.g. initializing a BertForSequenceClassification model from a BertForPretraining model ) . - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical ( initializing a BertForSequenceClassification model from a BertForSequenceClassification model ) . Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: [ 'classifier.weight' , 'classifier.bias' ] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Model loaded to ` cuda ` Dataset and DataLoader This is wehere I create the PyTorch Dataset and DataLoader objects that will be used to feed data into our model. This is where I use the MovieReviewsDataset class and create the dataset variables. Since data is partitioned for both train and test I will create a PyTorch Dataset and PyTorch DataLoader object for train and test. ONLY for simplicity I will use the test as validation. In practice NEVER USE THE TEST DATA FOR VALIDATION! Code Cell: print ( 'Dealing with Train...' ) # Create pytorch dataset. train_dataset = MovieReviewsDataset ( path = '/content/aclImdb/train' , use_tokenizer = tokenizer , labels_ids = labels_ids , max_sequence_len = max_length ) print ( 'Created `train_dataset` with %d examples!' % len ( train_dataset )) # Move pytorch dataset into dataloader. train_dataloader = DataLoader ( train_dataset , batch_size = batches , shuffle = True ) print ( 'Created `train_dataloader` with %d batches!' % len ( train_dataloader )) print () print ( 'Dealing with ...' ) # Create pytorch dataset. valid_dataset = MovieReviewsDataset ( path = '/content/aclImdb/test' , use_tokenizer = tokenizer , labels_ids = labels_ids , max_sequence_len = max_length ) print ( 'Created `valid_dataset` with %d examples!' % len ( valid_dataset )) # Move pytorch dataset into dataloader. valid_dataloader = DataLoader ( valid_dataset , batch_size = batches , shuffle = False ) print ( 'Created `eval_dataloader` with %d batches!' % len ( valid_dataloader )) Output: Dealing with Train... Reading partitions... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :34< 00 :00, 17 .28s/it ] Reading neg files... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 12500 /12500 [ 00 :34< 00 :00, 362 .01it/s ] Reading pos files... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 12500 /12500 [ 00 :23< 00 :00, 534 .34it/s ] Using tokenizer on all texts. This can take a while ... Texts padded or truncated to 40 length! Finished! Created ` train_dataset ` with 25000 examples! Created ` train_dataloader ` with 25000 batches! Dealing with ... Reading partitions... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 01 :28< 00 :00, 44 .13s/it ] Reading neg files... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 12500 /12500 [ 01 :28< 00 :00, 141 .71it/s ] Reading pos files... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 12500 /12500 [ 01 :17< 00 :00, 161 .60it/s ] Using tokenizer on all texts. This can take a while ... Texts padded or truncated to 40 length! Finished! Created ` valid_dataset ` with 25000 examples! Created ` eval_dataloader ` with 25000 batches! Train I create an optimizer and scheduler that will be used by PyTorch in training. I loop through the number of defined epochs and call the train and validation functions. I will output similar info after each epoch as in Keras: train_loss: - val_loss: - train_acc: - valid_acc . After training, I plot the train and validation loss and accuracy curves to check how the training went. Code Cell: # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the 'W' stands for 'Weight Decay fix\" optimizer = AdamW ( model . parameters (), lr = 2e-5 , # args.learning_rate - default is 5e-5, our notebook had 2e-5 eps = 1e-8 # args.adam_epsilon - default is 1e-8. ) # Total number of training steps is number of batches * number of epochs. total_steps = len ( train_dataset ) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup ( optimizer , num_warmup_steps = 0 , # Default value in run_glue.py num_training_steps = total_steps ) # Store the average loss after each epoch so we can plot them. all_loss = { 'train_loss' :[], 'val_loss' :[]} all_acc = { 'train_acc' :[], 'val_acc' :[]} # Loop through each epoch. print ( 'Epoch' ) for epoch in tqdm ( range ( epochs )): print () print ( 'Training on batches...' ) # Perform one full pass over the training set. train_labels , train_predict , train_loss = train ( train_dataloader , optimizer , scheduler , device ) train_acc = accuracy_score ( train_labels , train_predict ) # Get prediction form model on validation data. print ( 'Validation on batches...' ) valid_labels , valid_predict , val_loss = validation ( valid_dataloader , device ) val_acc = accuracy_score ( valid_labels , valid_predict ) # Print loss and accuracy values to see how training evolves. print ( \" train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f \" % ( train_loss , val_loss , train_acc , val_acc )) print () # Store the loss value for plotting the learning curve. all_loss [ 'train_loss' ] . append ( train_loss ) all_loss [ 'val_loss' ] . append ( val_loss ) all_acc [ 'train_acc' ] . append ( train_acc ) all_acc [ 'val_acc' ] . append ( val_acc ) # Plot loss curves. plot_dict ( all_loss , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) # Plot accuracy curves. plot_dict ( all_acc , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) Output: Epoch 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 /4 [ 13 :49< 00 :00, 207 .37s/it ] Training on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 02 :40< 00 :00, 4 .86it/s ] Validation on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 00 :46< 00 :00, 16 .80it/s ] train_loss: 0 .44816 - val_loss: 0 .38655 - train_acc: 0 .78372 - valid_acc: 0 .81892 Training on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 02 :40< 00 :00, 4 .86it/s ] Validation on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 02 :13< 00 :00, 5 .88it/s ] train_loss: 0 .29504 - val_loss: 0 .43493 - train_acc: 0 .87352 - valid_acc: 0 .82360 Training on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 02 :40< 00 :00, 4 .87it/s ] Validation on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 01 :43< 00 :00, 7 .58it/s ] train_loss: 0 .16901 - val_loss: 0 .48433 - train_acc: 0 .93544 - valid_acc: 0 .82624 Training on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 02 :40< 00 :00, 4 .87it/s ] Validation on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 00 :46< 00 :00, 16 .79it/s ] train_loss: 0 .09816 - val_loss: 0 .73001 - train_acc: 0 .96936 - valid_acc: 0 .82144 It looks like a little over one epoch is enough training for this model and dataset. Evaluate When dealing with classification it's useful to look at precision recall and f1 score. Another good thing to look at when evaluating the model is the confusion matrix. Code Cell: # Get prediction form model on validation data. This is where you should use # your test data. true_labels , predictions_labels , avg_epoch_loss = validation ( valid_dataloader , device ) # Create the evaluation report. evaluation_report = classification_report ( true_labels , predictions_labels , labels = list ( labels_ids . values ()), target_names = list ( labels_ids . keys ())) # Show the evaluation report. print ( evaluation_report ) # Plot confusion matrix. plot_confusion_matrix ( y_true = true_labels , y_pred = predictions_labels , classes = list ( labels_ids . keys ()), normalize = True , magnify = 3 , ); Output: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 00 :46< 00 :00, 16 .77it/s ] precision recall f1-score support neg 0 .83 0 .81 0 .82 12500 pos 0 .81 0 .83 0 .82 12500 accuracy 0 .82 25000 macro avg 0 .82 0 .82 0 .82 25000 weighted avg 0 .82 0 .82 0 .82 25000 Normalized confusion matrix Results are not great, but for this tutorial we are not interested in performance. Final Note If you made it this far Congrats and Thank you for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you have 1 minute please give me a feedback in the comments. If you see something wrong please let me know by opening an issue on my ml_things GitHub repository! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can. Contact GitHub: gmihaila Website: gmihaila.github.io LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila Thank you! Find out more About Me .","title":"Finetune Transformers"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#fine-tune-transformers-in-pytorch-using-hugging-face-transformers","text":"","title":"Fine-tune Transformers in PyTorch using Hugging Face Transformers"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#info","text":"This notebook is designed to use a pretrained transformers model and fine-tune it on a classification task. The focus of this tutorial will be on the code itself and how to adjust it to your needs. This notebook is using the AutoClasses from transformer by Hugging Face functionality. This functionality can guess a model's configuration, tokenizer and architecture just by passing in the model's name. This allows for code reusability on a large number of transformers models!","title":"Info"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#what-should-i-know-for-this-notebook","text":"I provided enough instructions and comments to be able to follow along with minimum Python coding knowledge. Since I am using PyTorch to fine-tune our transformers models any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too.","title":"What should I know for this notebook?"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#how-to-use-this-notebook","text":"I built this notebook with reusability in mind. The way I load the dataset into the PyTorch Dataset class is pretty standard and can be easily reused for any other dataset. The only modifications needed to use your own dataset will be in reading in the dataset inside the MovieReviewsDataset class which uses PyTorch Dataset . The DataLoader will return a dictionary of batch inputs format so that it can be fed straight to the model using the statement: outputs = model(**batch) . As long as this statement holds, the rest of the code will work!","title":"How to use this notebook?"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#what-transformers-models-work-with-this-notebook","text":"There are rare cases where I use a different model than Bert when dealing with classification from text data. When there is a need to run a different transformer model architecture, which one would work with this code? Since the name of the notebooks is finetune_transformers it should work with more than one type of transformers. I ran this notebook across all the pretrained models found on Hugging Face Transformer. This way you know ahead of time if the model you plan to use works with this code without any modifications. The list of pretrained transformers models that work with this notebook can be found here . There are 73 models that worked and 33 models that failed to work with this notebook.","title":"What transformers models work with this notebook?"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#dataset","text":"This notebook will cover fine-tune transformers for binary classification task. I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with.","title":"Dataset"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#coding","text":"Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.","title":"Coding"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#downloads","text":"Download the Large Movie Review Dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz","title":"Downloads"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#installs","text":"transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. Give it a try! Code Cell: # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Output: Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .9MB 6 .7MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 890kB 48 .9MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 .1MB 49 .0MB/s Building wheel for transformers ( PEP 517 ) ... done Building wheel for sacremoses ( setup.py ) ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 5 .2MB/s Building wheel for ml-things ( setup.py ) ... done Building wheel for ftfy ( setup.py ) ... done","title":"Installs"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#imports","text":"Import all needed libraries for this notebook. Declare parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. epochs - Number of training epochs (authors recommend between 2 and 4). batches - Number of batches - depending on the max sequence length and GPU memory. For 512 sequence length a batch of 10 USUALY works without cuda memory issues. For small sequence length can try batch of 32 or higher. max_length - Pad or truncate text sequences to a specific length. I will set it to 60 to speed up training. device - Look for gpu to use. Will use cpu by default if no gpu found. model_name_or_path - Name of transformers model - will use already pretrained model. Path of transformer model - will load your own model from local disk. I always like to start off with bert-base-cased : 12-layer, 768-hidden, 12-heads, 109M parameters. Trained on cased English text. labels_ids - Dictionary of labels and their id - this will be used to convert string labels to numbers. n_labels - How many labels are we using in this dataset. This is used to decide size of classification head. Code Cell: import io import os import torch from tqdm.notebook import tqdm from torch.utils.data import Dataset , DataLoader from ml_things import plot_dict , plot_confusion_matrix , fix_text from sklearn.metrics import classification_report , accuracy_score from transformers import ( AutoConfig , AutoModelForSequenceClassification , AutoTokenizer , AdamW , get_linear_schedule_with_warmup , set_seed , ) # Set seed for reproducibility, set_seed ( 123 ) # Number of training epochs (authors recommend between 2 and 4) epochs = 4 # Number of batches - depending on the max sequence length and GPU memory. # For 512 sequence length batch of 10 works without cuda memory issues. # For small sequence length can try batch of 32 or higher. batches = 32 # Pad or truncate text sequences to a specific length # if `None` it will use maximum sequence of word piece tokens allowed by model. max_length = 60 # Look for gpu to use. Will use `cpu` by default if no gpu found. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) # Name of transformers model - will use already pretrained model. # Path of transformer model - will load your own model from local disk. model_name_or_path = 'bert-base-cased' # Dicitonary of labels and their id - this will be used to convert. # String labels to number ids. labels_ids = { 'neg' : 0 , 'pos' : 1 } # How many labels are we using in training. # This is used to decide size of classification head. n_labels = len ( labels_ids )","title":"Imports"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#helper-functions","text":"I like to keep all Classes and functions that will be used in this notebook under this section to help maintain a clean look of the notebook: MovieReviewsDataset(Dataset) If you worked with PyTorch before this is pretty standard. We need this class to read in our dataset, parse it, use tokenizer that transforms text into numbers and get it into a nice format to be fed to the model. Lucky for use, Hugging Face thought of everything and made the tokenizer do all the heavy lifting (split text into tokens, padding, truncating, encode text into numbers) and is very easy to use! In this class I only need to read in the content of each file, use fix_text to fix any Unicode problems and keep track of positive and negative sentiments. I will append all texts and labels in lists that later I will feed to the tokenizer and to the label ids to transform everything into numbers. There are three main parts of this PyTorch Dataset class: init () where we read in the dataset and transform text and labels into numbers. len () where we need to return the number of examples we read in. This is used when calling len(MovieReviewsDataset()) . getitem() always takes as an input an int value that represents which example from our examples to return from our dataset. If a value of 3 is passed, we will return the example form our dataset at position 3. It needs to return an object with the format that can be fed to our model. Luckily our tokenizer does that for us and returns a dictionary of variables ready to be fed to the model in this way: model(**inputs) . Code Cell: class MovieReviewsDataset ( Dataset ): r \"\"\"PyTorch Dataset class for loading data. This is where the data parsing happens and where the text gets encoded using loaded tokenizer. This class is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: path (:obj:`str`): Path to the data partition. use_tokenizer (:obj:`transformers.tokenization_?`): Transformer type tokenizer used to process raw text into numbers. labels_ids (:obj:`dict`): Dictionary to encode any labels names into numbers. Keys map to labels names and Values map to number associated to those labels. max_sequence_len (:obj:`int`, `optional`) Value to indicate the maximum desired sequence to truncate or pad text sequences. If no value is passed it will used maximum sequence size supported by the tokenizer and model. \"\"\" def __init__ ( self , path , use_tokenizer , labels_ids , max_sequence_len = None ): # Check if path exists. if not os . path . isdir ( path ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) # Check max sequence length. max_sequence_len = use_tokenizer . max_len if max_sequence_len is None else max_sequence_len texts = [] labels = [] print ( 'Reading partitions...' ) # Since the labels are defined by folders with data we loop # through each label. for label , label_id , in tqdm ( labels_ids . items ()): sentiment_path = os . path . join ( path , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:10] # Sample for debugging. print ( 'Reading %s files...' % label ) # Go through each file and read its content. for file_name in tqdm ( files_names ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. texts . append ( content ) # Save encode labels. labels . append ( label_id ) # Number of exmaples. self . n_examples = len ( labels ) # Use tokenizer on texts. This can take a while. print ( 'Using tokenizer on all texts. This can take a while...' ) self . inputs = use_tokenizer ( texts , add_special_tokens = True , truncation = True , padding = True , return_tensors = 'pt' , max_length = max_sequence_len ) # Get maximum sequence length. self . sequence_len = self . inputs [ 'input_ids' ] . shape [ - 1 ] print ( 'Texts padded or truncated to %d length!' % self . sequence_len ) # Add labels. self . inputs . update ({ 'labels' : torch . tensor ( labels )}) print ( 'Finished! \\n ' ) return def __len__ ( self ): r \"\"\"When used `len` return the number of examples. \"\"\" return self . n_examples def __getitem__ ( self , item ): r \"\"\"Given an index return an example from the position. Arguments: item (:obj:`int`): Index position to pick an example to return. Returns: :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model. It holddes the statement `model(**Returned Dictionary)`. \"\"\" return { key : self . inputs [ key ][ item ] for key in self . inputs . keys ()} train(dataloader, optimizer_, scheduler_, device_) I created this function to perform a full pass through the DataLoader object (the DataLoader object is created from our Dataset* type object using the **MovieReviewsDataset class). This is basically one epoch train through the entire dataset. The dataloader is created from PyTorch DataLoader which takes the object created from MovieReviewsDataset class and puts each example in batches. This way we can feed our model batches of data! The optimizer_ and scheduler_ are very common in PyTorch. They are required to update the parameters of our model and update our learning rate during training. There is a lot more than that but I won't go into details. This can actually be a huge rabbit hole since A LOT happens behind these functions that we don't need to worry. Thank you PyTorch! In the process we keep track of the actual labels and the predicted labels along with the loss. def train ( dataloader , optimizer_ , scheduler_ , device_ ): r \"\"\" Train pytorch model on a single pass through the data loader. It will use the global variable `model` which is the transformer model loaded on `_device` that we want to train on. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. optimizer_ (:obj:`transformers.optimization.AdamW`): Optimizer used for training. scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`): PyTorch scheduler. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss]. \"\"\" # Use global variable for model. global model # Tracking variables. predictions_labels = [] true_labels = [] # Total loss for this epoch. total_loss = 0 # Put the model into training mode. model . train () # For each batch of training data... for batch in tqdm ( dataloader , total = len ( dataloader )): # Add original labels - use later for evaluation. true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Always clear any previously calculated gradients before performing a # backward pass. model . zero_grad () # Perform a forward pass (evaluate the model on this training batch). # This will return the loss (rather than the model output) because we # have provided the `labels`. # The documentation for this a bert model function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to calculate training accuracy. loss , logits = outputs [: 2 ] # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # Perform a backward pass to calculate the gradients. loss . backward () # Clip the norm of the gradients to 1.0. # This is to help prevent the \"exploding gradients\" problem. torch . nn . utils . clip_grad_norm_ ( model . parameters (), 1.0 ) # Update parameters and take a step using the computed gradient. # The optimizer dictates the \"update rule\"--how the parameters are # modified based on their gradients, the learning rate, etc. optimizer . step () # Update the learning rate. scheduler . step () # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Convert these logits to list of predicted labels values. predictions_labels += logits . argmax ( axis =- 1 ) . flatten () . tolist () # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediction for future evaluations. return true_labels , predictions_labels , avg_epoch_loss validation(dataloader, device_) I implemented this function in a very similar way as train but without the parameters update, backward pass and gradient decent part. We don't need to do all of those VERY computationally intensive tasks because we only care about our model's predictions. I use the DataLoader in a similar way as in train to get out batches to feed to our model. In the process I keep track of the actual labels and the predicted labels along with the loss. def validation ( dataloader , device_ ): r \"\"\"Validation function to evaluate model performance on a separate set of data. This function will return the true and predicted labels so we can use later to evaluate the model's performance. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss] \"\"\" # Use global variable for model. global model # Tracking variables predictions_labels = [] true_labels = [] #total loss for this epoch. total_loss = 0 # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model . eval () # Evaluate data for one epoch for batch in tqdm ( dataloader , total = len ( dataloader )): # add original labels true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Telling the model not to compute or store gradients, saving memory and # speeding up validation with torch . no_grad (): # Forward pass, calculate logit predictions. # This will return the logits rather than the loss because we have # not provided labels. # token_type_ids is the same as the \"segment ids\", which # differentiates sentence 1 and 2 in 2-sentence tasks. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to to calculate training accuracy. loss , logits = outputs [: 2 ] # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # get predicitons to list predict_content = logits . argmax ( axis =- 1 ) . flatten () . tolist () # update list predictions_labels += predict_content # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediciton for future evaluations. return true_labels , predictions_labels , avg_epoch_loss","title":"Helper Functions"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#load-model-and-tokenizer","text":"Loading the three essential parts of the pretrained transformers: configuration , tokenizer and model . I also need to load the model on the device I'm planning to use (GPU / CPU). Since I use the AutoClass functionality from Hugging Face I only need to worry about the model's name as input and the rest is handled by the transformers library. Code Cell: # Get model configuration. print ( 'Loading configuraiton...' ) model_config = AutoConfig . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , num_labels = n_labels ) # Get model's tokenizer. print ( 'Loading tokenizer...' ) tokenizer = AutoTokenizer . from_pretrained ( pretrained_model_name_or_path = model_name_or_path ) # Get the actual model. print ( 'Loading model...' ) model = AutoModelForSequenceClassification . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , config = model_config ) # Load model to defined device. model . to ( device ) print ( 'Model loaded to ` %s `' % device ) Output: Loading configuraiton... Loading tokenizer... Loading model... Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: [ 'cls.predictions.bias' , 'cls.predictions.transform.dense.weight' , 'cls.predictions.transform.dense.bias' , 'cls.predictions.decoder.weight' , 'cls.seq_relationship.weight' , 'cls.seq_relationship.bias' , 'cls.predictions.transform.LayerNorm.weight' , 'cls.predictions.transform.LayerNorm.bias' ] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture ( e.g. initializing a BertForSequenceClassification model from a BertForPretraining model ) . - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical ( initializing a BertForSequenceClassification model from a BertForSequenceClassification model ) . Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: [ 'classifier.weight' , 'classifier.bias' ] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Model loaded to ` cuda `","title":"Load Model and Tokenizer"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#dataset-and-dataloader","text":"This is wehere I create the PyTorch Dataset and DataLoader objects that will be used to feed data into our model. This is where I use the MovieReviewsDataset class and create the dataset variables. Since data is partitioned for both train and test I will create a PyTorch Dataset and PyTorch DataLoader object for train and test. ONLY for simplicity I will use the test as validation. In practice NEVER USE THE TEST DATA FOR VALIDATION! Code Cell: print ( 'Dealing with Train...' ) # Create pytorch dataset. train_dataset = MovieReviewsDataset ( path = '/content/aclImdb/train' , use_tokenizer = tokenizer , labels_ids = labels_ids , max_sequence_len = max_length ) print ( 'Created `train_dataset` with %d examples!' % len ( train_dataset )) # Move pytorch dataset into dataloader. train_dataloader = DataLoader ( train_dataset , batch_size = batches , shuffle = True ) print ( 'Created `train_dataloader` with %d batches!' % len ( train_dataloader )) print () print ( 'Dealing with ...' ) # Create pytorch dataset. valid_dataset = MovieReviewsDataset ( path = '/content/aclImdb/test' , use_tokenizer = tokenizer , labels_ids = labels_ids , max_sequence_len = max_length ) print ( 'Created `valid_dataset` with %d examples!' % len ( valid_dataset )) # Move pytorch dataset into dataloader. valid_dataloader = DataLoader ( valid_dataset , batch_size = batches , shuffle = False ) print ( 'Created `eval_dataloader` with %d batches!' % len ( valid_dataloader )) Output: Dealing with Train... Reading partitions... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :34< 00 :00, 17 .28s/it ] Reading neg files... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 12500 /12500 [ 00 :34< 00 :00, 362 .01it/s ] Reading pos files... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 12500 /12500 [ 00 :23< 00 :00, 534 .34it/s ] Using tokenizer on all texts. This can take a while ... Texts padded or truncated to 40 length! Finished! Created ` train_dataset ` with 25000 examples! Created ` train_dataloader ` with 25000 batches! Dealing with ... Reading partitions... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 01 :28< 00 :00, 44 .13s/it ] Reading neg files... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 12500 /12500 [ 01 :28< 00 :00, 141 .71it/s ] Reading pos files... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 12500 /12500 [ 01 :17< 00 :00, 161 .60it/s ] Using tokenizer on all texts. This can take a while ... Texts padded or truncated to 40 length! Finished! Created ` valid_dataset ` with 25000 examples! Created ` eval_dataloader ` with 25000 batches!","title":"Dataset and DataLoader"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#train","text":"I create an optimizer and scheduler that will be used by PyTorch in training. I loop through the number of defined epochs and call the train and validation functions. I will output similar info after each epoch as in Keras: train_loss: - val_loss: - train_acc: - valid_acc . After training, I plot the train and validation loss and accuracy curves to check how the training went. Code Cell: # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the 'W' stands for 'Weight Decay fix\" optimizer = AdamW ( model . parameters (), lr = 2e-5 , # args.learning_rate - default is 5e-5, our notebook had 2e-5 eps = 1e-8 # args.adam_epsilon - default is 1e-8. ) # Total number of training steps is number of batches * number of epochs. total_steps = len ( train_dataset ) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup ( optimizer , num_warmup_steps = 0 , # Default value in run_glue.py num_training_steps = total_steps ) # Store the average loss after each epoch so we can plot them. all_loss = { 'train_loss' :[], 'val_loss' :[]} all_acc = { 'train_acc' :[], 'val_acc' :[]} # Loop through each epoch. print ( 'Epoch' ) for epoch in tqdm ( range ( epochs )): print () print ( 'Training on batches...' ) # Perform one full pass over the training set. train_labels , train_predict , train_loss = train ( train_dataloader , optimizer , scheduler , device ) train_acc = accuracy_score ( train_labels , train_predict ) # Get prediction form model on validation data. print ( 'Validation on batches...' ) valid_labels , valid_predict , val_loss = validation ( valid_dataloader , device ) val_acc = accuracy_score ( valid_labels , valid_predict ) # Print loss and accuracy values to see how training evolves. print ( \" train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f \" % ( train_loss , val_loss , train_acc , val_acc )) print () # Store the loss value for plotting the learning curve. all_loss [ 'train_loss' ] . append ( train_loss ) all_loss [ 'val_loss' ] . append ( val_loss ) all_acc [ 'train_acc' ] . append ( train_acc ) all_acc [ 'val_acc' ] . append ( val_acc ) # Plot loss curves. plot_dict ( all_loss , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) # Plot accuracy curves. plot_dict ( all_acc , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) Output: Epoch 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 4 /4 [ 13 :49< 00 :00, 207 .37s/it ] Training on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 02 :40< 00 :00, 4 .86it/s ] Validation on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 00 :46< 00 :00, 16 .80it/s ] train_loss: 0 .44816 - val_loss: 0 .38655 - train_acc: 0 .78372 - valid_acc: 0 .81892 Training on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 02 :40< 00 :00, 4 .86it/s ] Validation on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 02 :13< 00 :00, 5 .88it/s ] train_loss: 0 .29504 - val_loss: 0 .43493 - train_acc: 0 .87352 - valid_acc: 0 .82360 Training on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 02 :40< 00 :00, 4 .87it/s ] Validation on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 01 :43< 00 :00, 7 .58it/s ] train_loss: 0 .16901 - val_loss: 0 .48433 - train_acc: 0 .93544 - valid_acc: 0 .82624 Training on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 02 :40< 00 :00, 4 .87it/s ] Validation on batches... 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 00 :46< 00 :00, 16 .79it/s ] train_loss: 0 .09816 - val_loss: 0 .73001 - train_acc: 0 .96936 - valid_acc: 0 .82144 It looks like a little over one epoch is enough training for this model and dataset.","title":"Train"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#evaluate","text":"When dealing with classification it's useful to look at precision recall and f1 score. Another good thing to look at when evaluating the model is the confusion matrix. Code Cell: # Get prediction form model on validation data. This is where you should use # your test data. true_labels , predictions_labels , avg_epoch_loss = validation ( valid_dataloader , device ) # Create the evaluation report. evaluation_report = classification_report ( true_labels , predictions_labels , labels = list ( labels_ids . values ()), target_names = list ( labels_ids . keys ())) # Show the evaluation report. print ( evaluation_report ) # Plot confusion matrix. plot_confusion_matrix ( y_true = true_labels , y_pred = predictions_labels , classes = list ( labels_ids . keys ()), normalize = True , magnify = 3 , ); Output: 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 782 /782 [ 00 :46< 00 :00, 16 .77it/s ] precision recall f1-score support neg 0 .83 0 .81 0 .82 12500 pos 0 .81 0 .83 0 .82 12500 accuracy 0 .82 25000 macro avg 0 .82 0 .82 0 .82 25000 weighted avg 0 .82 0 .82 0 .82 25000 Normalized confusion matrix Results are not great, but for this tutorial we are not interested in performance.","title":"Evaluate"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#final-note","text":"If you made it this far Congrats and Thank you for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you have 1 minute please give me a feedback in the comments. If you see something wrong please let me know by opening an issue on my ml_things GitHub repository! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.","title":"Final Note"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#contact","text":"GitHub: gmihaila Website: gmihaila.github.io LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila Thank you! Find out more About Me .","title":"Contact"},{"location":"tutorial_notebooks/pretrain_transformer/","text":"Pretrain Transformers Info This notebook is used to pretrain transformers models using Huggingface . This notebooks is part of my trusty notebooks for Machine Learning. Check out more similar content on my website gmihaila.github.io/useful/useful/ where I post useful notebooks like this one. This notebook is heavily inspired from the Huggingface script used for training language models: transformers/tree/master/examples/language-modeling . 'Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.' How to use this notebook? This notebooks is a code adaptation of the run_language_modeling.py . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . Parse the arguments needed that are split in TrainingArguments, ModelArguments and DataTrainingArguments. The only variables that need configuration depending on your needs are model_args , data_args and training_args in Parameters : model_args of type ModelArguments : These are the arguments for the model that you want to use such as the model_name_or_path, tokenizer_name etc. You'll need these to load the model and tokenizer. Minimum setup: model_args = ModelArguments ( model_name_or_path , model_type , tokenizer_name , ) model_name_or_path path to existing transformers model or name of transformer model to be used: bert-base-cased , roberta-base , gpt2 etc. More details here . model_type type of model used: bert , roberta , gpt2 . More details here . tokenizer_name tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased , roberta-base , gpt2 etc. data_args of type DataTrainingArguments : These are as the name suggests arguments needed for the dataset. Such as the directory name where your files are stored etc. You'll need these to load/process the dataset. Minimum setup: data_args = DataArgs ( train_data_file , eval_data_file , mlm , ) train_data_file path to your dataset. This is a plain file that contains all your text data to train a model. Use each line to separate examples: i.e. if you have a dataset composed of multiple text documents, create a single file with each line in the file associated to a text document. eval_data_file same story as train_data_file . This file is used to evaluate the model performance mlm is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta . training_args of type TrainingArguments : These are the training hyper-parameters such as learning rate, batch size, weight decay, gradient accumulation steps etc. See all possible arguments here . These are used by the Trainer. Minimum setup: model_args training_args = TrainingArguments ( output_dir , do_train , do_eval , ) output_dir path where to save the pre-trained model. do_train variable to signal if you're using train data or not. Set it to True if you mentioned train_data_file . do_eval variable to signal if you're using evaluate data or not. Set it to True if you mentioned eval_data_file . Example: Pre-train Bert In the Parameters section use arguments: # process model arguments. Check Info - Notes for more details model_args = ModelArguments ( model_name_or_path = 'bert-base-cased' , model_type = 'bert' , tokenizer_name = 'bert-base-cased' , ) # process data arguments. Check Info - Notes for more details data_args = DataArgs ( train_data_file = '/content/your_train_data' , eval_data_file = '/content/your_test_data, mlm = True , ) # process training arguments. Check Info - Notes for more details training_args = TrainingArguments ( output_dir = '/content/pretrained_bert' , do_train = True , do_eval = False ) Notes: Parameters details got from here . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . I plan on testing more models in the future. I used the The WikiText Long Term Dependency Language Modeling Dataset as an example. To reduce training time I used the evaluate split as training and test split as evaluation! .","title":"Pretrain Transformers"},{"location":"tutorial_notebooks/pretrain_transformer/#pretrain-transformers","text":"","title":"Pretrain Transformers"},{"location":"tutorial_notebooks/pretrain_transformer/#info","text":"This notebook is used to pretrain transformers models using Huggingface . This notebooks is part of my trusty notebooks for Machine Learning. Check out more similar content on my website gmihaila.github.io/useful/useful/ where I post useful notebooks like this one. This notebook is heavily inspired from the Huggingface script used for training language models: transformers/tree/master/examples/language-modeling . 'Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.'","title":"Info"},{"location":"tutorial_notebooks/pretrain_transformer/#how-to-use-this-notebook","text":"This notebooks is a code adaptation of the run_language_modeling.py . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . Parse the arguments needed that are split in TrainingArguments, ModelArguments and DataTrainingArguments. The only variables that need configuration depending on your needs are model_args , data_args and training_args in Parameters : model_args of type ModelArguments : These are the arguments for the model that you want to use such as the model_name_or_path, tokenizer_name etc. You'll need these to load the model and tokenizer. Minimum setup: model_args = ModelArguments ( model_name_or_path , model_type , tokenizer_name , ) model_name_or_path path to existing transformers model or name of transformer model to be used: bert-base-cased , roberta-base , gpt2 etc. More details here . model_type type of model used: bert , roberta , gpt2 . More details here . tokenizer_name tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased , roberta-base , gpt2 etc. data_args of type DataTrainingArguments : These are as the name suggests arguments needed for the dataset. Such as the directory name where your files are stored etc. You'll need these to load/process the dataset. Minimum setup: data_args = DataArgs ( train_data_file , eval_data_file , mlm , ) train_data_file path to your dataset. This is a plain file that contains all your text data to train a model. Use each line to separate examples: i.e. if you have a dataset composed of multiple text documents, create a single file with each line in the file associated to a text document. eval_data_file same story as train_data_file . This file is used to evaluate the model performance mlm is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta . training_args of type TrainingArguments : These are the training hyper-parameters such as learning rate, batch size, weight decay, gradient accumulation steps etc. See all possible arguments here . These are used by the Trainer. Minimum setup: model_args training_args = TrainingArguments ( output_dir , do_train , do_eval , ) output_dir path where to save the pre-trained model. do_train variable to signal if you're using train data or not. Set it to True if you mentioned train_data_file . do_eval variable to signal if you're using evaluate data or not. Set it to True if you mentioned eval_data_file .","title":"How to use this notebook?"},{"location":"tutorial_notebooks/pretrain_transformer/#example","text":"","title":"Example:"},{"location":"tutorial_notebooks/pretrain_transformer/#pre-train-bert","text":"In the Parameters section use arguments: # process model arguments. Check Info - Notes for more details model_args = ModelArguments ( model_name_or_path = 'bert-base-cased' , model_type = 'bert' , tokenizer_name = 'bert-base-cased' , ) # process data arguments. Check Info - Notes for more details data_args = DataArgs ( train_data_file = '/content/your_train_data' , eval_data_file = '/content/your_test_data, mlm = True , ) # process training arguments. Check Info - Notes for more details training_args = TrainingArguments ( output_dir = '/content/pretrained_bert' , do_train = True , do_eval = False )","title":"Pre-train Bert"},{"location":"tutorial_notebooks/pretrain_transformer/#notes","text":"Parameters details got from here . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . I plan on testing more models in the future. I used the The WikiText Long Term Dependency Language Modeling Dataset as an example. To reduce training time I used the evaluate split as training and test split as evaluation! .","title":"Notes:"},{"location":"tutorial_notebooks/pytorchtext/","text":"PyTorchText Example on how to batch text sequences with BucketIterator This notebook is an example of using pytorchtext powerful BucketIterator function which allows grouping examples of similar lengths to provide the most optimal batching method. The batching problem provides a lot of gain for recurrent models (RNN, GRU, LSTM) and transformers models (bert, roBerta, gpt2, xlnet, etc.). Basically any model that can deal with variable input batches. I will not train any model in this notebook. There are other notebooks where I use this batching method to train models. The purpose is to use an example datasets and batch it using torchtext with BucketIterator and show how it groups text sequences of similar length in batches. How to use this notebook? I am using the Large Movie Review Dataset v1.0 dataset which contains positive sentiments and negative sentiments of movie review. This dataset requires using Supervised Training with Binary Classification . The code is made with reusability in mind. It can be easily adapted for other text datasets and other NLP tasks. Comments should provide enough guidance to easily adapt this notebook to your needs. This code is designed mostly for classification tasks in mind, but it can be adapted for dialogue generation tasks . Notes: * This notebooks is a code adaptation of a few sources I foudn online: torchtext_translation_tutorial , pytorch/text - GitHub , torchtext documentation and A Comprehensive Introduction to Torchtext . Downloads Download the IMDB Movie Reviews sentiment dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Output: Installs I will use ftfy to fix any bad Unicode there might be in the text data files. Since we don't know for sure of anything is wrong with the text files its safer to run all text through ftfy. Code Cell: # install ftfy to fix any text encoding issues !pip install -q ftfy Output: | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 1 .9MB/s Building wheel for ftfy ( setup.py ) ... done Imports Import python all needed python packages. Code Cell: import io import os from tqdm.notebook import tqdm from ftfy import fix_text from torchtext import data Output: Helper Functions I've created the file_tsv function in order to concatenate all text files into a single .tsv file. Since I'll use the TabularDataset from pytorch.data I need to pass tabular format file. For text data I find the Tab Separated Values format easier to deal with - don't need to import pandas for this. Code Cell: def file_tsv ( partition_path , save_path = './' ): \"\"\"Parse each file in partition and keep track of sentiments. Create a list of paris [tag, text] \"\"\" # list of all examples in format [tag, text] examples = [] print ( partition_path ) # for each sentiment for sentiment in tqdm ([ 'pos' , 'neg' ]): # find path for sentiment sentiment_path = os . path . join ( partition_path , sentiment ) # get all files from path sentiment files_names = os . listdir ( sentiment_path ) # for each file in path sentiment for file_name in files_names : # get file content file_content = io . open ( os . path . join ( sentiment_path , file_name ), mode = 'r' , encoding = 'utf-8' ) . read () # fix any format errors file_content = fix_text ( file_content ) # append sentiment and file content examples . append ([ sentiment , file_content ]) # create a TSV file with same format `sentiment text` examples = [ \" %s \\t %s \" % ( example [ 0 ], example [ 1 ]) for example in examples ] # create file name tsv_filename = os . path . basename ( partition_path ) + '_pos_neg_ %d .tsv' % len ( examples ) # write to file io . open ( os . path . join ( save_path , tsv_filename ), mode = 'w' , encoding = 'utf-8' ) . write ( ' \\n ' . join ( examples )) return tsv_filename Output: Convert data to .tsv I will call the file_tsv function for each of the two partitions train and test . The function will return the path where the .tsv file is saved so we can use it later in pytorchtext. Code Cell: # path where to save tsv file data_path = '/content' # convert train files to tsv file train_filename = file_tsv ( partition_path = '/content/aclImdb/train' , save_path = data_path ) # convert test files to tsv file test_filename = file_tsv ( partition_path = '/content/aclImdb/test' , save_path = data_path ) Output: /content/aclImdb/train 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :39< 00 :00, 19 .80s/it ] /content/aclImdb/test 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :19< 00 :00, 9 .97s/it ] PyTorchText Setup Here I will setup the dataset to be processed by PyTrochText. I will try to add as many useful comments as possible to make the code very easy to adapt to other projects. Setup data fields Here I setup data fields for PyTorchText. We have to tell the library how to handle each column of the .tsv file. For this we need to create data.Field objects for each column. The tokenizer for text column is a simple split on white-space tokenizer. Depending on the project, it can be changed to any tokenizer. It needs to take as input text and output a list. The label tokenizer is not actually a tokenizer. It just encodes the pos into 1 and neg into 0 . Since we have two .tsv files it's great that we can use the .split function from TabularDataset to handle two files at the same time one for train and the other one for test. Find more details about torchtext.data functionality here . Code Cell: # text tokenizer function - simple white-space split text_tokenizer = lambda x : x . split () # label tokenizer - encode labels to int 0:negative and 1:positive label_tokenizer = lambda x : 0 if x == 'neg' else 1 # data fiels for text column - invoke tokenizer TEXT = data . Field ( sequential = True , tokenize = text_tokenizer , lower = False ) # data field for labels - invoke tokenize label encoder LABEL = data . Field ( sequential = True , tokenize = label_tokenizer , use_vocab = False ) # create data fields at tuples of description variable and data fiels datafields = [( \"label\" , LABEL ), ( \"text\" , TEXT )] # since we have have tab separated data we use TabularDataset train_split , test_split = data . TabularDataset . splits ( path = data_path , # path to data train = train_filename , validation = test_filename , format = 'tsv' , skip_header = False , # important fields = datafields ) Output: Bucket Iterator Here is where the magic happens! We pass in the train_split and test_split TabularDatasets splits into BucketIterator to create the actual batches. It's very nice that pytorchtext can handle splits! We need to tell the BucketIterator the batch size for both our splits. The sort_key parameter is very important. It is used to order text sequences in batches. Since we want to batch sequence of text with similar length, we will use a simple function that returns the length of our text ( len(x.text) ). It is important to keep sort=False and sort_with_batch=True to sort the batches only and not the whole dataset. Find more details in the pytorchtext BucketIterator documentation here - look at the BPTTIterator because it has same parameters except the bptt_len argument. Code Cell: # batch size used for train train_batch_size = 10 # batch size used for test test_batch_size = 20 # bucket similar length text sequences together to create batches train_iterator , val_iterator = data . BucketIterator . splits ( ( train_split , test_split ), # datasets for iterator to draw data from batch_sizes = ( train_batch_size , test_batch_size ), device = 'cpu' , # if we want to load batches on specific device sort_key = lambda x : len ( x . text ), # what function should use to group batches repeat = True , # repeat the iterator for multiple epochs(DON'T TRUST) sort = False , # avoid sorting all data using sort_key shuffle = True , # if data needs to be shuffled each time batches are created sort_within_batch = True # only sort each batch using sort_key (better to use) ) Output: Sample batch Now let's see how a batch looks like! The print format is: label sequence_length tokenized_text . We see the labels as 0 and 1 values along with the length of tokens for that text sequence and along with the list of tokens from that sequence. It looks like the lengths of sequences for this batches are very close together! This actually works! Note: I would call .create_batches() after each epoch. The repeat=True in BucketIterator should allow more epochs to run but I don't trust it! Code Cell: # create batches - needs to be called after each epoch train_iterator . create_batches () # loop through each batch for batch in train_iterator . batches : print ( len ( batch )) # print each example for example in batch : print ( example . label , len ( example . text ), example . text ) print ( ' \\n ' ) break Output: 10 0 212 [ 'Sometimes' , 'a' , 'premise' ... 1 212 [ \"I've\" , 'loved' , 'all' , 'of... 0 213 [' Apparently ', ' most ', ' view... 0 214 [ 'This' , 'movie' , 'had' , 'th... 1 215 [' This ', ' is ', ' the ', ' Neil '... 0 215 [' Hanna-Barbera ', ' sucks ', ' ... ... Train Loop Example Now let's print a list of lengths of each sequence in a batch, to see if the BucketIterator works as promised. We can see how nicely examples of similar length are grouped together by length in a single batch. After each epoch, new batches of similar length are generated when shuffle=True . It looks like we have setup everything we need to train a model! Code Cell: # example of number of epochs epochs = 1 # loop through each epoch for epoch in range ( epochs ): # create batches - needs to be called after each epoch train_iterator . create_batches () # get each batch for batch in train_iterator . batches : # put all example.text of batch in single array batch_text = [ example . text for example in batch ] # put all example.label of batch in single array batch_label = [ example . label for example in batch ] # get maximum sequence length in batch - used for padding max_sequence_len = max ([ len ( text ) for text in batch_text ]) # CODE HERE TO FEED BATCHES TO MODEL print ([ len ( text ) for text in batch_text ]) Output: [ 200 , 200 , 200 , 201 , 201 , 203 , 203 , 203 , 203 , 203 ] [ 127 , 127 , 127 , 127 , 127 , 127 , 128 , 128 , 128 , 128 ] [ 213 , 214 , 214 , 214 , 216 , 216 , 216 , 216 , 217 , 217 ] [ 238 , 238 , 239 , 240 , 240 , 241 , 241 , 241 , 241 , 241 ] [ 197 , 197 , 198 , 198 , 198 , 198 , 199 , 199 , 200 , 200 ] [ 354 , 355 , 355 , 357 , 359 , 360 , 360 , 360 , 360 , 361 ] [ 251 , 252 , 256 , 257 , 258 , 259 , 260 , 260 , 260 , 260 ] [ 148 , 148 , 148 , 148 , 148 , 148 , 149 , 149 , 149 , 149 ] [ 122 , 122 , 122 , 122 , 123 , 123 , 123 , 123 , 123 , 123 ] [ 223 , 223 , 226 , 227 , 227 , 228 , 228 , 229 , 229 , 229 ] [ 89 , 89 , 90 , 91 , 92 , 93 , 94 , 94 , 94 , 94 ] [ 131 , 131 , 132 , 132 , 132 , 132 , 132 , 133 , 133 , 133 ] ...","title":"PytorchText"},{"location":"tutorial_notebooks/pytorchtext/#pytorchtext","text":"","title":"PyTorchText"},{"location":"tutorial_notebooks/pytorchtext/#example-on-how-to-batch-text-sequences-with-bucketiterator","text":"This notebook is an example of using pytorchtext powerful BucketIterator function which allows grouping examples of similar lengths to provide the most optimal batching method. The batching problem provides a lot of gain for recurrent models (RNN, GRU, LSTM) and transformers models (bert, roBerta, gpt2, xlnet, etc.). Basically any model that can deal with variable input batches. I will not train any model in this notebook. There are other notebooks where I use this batching method to train models. The purpose is to use an example datasets and batch it using torchtext with BucketIterator and show how it groups text sequences of similar length in batches.","title":"Example on how to batch text sequences with BucketIterator"},{"location":"tutorial_notebooks/pytorchtext/#how-to-use-this-notebook","text":"I am using the Large Movie Review Dataset v1.0 dataset which contains positive sentiments and negative sentiments of movie review. This dataset requires using Supervised Training with Binary Classification . The code is made with reusability in mind. It can be easily adapted for other text datasets and other NLP tasks. Comments should provide enough guidance to easily adapt this notebook to your needs. This code is designed mostly for classification tasks in mind, but it can be adapted for dialogue generation tasks . Notes: * This notebooks is a code adaptation of a few sources I foudn online: torchtext_translation_tutorial , pytorch/text - GitHub , torchtext documentation and A Comprehensive Introduction to Torchtext .","title":"How to use this notebook?"},{"location":"tutorial_notebooks/pytorchtext/#downloads","text":"Download the IMDB Movie Reviews sentiment dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Output:","title":"Downloads"},{"location":"tutorial_notebooks/pytorchtext/#installs","text":"I will use ftfy to fix any bad Unicode there might be in the text data files. Since we don't know for sure of anything is wrong with the text files its safer to run all text through ftfy. Code Cell: # install ftfy to fix any text encoding issues !pip install -q ftfy Output: | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 1 .9MB/s Building wheel for ftfy ( setup.py ) ... done","title":"Installs"},{"location":"tutorial_notebooks/pytorchtext/#imports","text":"Import python all needed python packages. Code Cell: import io import os from tqdm.notebook import tqdm from ftfy import fix_text from torchtext import data Output:","title":"Imports"},{"location":"tutorial_notebooks/pytorchtext/#helper-functions","text":"I've created the file_tsv function in order to concatenate all text files into a single .tsv file. Since I'll use the TabularDataset from pytorch.data I need to pass tabular format file. For text data I find the Tab Separated Values format easier to deal with - don't need to import pandas for this. Code Cell: def file_tsv ( partition_path , save_path = './' ): \"\"\"Parse each file in partition and keep track of sentiments. Create a list of paris [tag, text] \"\"\" # list of all examples in format [tag, text] examples = [] print ( partition_path ) # for each sentiment for sentiment in tqdm ([ 'pos' , 'neg' ]): # find path for sentiment sentiment_path = os . path . join ( partition_path , sentiment ) # get all files from path sentiment files_names = os . listdir ( sentiment_path ) # for each file in path sentiment for file_name in files_names : # get file content file_content = io . open ( os . path . join ( sentiment_path , file_name ), mode = 'r' , encoding = 'utf-8' ) . read () # fix any format errors file_content = fix_text ( file_content ) # append sentiment and file content examples . append ([ sentiment , file_content ]) # create a TSV file with same format `sentiment text` examples = [ \" %s \\t %s \" % ( example [ 0 ], example [ 1 ]) for example in examples ] # create file name tsv_filename = os . path . basename ( partition_path ) + '_pos_neg_ %d .tsv' % len ( examples ) # write to file io . open ( os . path . join ( save_path , tsv_filename ), mode = 'w' , encoding = 'utf-8' ) . write ( ' \\n ' . join ( examples )) return tsv_filename Output:","title":"Helper Functions"},{"location":"tutorial_notebooks/pytorchtext/#convert-data-to-tsv","text":"I will call the file_tsv function for each of the two partitions train and test . The function will return the path where the .tsv file is saved so we can use it later in pytorchtext. Code Cell: # path where to save tsv file data_path = '/content' # convert train files to tsv file train_filename = file_tsv ( partition_path = '/content/aclImdb/train' , save_path = data_path ) # convert test files to tsv file test_filename = file_tsv ( partition_path = '/content/aclImdb/test' , save_path = data_path ) Output: /content/aclImdb/train 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :39< 00 :00, 19 .80s/it ] /content/aclImdb/test 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :19< 00 :00, 9 .97s/it ]","title":"Convert data to .tsv"},{"location":"tutorial_notebooks/pytorchtext/#pytorchtext-setup","text":"Here I will setup the dataset to be processed by PyTrochText. I will try to add as many useful comments as possible to make the code very easy to adapt to other projects.","title":"PyTorchText Setup"},{"location":"tutorial_notebooks/pytorchtext/#setup-data-fields","text":"Here I setup data fields for PyTorchText. We have to tell the library how to handle each column of the .tsv file. For this we need to create data.Field objects for each column. The tokenizer for text column is a simple split on white-space tokenizer. Depending on the project, it can be changed to any tokenizer. It needs to take as input text and output a list. The label tokenizer is not actually a tokenizer. It just encodes the pos into 1 and neg into 0 . Since we have two .tsv files it's great that we can use the .split function from TabularDataset to handle two files at the same time one for train and the other one for test. Find more details about torchtext.data functionality here . Code Cell: # text tokenizer function - simple white-space split text_tokenizer = lambda x : x . split () # label tokenizer - encode labels to int 0:negative and 1:positive label_tokenizer = lambda x : 0 if x == 'neg' else 1 # data fiels for text column - invoke tokenizer TEXT = data . Field ( sequential = True , tokenize = text_tokenizer , lower = False ) # data field for labels - invoke tokenize label encoder LABEL = data . Field ( sequential = True , tokenize = label_tokenizer , use_vocab = False ) # create data fields at tuples of description variable and data fiels datafields = [( \"label\" , LABEL ), ( \"text\" , TEXT )] # since we have have tab separated data we use TabularDataset train_split , test_split = data . TabularDataset . splits ( path = data_path , # path to data train = train_filename , validation = test_filename , format = 'tsv' , skip_header = False , # important fields = datafields ) Output:","title":"Setup data fields"},{"location":"tutorial_notebooks/pytorchtext/#bucket-iterator","text":"Here is where the magic happens! We pass in the train_split and test_split TabularDatasets splits into BucketIterator to create the actual batches. It's very nice that pytorchtext can handle splits! We need to tell the BucketIterator the batch size for both our splits. The sort_key parameter is very important. It is used to order text sequences in batches. Since we want to batch sequence of text with similar length, we will use a simple function that returns the length of our text ( len(x.text) ). It is important to keep sort=False and sort_with_batch=True to sort the batches only and not the whole dataset. Find more details in the pytorchtext BucketIterator documentation here - look at the BPTTIterator because it has same parameters except the bptt_len argument. Code Cell: # batch size used for train train_batch_size = 10 # batch size used for test test_batch_size = 20 # bucket similar length text sequences together to create batches train_iterator , val_iterator = data . BucketIterator . splits ( ( train_split , test_split ), # datasets for iterator to draw data from batch_sizes = ( train_batch_size , test_batch_size ), device = 'cpu' , # if we want to load batches on specific device sort_key = lambda x : len ( x . text ), # what function should use to group batches repeat = True , # repeat the iterator for multiple epochs(DON'T TRUST) sort = False , # avoid sorting all data using sort_key shuffle = True , # if data needs to be shuffled each time batches are created sort_within_batch = True # only sort each batch using sort_key (better to use) ) Output:","title":"Bucket Iterator"},{"location":"tutorial_notebooks/pytorchtext/#sample-batch","text":"Now let's see how a batch looks like! The print format is: label sequence_length tokenized_text . We see the labels as 0 and 1 values along with the length of tokens for that text sequence and along with the list of tokens from that sequence. It looks like the lengths of sequences for this batches are very close together! This actually works! Note: I would call .create_batches() after each epoch. The repeat=True in BucketIterator should allow more epochs to run but I don't trust it! Code Cell: # create batches - needs to be called after each epoch train_iterator . create_batches () # loop through each batch for batch in train_iterator . batches : print ( len ( batch )) # print each example for example in batch : print ( example . label , len ( example . text ), example . text ) print ( ' \\n ' ) break Output: 10 0 212 [ 'Sometimes' , 'a' , 'premise' ... 1 212 [ \"I've\" , 'loved' , 'all' , 'of... 0 213 [' Apparently ', ' most ', ' view... 0 214 [ 'This' , 'movie' , 'had' , 'th... 1 215 [' This ', ' is ', ' the ', ' Neil '... 0 215 [' Hanna-Barbera ', ' sucks ', ' ... ...","title":"Sample batch"},{"location":"tutorial_notebooks/pytorchtext/#train-loop-example","text":"Now let's print a list of lengths of each sequence in a batch, to see if the BucketIterator works as promised. We can see how nicely examples of similar length are grouped together by length in a single batch. After each epoch, new batches of similar length are generated when shuffle=True . It looks like we have setup everything we need to train a model! Code Cell: # example of number of epochs epochs = 1 # loop through each epoch for epoch in range ( epochs ): # create batches - needs to be called after each epoch train_iterator . create_batches () # get each batch for batch in train_iterator . batches : # put all example.text of batch in single array batch_text = [ example . text for example in batch ] # put all example.label of batch in single array batch_label = [ example . label for example in batch ] # get maximum sequence length in batch - used for padding max_sequence_len = max ([ len ( text ) for text in batch_text ]) # CODE HERE TO FEED BATCHES TO MODEL print ([ len ( text ) for text in batch_text ]) Output: [ 200 , 200 , 200 , 201 , 201 , 203 , 203 , 203 , 203 , 203 ] [ 127 , 127 , 127 , 127 , 127 , 127 , 128 , 128 , 128 , 128 ] [ 213 , 214 , 214 , 214 , 216 , 216 , 216 , 216 , 217 , 217 ] [ 238 , 238 , 239 , 240 , 240 , 241 , 241 , 241 , 241 , 241 ] [ 197 , 197 , 198 , 198 , 198 , 198 , 199 , 199 , 200 , 200 ] [ 354 , 355 , 355 , 357 , 359 , 360 , 360 , 360 , 360 , 361 ] [ 251 , 252 , 256 , 257 , 258 , 259 , 260 , 260 , 260 , 260 ] [ 148 , 148 , 148 , 148 , 148 , 148 , 149 , 149 , 149 , 149 ] [ 122 , 122 , 122 , 122 , 123 , 123 , 123 , 123 , 123 , 123 ] [ 223 , 223 , 226 , 227 , 227 , 228 , 228 , 229 , 229 , 229 ] [ 89 , 89 , 90 , 91 , 92 , 93 , 94 , 94 , 94 , 94 ] [ 131 , 131 , 132 , 132 , 132 , 132 , 132 , 133 , 133 , 133 ] ...","title":"Train Loop Example"},{"location":"tutorial_notebooks/tutorial_template_page/","text":"Title Info What should I know for this notebook? How to use this notebook? Coding Now lets do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and what should be the output. I made this format to be easy to follow if you decide to run each cell n your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations. Downloads Code Cell: Installs some_library library needs to be installed \u2026 Code Cell: Output: Loading Bard | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | Imports Code Cell: Output: Helper Functions Class_one function_one Class_one Code Cell: function_one Code Cell: Load Model and Tokenizer Code Cell: Output: Dataset and DataLoader Code Cell: Output: Train Code Cell: Output: Evaluate Code Cell: Output:","title":":violin: Title"},{"location":"tutorial_notebooks/tutorial_template_page/#title","text":"","title":"Title"},{"location":"tutorial_notebooks/tutorial_template_page/#info","text":"","title":"Info"},{"location":"tutorial_notebooks/tutorial_template_page/#what-should-i-know-for-this-notebook","text":"","title":"What should I know for this notebook?"},{"location":"tutorial_notebooks/tutorial_template_page/#how-to-use-this-notebook","text":"","title":"How to use this notebook?"},{"location":"tutorial_notebooks/tutorial_template_page/#coding","text":"Now lets do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and what should be the output. I made this format to be easy to follow if you decide to run each cell n your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.","title":"Coding"},{"location":"tutorial_notebooks/tutorial_template_page/#downloads","text":"Code Cell:","title":"Downloads"},{"location":"tutorial_notebooks/tutorial_template_page/#installs","text":"some_library library needs to be installed \u2026 Code Cell: Output: Loading Bard | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 |","title":"Installs"},{"location":"tutorial_notebooks/tutorial_template_page/#imports","text":"Code Cell: Output:","title":"Imports"},{"location":"tutorial_notebooks/tutorial_template_page/#helper-functions","text":"Class_one function_one","title":"Helper Functions"},{"location":"tutorial_notebooks/tutorial_template_page/#class_one","text":"Code Cell:","title":"Class_one"},{"location":"tutorial_notebooks/tutorial_template_page/#function_one","text":"Code Cell:","title":"function_one"},{"location":"tutorial_notebooks/tutorial_template_page/#load-model-and-tokenizer","text":"Code Cell: Output:","title":"Load Model and Tokenizer"},{"location":"tutorial_notebooks/tutorial_template_page/#dataset-and-dataloader","text":"Code Cell: Output:","title":"Dataset and DataLoader"},{"location":"tutorial_notebooks/tutorial_template_page/#train","text":"Code Cell: Output:","title":"Train"},{"location":"tutorial_notebooks/tutorial_template_page/#evaluate","text":"Code Cell: Output:","title":"Evaluate"},{"location":"useful/useful/","text":"Useful Code Here is where I put Python code snippets that I use in my Machine Learning research work. I'm using this page to have code easily accessible and to be able to share it with others. Tip : use Table of contents on the top-right side of the page to avoid endless scrolling, and is a good idea to use Copy to clipboard button on the upper right corner of each code cell to get things done quickly. Read FIle One liner to read any file: io . open ( \"my_file.txt\" , mode = 'r' , encoding = 'utf-8' ) . read () Details: import io Write File One liner to write a string to a file: io . open ( \"my_file.txt\" , mode = 'w' , encoding = 'utf-8' ) . write ( \"Your text!\" ) Details: import io Debug Start debugging after this line: import pdb ; pdb . set_trace () Details: use dir() to see all current variables, locals() to see variables and their values and globals() to see all global variables with values. Pip Install GitHub Install library directly from GitHub using pip: pip install git+github_url Details: add @version_number at the end to use a certain version to install. Parse Argument Parse arguments given when running a .py file. parser = argparse . ArgumentParser ( description = 'Description' ) parser . add_argument ( '--argument' , help = 'Help me.' , type = str ) # parse arguments args = parser . parse_args () Details: import argparse and use python script.py --argument something when running script. Doctest How to run a simple unittesc using function documentaiton. Useful when need to do unittest inside notebook: # function to test def add ( a , b ): ''' >>> add(2, 2) 5 ''' return a + b # run doctest import doctest doctest . testmod ( verbose = True ) Details: ml_things Fix Text I use this package everytime I read text data from a source I don't trust. Since text data is always messy, I always use it. It is great in fixing any bad Unicode. fix_text ( text = \"Text to be fixed\" ) Details: Install it pip install ftfy and import it from ftfy import fix_text . Current Date How to get current date in Python. I use this when need to name log files: from datetime import date today = date . today () # dd/mm/YY in string format today . strftime ( \" %d /%m/%Y\" ) Details: More details here Current Time Get current time in Python: from datetime import datetime # datetime object containing current date and time now = datetime . now () # dd/mm/YY H:M:S now . strftime ( \" %d /%m/%Y %H:%M:%S\" ) Details: More details here Remove Punctuation The fastest way to remove punctuation in Python3: table = str . maketrans ( dict . fromkeys ( string . punctuation )) \"string. With. Punctuation?\" . translate ( table ) Details: Import string . Code adapted from StackOverflow Remove punctuation from Unicode formatted strings . Class Instances from Dictionary Create class instances from dictionary. Very handy when working with notebooks and need to pass arguments as class instances. # Create dictionary of arguments. my_args = dict ( argument_one = 23 , argument_two = False ) # Convert dicitonary to class instances. my_args = type ( 'allMyArguments' , ( object ,), my_args ) Details: Code adapted from StackOverflow Creating class instance properties from a dictionary? List of Lists into Flat List Given a list of lists convert it to a single flat size list. It is the fasest way to conserve each elemnt type. l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ], [ 7 ], [ 8 , 9 ], [ 'this' , 'is' ]] functools . reduce ( operator . concat , l ) Details: Import operator, functools . Code adapted from StackOverflow How to make a flat list out of list of lists? Pickle and Unpickle Save python objects into binary using pickle. Load python objects from binary files using pickle. a = { 'hello' : 'world' } with open ( 'filename.pickle' , 'wb' ) as handle : pickle . dump ( a , handle , protocol = pickle . HIGHEST_PROTOCOL ) with open ( 'filename.pickle' , 'rb' ) as handle : b = pickle . load ( handle ) Details: Import pickle . Code adapted from StackOverflow How can I use pickle to save a dict? PyTorch Code snippets related to PyTorch : Dataset Code sample on how to create a PyTorch Dataset. The __len__(self) function needs to return the number of examples in your dataset and _getitem__(self,item) will use the index item to select an example from your dataset: from torch.utils.data import Dataset , DataLoader class PyTorchDataset ( Dataset ): \"\"\"PyTorch Dataset. \"\"\" def __init__ ( self ,): return def __len__ ( self ): return def __getitem__ ( self , item ): return # create pytorch dataset pytorch_dataset = PyTorchDataset () # move pytorch dataset into dataloader pytorch_dataloader = DataLoader ( pytorch_dataset , batch_size = 32 , shuffle = True ) Details: Find more details here PyTorch Device How to setup device in PyTorch to detect if GPU is available. If there is no GPU available it will default to CPU. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) Details: Adapted from Stack Overflow How to check if pytorch is using the GPU? .","title":"Useful Code"},{"location":"useful/useful/#useful-code","text":"Here is where I put Python code snippets that I use in my Machine Learning research work. I'm using this page to have code easily accessible and to be able to share it with others. Tip : use Table of contents on the top-right side of the page to avoid endless scrolling, and is a good idea to use Copy to clipboard button on the upper right corner of each code cell to get things done quickly.","title":"Useful Code"},{"location":"useful/useful/#read-file","text":"One liner to read any file: io . open ( \"my_file.txt\" , mode = 'r' , encoding = 'utf-8' ) . read () Details: import io","title":"Read FIle"},{"location":"useful/useful/#write-file","text":"One liner to write a string to a file: io . open ( \"my_file.txt\" , mode = 'w' , encoding = 'utf-8' ) . write ( \"Your text!\" ) Details: import io","title":"Write File"},{"location":"useful/useful/#debug","text":"Start debugging after this line: import pdb ; pdb . set_trace () Details: use dir() to see all current variables, locals() to see variables and their values and globals() to see all global variables with values.","title":"Debug"},{"location":"useful/useful/#pip-install-github","text":"Install library directly from GitHub using pip: pip install git+github_url Details: add @version_number at the end to use a certain version to install.","title":"Pip Install GitHub"},{"location":"useful/useful/#parse-argument","text":"Parse arguments given when running a .py file. parser = argparse . ArgumentParser ( description = 'Description' ) parser . add_argument ( '--argument' , help = 'Help me.' , type = str ) # parse arguments args = parser . parse_args () Details: import argparse and use python script.py --argument something when running script.","title":"Parse Argument"},{"location":"useful/useful/#doctest","text":"How to run a simple unittesc using function documentaiton. Useful when need to do unittest inside notebook: # function to test def add ( a , b ): ''' >>> add(2, 2) 5 ''' return a + b # run doctest import doctest doctest . testmod ( verbose = True ) Details: ml_things","title":"Doctest"},{"location":"useful/useful/#fix-text","text":"I use this package everytime I read text data from a source I don't trust. Since text data is always messy, I always use it. It is great in fixing any bad Unicode. fix_text ( text = \"Text to be fixed\" ) Details: Install it pip install ftfy and import it from ftfy import fix_text .","title":"Fix Text"},{"location":"useful/useful/#current-date","text":"How to get current date in Python. I use this when need to name log files: from datetime import date today = date . today () # dd/mm/YY in string format today . strftime ( \" %d /%m/%Y\" ) Details: More details here","title":"Current Date"},{"location":"useful/useful/#current-time","text":"Get current time in Python: from datetime import datetime # datetime object containing current date and time now = datetime . now () # dd/mm/YY H:M:S now . strftime ( \" %d /%m/%Y %H:%M:%S\" ) Details: More details here","title":"Current Time"},{"location":"useful/useful/#remove-punctuation","text":"The fastest way to remove punctuation in Python3: table = str . maketrans ( dict . fromkeys ( string . punctuation )) \"string. With. Punctuation?\" . translate ( table ) Details: Import string . Code adapted from StackOverflow Remove punctuation from Unicode formatted strings .","title":"Remove Punctuation"},{"location":"useful/useful/#class-instances-from-dictionary","text":"Create class instances from dictionary. Very handy when working with notebooks and need to pass arguments as class instances. # Create dictionary of arguments. my_args = dict ( argument_one = 23 , argument_two = False ) # Convert dicitonary to class instances. my_args = type ( 'allMyArguments' , ( object ,), my_args ) Details: Code adapted from StackOverflow Creating class instance properties from a dictionary?","title":"Class Instances from Dictionary"},{"location":"useful/useful/#list-of-lists-into-flat-list","text":"Given a list of lists convert it to a single flat size list. It is the fasest way to conserve each elemnt type. l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ], [ 7 ], [ 8 , 9 ], [ 'this' , 'is' ]] functools . reduce ( operator . concat , l ) Details: Import operator, functools . Code adapted from StackOverflow How to make a flat list out of list of lists?","title":"List of Lists into Flat List"},{"location":"useful/useful/#pickle-and-unpickle","text":"Save python objects into binary using pickle. Load python objects from binary files using pickle. a = { 'hello' : 'world' } with open ( 'filename.pickle' , 'wb' ) as handle : pickle . dump ( a , handle , protocol = pickle . HIGHEST_PROTOCOL ) with open ( 'filename.pickle' , 'rb' ) as handle : b = pickle . load ( handle ) Details: Import pickle . Code adapted from StackOverflow How can I use pickle to save a dict?","title":"Pickle and Unpickle"},{"location":"useful/useful/#pytorch","text":"Code snippets related to PyTorch :","title":"PyTorch"},{"location":"useful/useful/#dataset","text":"Code sample on how to create a PyTorch Dataset. The __len__(self) function needs to return the number of examples in your dataset and _getitem__(self,item) will use the index item to select an example from your dataset: from torch.utils.data import Dataset , DataLoader class PyTorchDataset ( Dataset ): \"\"\"PyTorch Dataset. \"\"\" def __init__ ( self ,): return def __len__ ( self ): return def __getitem__ ( self , item ): return # create pytorch dataset pytorch_dataset = PyTorchDataset () # move pytorch dataset into dataloader pytorch_dataloader = DataLoader ( pytorch_dataset , batch_size = 32 , shuffle = True ) Details: Find more details here","title":"Dataset"},{"location":"useful/useful/#pytorch-device","text":"How to setup device in PyTorch to detect if GPU is available. If there is no GPU available it will default to CPU. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) Details: Adapted from Stack Overflow How to check if pytorch is using the GPU? .","title":"PyTorch Device"}]}