{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About me George is a PhD candidate at University of North Texas, in the Department of Computer Science. He received his Bachelors Degree in Electrical Engineering in his home-country, Romania, and he got his Masters Degree in Computer Science in the United States. He worked three summers for State Farm as a Data Scientist (DS) and Machine Learning (ML) Engineer, and he has more than 4 years of experience in Natural Language Processing (NLP), Computer Vision (CV), Deep Learning (DL) and Reinforcement Learning (RL). He is currently working towards his doctoral thesis in casual dialog generation with persona. Competencies: Data Science, parallel computing, Tensorflow2.0, PyTorch, Python, R, Java, Matlab. Current Position Teaching Assistant Computer Science | University of North Texas August 2020 \u2013 Present Artificial Intelligence \u2013 Machine Learning Researcher University of North Texas (UNT) | Department of Computer Science Jan 2017 - Present Reading How Not to Be Wrong: The Power of Mathematical Thinking by Jordan Ellenberg Contact GitHub: gmihaila LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila","title":"About"},{"location":"#about-me","text":"George is a PhD candidate at University of North Texas, in the Department of Computer Science. He received his Bachelors Degree in Electrical Engineering in his home-country, Romania, and he got his Masters Degree in Computer Science in the United States. He worked three summers for State Farm as a Data Scientist (DS) and Machine Learning (ML) Engineer, and he has more than 4 years of experience in Natural Language Processing (NLP), Computer Vision (CV), Deep Learning (DL) and Reinforcement Learning (RL). He is currently working towards his doctoral thesis in casual dialog generation with persona. Competencies: Data Science, parallel computing, Tensorflow2.0, PyTorch, Python, R, Java, Matlab.","title":"About me"},{"location":"#current-position","text":"","title":"Current Position"},{"location":"#teaching-assistant","text":"Computer Science | University of North Texas August 2020 \u2013 Present","title":"Teaching Assistant"},{"location":"#artificial-intelligence--machine-learning-researcher","text":"University of North Texas (UNT) | Department of Computer Science Jan 2017 - Present","title":"Artificial Intelligence \u2013 Machine Learning Researcher"},{"location":"#reading","text":"How Not to Be Wrong: The Power of Mathematical Thinking by Jordan Ellenberg","title":"Reading"},{"location":"#contact","text":"GitHub: gmihaila LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila","title":"Contact"},{"location":"stuff/","text":"Welcome to Talon Hight-Performance Computing Documentation BTW this is mobile friendly and easy to deploy on GitHub Pages http://UNT-RITS.github.io/talon_sop/_build/html/index.html Task List Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Bundle code together Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } } Python Starter Content import tensorflow as tf import os print ( \"Hello world\" ) import tensorflow as tf import os print ( \"Hello world\" ) Formulas \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons \u2013 that's not all, we can also use GitHub's Octicons Foot Note Lorem ipsum 1 dolor sit amet R Starter Content Job Submit Content Tables Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Welcome to Talon Hight-Performance Computing Documentation"},{"location":"stuff/#welcome-to-talon-hight-performance-computing-documentation","text":"BTW this is mobile friendly and easy to deploy on GitHub Pages http://UNT-RITS.github.io/talon_sop/_build/html/index.html","title":"Welcome to Talon Hight-Performance Computing Documentation"},{"location":"stuff/#task-list","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Task List"},{"location":"stuff/#bundle-code-together","text":"Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } }","title":"Bundle code together"},{"location":"stuff/#python-starter","text":"","title":"Python Starter"},{"location":"stuff/#content","text":"import tensorflow as tf import os print ( \"Hello world\" ) import tensorflow as tf import os print ( \"Hello world\" )","title":"Content"},{"location":"stuff/#formulas","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons \u2013 that's not all, we can also use GitHub's Octicons","title":"Formulas"},{"location":"stuff/#foot-note","text":"Lorem ipsum 1 dolor sit amet","title":"Foot Note"},{"location":"stuff/#r-starter","text":"","title":"R Starter"},{"location":"stuff/#content_1","text":"","title":"Content"},{"location":"stuff/#job-submit","text":"","title":"Job Submit"},{"location":"stuff/#content_2","text":"","title":"Content"},{"location":"stuff/#tables","text":"Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Tables"},{"location":"activities/activities/","text":"Activities GPU Technology Conference (GTC) Code | Slides march 20-22, 2020 | San Jose California GPUs in Natural Language Processing Invited to participate in conference Instructor-Led Training at the GPU Technology Conference (GTC). Title: T22128: GPUs in Natural Language Processing Session Type: Instructor-Led Training Duration: 1 Hour 45 Minutes Find New Sentiments in Text Data Code | Slides march 3, 2020 | University of North Texas DP F223 Using text embedding and clustering methods to find new sentiments in text data. Use imdb movie reviews dataset and see how positive-negative sentiments can be used to find other sentiments. Use Tensorflow 2, Word Embeddings, Contextual Embeddings. We'll do clustering and find best number of cluster in data we don't know anything about. We'll do pretty plots to interpret results! We'll do all this on our UNT Talon Supercomputer! Duration: 1 Hour 15 Minutes Intro to using Git Lab Info | Slides may 3, 2019 | University of North Texas DP F285 How to use UNT HPC GitLab server Ever wondered what is Git and why every software company uses it? We'll teach some basic commands in Git Lab (platform used in almost every software company). And Yes, UNT has a Git Lab! Duration: 1 Hour 15 Minutes Intro to Word Embeddings - NLP Tools on Talon Info | Slides may 2, 2019 | University of North Texas DP F223 Natural Language Processing on Talon HPC Ever wondered or curious about natural language processing? How do you load a text file, how do you 'clean' it, how do you make use of it? We will learn all that and at the end you will build a Neural Network model that can predict a sentiment of a Movie Review. All this on Talon! Duration: 1 Hour 15 Minutes Using Python and Jupyter Notebooks on Talon Info | Slides may 1, 2019 | University of North Texas DP F223 Hands on work on Talon UNT HPC This workshop will go over some basic instruction about Python and Jupyter Notebooks. We will learn how to see what libraries we have available and how to install your own.Learn how to create a virtual environment, and most exciting build a simple Neural Network model to predict diabetes! All this on Talon! Duration: 1 Hour 15 Minutes Machine Learning - Neural Networks on Talon Info | Slides november 30, 2018 | University of North Texas GAB 535 What are Neural Networks, and why everyone is talking about them? This workshop is meant to give a general overview of why Neural Networks and Deep Learning are getting so much attention and help shed some light on how they work and what applications are possible because of them. Duration: 1 Hour 15 Minutes","title":"Activities"},{"location":"activities/activities/#activities","text":"","title":"Activities"},{"location":"activities/activities/#gpu-technology-conference-gtc","text":"Code | Slides march 20-22, 2020 | San Jose California GPUs in Natural Language Processing Invited to participate in conference Instructor-Led Training at the GPU Technology Conference (GTC). Title: T22128: GPUs in Natural Language Processing Session Type: Instructor-Led Training Duration: 1 Hour 45 Minutes","title":"GPU Technology Conference (GTC)"},{"location":"activities/activities/#find-new-sentiments-in-text-data","text":"Code | Slides march 3, 2020 | University of North Texas DP F223 Using text embedding and clustering methods to find new sentiments in text data. Use imdb movie reviews dataset and see how positive-negative sentiments can be used to find other sentiments. Use Tensorflow 2, Word Embeddings, Contextual Embeddings. We'll do clustering and find best number of cluster in data we don't know anything about. We'll do pretty plots to interpret results! We'll do all this on our UNT Talon Supercomputer! Duration: 1 Hour 15 Minutes","title":"Find New Sentiments in Text Data"},{"location":"activities/activities/#intro-to-using-git-lab","text":"Info | Slides may 3, 2019 | University of North Texas DP F285 How to use UNT HPC GitLab server Ever wondered what is Git and why every software company uses it? We'll teach some basic commands in Git Lab (platform used in almost every software company). And Yes, UNT has a Git Lab! Duration: 1 Hour 15 Minutes","title":"Intro to using Git Lab"},{"location":"activities/activities/#intro-to-word-embeddings---nlp-tools-on-talon","text":"Info | Slides may 2, 2019 | University of North Texas DP F223 Natural Language Processing on Talon HPC Ever wondered or curious about natural language processing? How do you load a text file, how do you 'clean' it, how do you make use of it? We will learn all that and at the end you will build a Neural Network model that can predict a sentiment of a Movie Review. All this on Talon! Duration: 1 Hour 15 Minutes","title":"Intro to Word Embeddings - NLP Tools on Talon"},{"location":"activities/activities/#using-python-and-jupyter-notebooks-on-talon","text":"Info | Slides may 1, 2019 | University of North Texas DP F223 Hands on work on Talon UNT HPC This workshop will go over some basic instruction about Python and Jupyter Notebooks. We will learn how to see what libraries we have available and how to install your own.Learn how to create a virtual environment, and most exciting build a simple Neural Network model to predict diabetes! All this on Talon! Duration: 1 Hour 15 Minutes","title":"Using Python and Jupyter Notebooks on Talon"},{"location":"activities/activities/#machine-learning---neural-networks-on-talon","text":"Info | Slides november 30, 2018 | University of North Texas GAB 535 What are Neural Networks, and why everyone is talking about them? This workshop is meant to give a general overview of why Neural Networks and Deep Learning are getting so much attention and help shed some light on how they work and what applications are possible because of them. Duration: 1 Hour 15 Minutes","title":"Machine Learning - Neural Networks on Talon"},{"location":"resume/resume/","text":"Resume pdf Summary Interest areas: Reinforcement Learning, Computer Vision, Big Data, Scaling Machine Learning, Parallel Processing. Main Research areas: Deep Learning, Natural Language Processing Thesis project: Casual Dialog Generation with different personas . For example, imagine a chatbot from the TV Series Friends that talks like your favorite character (Joey, Chandler etc.) Key Assets: Being a PhD student set me up to work with little guidance and being self-taught . How I like to takle problems: always start small and with good understating of the problem, build something simple that works and keep adding and testing on that until reaching the goal. Experience Data Scientist Intern State Farm | Enterprise Data & Analytics Align machine learning and artificial inteligence research to bussiness needs. Created a Reinforcement Learning (RL) proof of concept application. Built a framework around RL that makes it expandable into other business areas. Created custom data environments to easily train and deploy RL models. May 2020 \u2013 July 2020 Data Scientist \u2013 Machine Learning Engineer University of North Texas (UNT) | Research Information Technology Supported research project data science and Machine Learning (ML) needs. Maintain latest ML frameworks for UNT\u2019s High Performance Computing (HPC). Created workshops and tutorials about cloud computing HPC services. Sep 2018 - May 2020 Machine Learning Engineer Intern State Farm | Enterprise Data & Analytics Automated model risk validation reducing deployment timeline by 90%. Deployed Python package for automated model building and data analysis. Built a user interface for automated model comparison with CI/CD pipeline. May 2019 \u2013 July 2019 Data Scientist Intern State Farm | Enterprise &Data Analytics Introduced GPUs in ML tutorials that speed up training by over 50%. Increased model performance used in claims process by 5% using text data. Built Deep Learning Optical Character Recognition with over 75% accuracy. May 2018 \u2013 August 2018 Teaching Assistant \u2013 Computer Science University of North Texas (UNT) | Department of Computer Science Built and debugged C/C++ coding assignments and exams. Assisted Computer Science students as a peer mentor and grader. Sep 2017 \u2013 May 2018 Artificial Intelligence \u2013 Machine Learning Researcher University of North Texas (UNT) | Department of Computer Science Built state of the art Deep Learning models in Natural Language Processing. Implement concepts from research papers related to state of the art chatbots. Jan 2017 - Present Education PhD in Computer Science University of North Texas (UNT) | Department of Computer Science Focus on Natural Language Processing, Deep Generative Modeling, Companionable Robots, Persona Detection. May 2019 - Present | GPA 4.0 Masters in Computer Science University of North Texas (UNT) | Department of Computer Science Specialized in Machine Learning, Natural Language Processing, Big Data, Data Mining, Computer Vision and Deep Reinforcement Learning. May 2019 | GPA 3.9 Skills Reference Dr. Rodney D. Nielsen Associate Professor - Director HiLT Lab University of North Texas \u2013 Computer Science Engineer Email: Rodney.Nielsen@UNT.edu","title":"Resume"},{"location":"resume/resume/#resume--pdf","text":"","title":"Resume  pdf"},{"location":"resume/resume/#summary","text":"Interest areas: Reinforcement Learning, Computer Vision, Big Data, Scaling Machine Learning, Parallel Processing. Main Research areas: Deep Learning, Natural Language Processing Thesis project: Casual Dialog Generation with different personas . For example, imagine a chatbot from the TV Series Friends that talks like your favorite character (Joey, Chandler etc.) Key Assets: Being a PhD student set me up to work with little guidance and being self-taught . How I like to takle problems: always start small and with good understating of the problem, build something simple that works and keep adding and testing on that until reaching the goal.","title":"Summary"},{"location":"resume/resume/#experience","text":"","title":"Experience"},{"location":"resume/resume/#data-scientist-intern","text":"State Farm | Enterprise Data & Analytics Align machine learning and artificial inteligence research to bussiness needs. Created a Reinforcement Learning (RL) proof of concept application. Built a framework around RL that makes it expandable into other business areas. Created custom data environments to easily train and deploy RL models. May 2020 \u2013 July 2020","title":"Data Scientist Intern"},{"location":"resume/resume/#data-scientist--machine-learning-engineer","text":"University of North Texas (UNT) | Research Information Technology Supported research project data science and Machine Learning (ML) needs. Maintain latest ML frameworks for UNT\u2019s High Performance Computing (HPC). Created workshops and tutorials about cloud computing HPC services. Sep 2018 - May 2020","title":"Data Scientist \u2013 Machine Learning Engineer"},{"location":"resume/resume/#machine-learning-engineer-intern","text":"State Farm | Enterprise Data & Analytics Automated model risk validation reducing deployment timeline by 90%. Deployed Python package for automated model building and data analysis. Built a user interface for automated model comparison with CI/CD pipeline. May 2019 \u2013 July 2019","title":"Machine Learning Engineer Intern"},{"location":"resume/resume/#data-scientist-intern_1","text":"State Farm | Enterprise &Data Analytics Introduced GPUs in ML tutorials that speed up training by over 50%. Increased model performance used in claims process by 5% using text data. Built Deep Learning Optical Character Recognition with over 75% accuracy. May 2018 \u2013 August 2018","title":"Data Scientist Intern"},{"location":"resume/resume/#teaching-assistant--computer-science","text":"University of North Texas (UNT) | Department of Computer Science Built and debugged C/C++ coding assignments and exams. Assisted Computer Science students as a peer mentor and grader. Sep 2017 \u2013 May 2018","title":"Teaching Assistant \u2013 Computer Science"},{"location":"resume/resume/#artificial-intelligence--machine-learning-researcher","text":"University of North Texas (UNT) | Department of Computer Science Built state of the art Deep Learning models in Natural Language Processing. Implement concepts from research papers related to state of the art chatbots. Jan 2017 - Present","title":"Artificial Intelligence \u2013 Machine Learning Researcher"},{"location":"resume/resume/#education","text":"","title":"Education"},{"location":"resume/resume/#phd-in-computer-science","text":"University of North Texas (UNT) | Department of Computer Science Focus on Natural Language Processing, Deep Generative Modeling, Companionable Robots, Persona Detection. May 2019 - Present | GPA 4.0","title":"PhD in Computer Science"},{"location":"resume/resume/#masters-in-computer-science","text":"University of North Texas (UNT) | Department of Computer Science Specialized in Machine Learning, Natural Language Processing, Big Data, Data Mining, Computer Vision and Deep Reinforcement Learning. May 2019 | GPA 3.9","title":"Masters in Computer Science"},{"location":"resume/resume/#skills","text":"","title":"Skills"},{"location":"resume/resume/#reference","text":"","title":"Reference"},{"location":"resume/resume/#dr-rodney-d-nielsen","text":"Associate Professor - Director HiLT Lab University of North Texas \u2013 Computer Science Engineer Email: Rodney.Nielsen@UNT.edu","title":"Dr. Rodney D. Nielsen"},{"location":"tutorial_notebooks/pretrain_transformer/","text":"Pretrain Transformers Info This notebook is used to pretrain transformers models using Huggingface . This notebooks is part of my trusty notebooks for Machine Learning. Check out more similar content on my website gmihaila.github.io/useful/useful/ where I post useful notebooks like this one. This notebook is heavily inspired from the Huggingface script used for training language models: transformers/tree/master/examples/language-modeling . 'Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.' How to use this notebook? This notebooks is a code adaptation of the run_language_modeling.py . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . Parse the arguments needed that are split in TrainingArguments, ModelArguments and DataTrainingArguments. The only variables that need configuration depending on your needs are model_args , data_args and training_args in Parameters : model_args of type ModelArguments : These are the arguments for the model that you want to use such as the model_name_or_path, tokenizer_name etc. You'll need these to load the model and tokenizer. Minimum setup: model_args = ModelArguments ( model_name_or_path , model_type , tokenizer_name , ) model_name_or_path path to existing transformers model or name of transformer model to be used: bert-base-cased , roberta-base , gpt2 etc. More details here . model_type type of model used: bert , roberta , gpt2 . More details here . tokenizer_name tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased , roberta-base , gpt2 etc. data_args of type DataTrainingArguments : These are as the name suggests arguments needed for the dataset. Such as the directory name where your files are stored etc. You'll need these to load/process the dataset. Minimum setup: data_args = DataArgs ( train_data_file , eval_data_file , mlm , ) train_data_file path to your dataset. This is a plain file that contains all your text data to train a model. Use each line to separate examples: i.e. if you have a dataset composed of multiple text documents, create a single file with each line in the file associated to a text document. eval_data_file same story as train_data_file . This file is used to evaluate the model performance mlm is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta . training_args of type TrainingArguments : These are the training hyper-parameters such as learning rate, batch size, weight decay, gradient accumulation steps etc. See all possible arguments here . These are used by the Trainer. Minimum setup: model_args training_args = TrainingArguments ( output_dir , do_train , do_eval , ) output_dir path where to save the pre-trained model. do_train variable to signal if you're using train data or not. Set it to True if you mentioned train_data_file . do_eval variable to signal if you're using evaluate data or not. Set it to True if you mentioned eval_data_file . Example: Pre-train Bert In the Parameters section use arguments: # process model arguments. Check Info - Notes for more details model_args = ModelArguments ( model_name_or_path = 'bert-base-cased' , model_type = 'bert' , tokenizer_name = 'bert-base-cased' , ) # process data arguments. Check Info - Notes for more details data_args = DataArgs ( train_data_file = '/content/your_train_data' , eval_data_file = '/content/your_test_data, mlm = True , ) # process training arguments. Check Info - Notes for more details training_args = TrainingArguments ( output_dir = '/content/pretrained_bert' , do_train = True , do_eval = False ) Notes: Parameters details got from here . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . I plan on testing more models in the future. I used the The WikiText Long Term Dependency Language Modeling Dataset as an example. To reduce training time I used the evaluate split as training and test split as evaluation! .","title":"Pretrain Transformers"},{"location":"tutorial_notebooks/pretrain_transformer/#pretrain-transformers","text":"","title":"Pretrain Transformers"},{"location":"tutorial_notebooks/pretrain_transformer/#info","text":"This notebook is used to pretrain transformers models using Huggingface . This notebooks is part of my trusty notebooks for Machine Learning. Check out more similar content on my website gmihaila.github.io/useful/useful/ where I post useful notebooks like this one. This notebook is heavily inspired from the Huggingface script used for training language models: transformers/tree/master/examples/language-modeling . 'Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.'","title":"Info"},{"location":"tutorial_notebooks/pretrain_transformer/#how-to-use-this-notebook","text":"This notebooks is a code adaptation of the run_language_modeling.py . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . Parse the arguments needed that are split in TrainingArguments, ModelArguments and DataTrainingArguments. The only variables that need configuration depending on your needs are model_args , data_args and training_args in Parameters : model_args of type ModelArguments : These are the arguments for the model that you want to use such as the model_name_or_path, tokenizer_name etc. You'll need these to load the model and tokenizer. Minimum setup: model_args = ModelArguments ( model_name_or_path , model_type , tokenizer_name , ) model_name_or_path path to existing transformers model or name of transformer model to be used: bert-base-cased , roberta-base , gpt2 etc. More details here . model_type type of model used: bert , roberta , gpt2 . More details here . tokenizer_name tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased , roberta-base , gpt2 etc. data_args of type DataTrainingArguments : These are as the name suggests arguments needed for the dataset. Such as the directory name where your files are stored etc. You'll need these to load/process the dataset. Minimum setup: data_args = DataArgs ( train_data_file , eval_data_file , mlm , ) train_data_file path to your dataset. This is a plain file that contains all your text data to train a model. Use each line to separate examples: i.e. if you have a dataset composed of multiple text documents, create a single file with each line in the file associated to a text document. eval_data_file same story as train_data_file . This file is used to evaluate the model performance mlm is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta . training_args of type TrainingArguments : These are the training hyper-parameters such as learning rate, batch size, weight decay, gradient accumulation steps etc. See all possible arguments here . These are used by the Trainer. Minimum setup: model_args training_args = TrainingArguments ( output_dir , do_train , do_eval , ) output_dir path where to save the pre-trained model. do_train variable to signal if you're using train data or not. Set it to True if you mentioned train_data_file . do_eval variable to signal if you're using evaluate data or not. Set it to True if you mentioned eval_data_file .","title":"How to use this notebook?"},{"location":"tutorial_notebooks/pretrain_transformer/#example","text":"","title":"Example:"},{"location":"tutorial_notebooks/pretrain_transformer/#pre-train-bert","text":"In the Parameters section use arguments: # process model arguments. Check Info - Notes for more details model_args = ModelArguments ( model_name_or_path = 'bert-base-cased' , model_type = 'bert' , tokenizer_name = 'bert-base-cased' , ) # process data arguments. Check Info - Notes for more details data_args = DataArgs ( train_data_file = '/content/your_train_data' , eval_data_file = '/content/your_test_data, mlm = True , ) # process training arguments. Check Info - Notes for more details training_args = TrainingArguments ( output_dir = '/content/pretrained_bert' , do_train = True , do_eval = False )","title":"Pre-train Bert"},{"location":"tutorial_notebooks/pretrain_transformer/#notes","text":"Parameters details got from here . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . I plan on testing more models in the future. I used the The WikiText Long Term Dependency Language Modeling Dataset as an example. To reduce training time I used the evaluate split as training and test split as evaluation! .","title":"Notes:"},{"location":"tutorial_notebooks/pytorchtext/","text":"PyTorchText Example on how to batch text sequences with BucketIterator This notebook is an example of using pytorchtext powerful BucketIterator function which allows grouping examples of similar lengths to provide the most optimal batching method. The batching problem provides a lot of gain for recurrent models (RNN, GRU, LSTM) and transformers models (bert, roBerta, gpt2, xlnet, etc.). Basically any model that can deal with variable input batches. I will not train any model in this notebook. There are other notebooks where I use this batching method to train models. The purpose is to use an example datasets and batch it using torchtext with BucketIterator and show how it groups text sequences of similar length in batches. How to use this notebook? I am using the Large Movie Review Dataset v1.0 dataset which contains positive sentiments and negative sentiments of movie review. This dataset requires using Supervised Training with Binary Classification . The code is made with reusability in mind. It can be easily adapted for other text datasets and other NLP tasks. Comments should provide enough guidance to easily adapt this notebook to your needs. This code is designed mostly for classification tasks in mind, but it can be adapted for dialogue generation tasks . Notes: * This notebooks is a code adaptation of a few sources I foudn online: torchtext_translation_tutorial , pytorch/text - GitHub , torchtext documentation and A Comprehensive Introduction to Torchtext . Downloads Download the IMDB Movie Reviews sentiment dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Output: Installs I will use ftfy to fix any bad Unicode there might be in the text data files. Since we don't know for sure of anything is wrong with the text files its safer to run all text through ftfy. Code Cell: # install ftfy to fix any text encoding issues !pip install -q ftfy Output: | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 1 .9MB/s Building wheel for ftfy ( setup.py ) ... done Imports Import python all needed python packages. Code Cell: import io import os from tqdm.notebook import tqdm from ftfy import fix_text from torchtext import data Output: Helper Functions I've created the file_tsv function in order to concatenate all text files into a single .tsv file. Since I'll use the TabularDataset from pytorch.data I need to pass tabular format file. For text data I find the Tab Separated Values format easier to deal with - don't need to import pandas for this. Code Cell: def file_tsv ( partition_path , save_path = './' ): \"\"\"Parse each file in partition and keep track of sentiments. Create a list of paris [tag, text] \"\"\" # list of all examples in format [tag, text] examples = [] print ( partition_path ) # for each sentiment for sentiment in tqdm ([ 'pos' , 'neg' ]): # find path for sentiment sentiment_path = os . path . join ( partition_path , sentiment ) # get all files from path sentiment files_names = os . listdir ( sentiment_path ) # for each file in path sentiment for file_name in files_names : # get file content file_content = io . open ( os . path . join ( sentiment_path , file_name ), mode = 'r' , encoding = 'utf-8' ) . read () # fix any format errors file_content = fix_text ( file_content ) # append sentiment and file content examples . append ([ sentiment , file_content ]) # create a TSV file with same format `sentiment text` examples = [ \" %s \\t %s \" % ( example [ 0 ], example [ 1 ]) for example in examples ] # create file name tsv_filename = os . path . basename ( partition_path ) + '_pos_neg_ %d .tsv' % len ( examples ) # write to file io . open ( os . path . join ( save_path , tsv_filename ), mode = 'w' , encoding = 'utf-8' ) . write ( ' \\n ' . join ( examples )) return tsv_filename Output: Convert data to .tsv I will call the file_tsv function for each of the two partitions train and test . The function will return the path where the .tsv file is saved so we can use it later in pytorchtext. Code Cell: # path where to save tsv file data_path = '/content' # convert train files to tsv file train_filename = file_tsv ( partition_path = '/content/aclImdb/train' , save_path = data_path ) # convert test files to tsv file test_filename = file_tsv ( partition_path = '/content/aclImdb/test' , save_path = data_path ) Output: /content/aclImdb/train 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :39< 00 :00, 19 .80s/it ] /content/aclImdb/test 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :19< 00 :00, 9 .97s/it ] PyTorchText Setup Here I will setup the dataset to be processed by PyTrochText. I will try to add as many useful comments as possible to make the code very easy to adapt to other projects. Setup data fields Here I setup data fields for PyTorchText. We have to tell the library how to handle each column of the .tsv file. For this we need to create data.Field objects for each column. The tokenizer for text column is a simple split on white-space tokenizer. Depending on the project, it can be changed to any tokenizer. It needs to take as input text and output a list. The label tokenizer is not actually a tokenizer. It just encodes the pos into 1 and neg into 0 . Since we have two .tsv files it's great that we can use the .split function from TabularDataset to handle two files at the same time one for train and the other one for test. Find more details about torchtext.data functionality here . Code Cell: # text tokenizer function - simple white-space split text_tokenizer = lambda x : x . split () # label tokenizer - encode labels to int 0:negative and 1:positive label_tokenizer = lambda x : 0 if x == 'neg' else 1 # data fiels for text column - invoke tokenizer TEXT = data . Field ( sequential = True , tokenize = text_tokenizer , lower = False ) # data field for labels - invoke tokenize label encoder LABEL = data . Field ( sequential = True , tokenize = label_tokenizer , use_vocab = False ) # create data fields at tuples of description variable and data fiels datafields = [( \"label\" , LABEL ), ( \"text\" , TEXT )] # since we have have tab separated data we use TabularDataset train_split , test_split = data . TabularDataset . splits ( path = data_path , # path to data train = train_filename , validation = test_filename , format = 'tsv' , skip_header = False , # important fields = datafields ) Output: Bucket Iterator Here is where the magic happens! We pass in the train_split and test_split TabularDatasets splits into BucketIterator to create the actual batches. It's very nice that pytorchtext can handle splits! We need to tell the BucketIterator the batch size for both our splits. The sort_key parameter is very important. It is used to order text sequences in batches. Since we want to batch sequence of text with similar length, we will use a simple function that returns the length of our text ( len(x.text) ). It is important to keep sort=False and sort_with_batch=True to sort the batches only and not the whole dataset. Find more details in the pytorchtext BucketIterator documentation here - look at the BPTTIterator because it has same parameters except the bptt_len argument. Code Cell: # batch size used for train train_batch_size = 10 # batch size used for test test_batch_size = 20 # bucket similar length text sequences together to create batches train_iterator , val_iterator = data . BucketIterator . splits ( ( train_split , test_split ), # datasets for iterator to draw data from batch_sizes = ( train_batch_size , test_batch_size ), device = 'cpu' , # if we want to load batches on specific device sort_key = lambda x : len ( x . text ), # what function should use to group batches repeat = True , # repeat the iterator for multiple epochs(DON'T TRUST) sort = False , # avoid sorting all data using sort_key shuffle = True , # if data needs to be shuffled each time batches are created sort_within_batch = True # only sort each batch using sort_key (better to use) ) Output: Sample batch Now let's see how a batch looks like! The print format is: label sequence_length tokenized_text . We see the labels as 0 and 1 values along with the length of tokens for that text sequence and along with the list of tokens from that sequence. It looks like the lengths of sequences for this batches are very close together! This actually works! Note: I would call .create_batches() after each epoch. The repeat=True in BucketIterator should allow more epochs to run but I don't trust it! Code Cell: # create batches - needs to be called after each epoch train_iterator . create_batches () # loop through each batch for batch in train_iterator . batches : print ( len ( batch )) # print each example for example in batch : print ( example . label , len ( example . text ), example . text ) print ( ' \\n ' ) break Output: 10 0 212 [ 'Sometimes' , 'a' , 'premise' ... 1 212 [ \"I've\" , 'loved' , 'all' , 'of... 0 213 [' Apparently ', ' most ', ' view... 0 214 [ 'This' , 'movie' , 'had' , 'th... 1 215 [' This ', ' is ', ' the ', ' Neil '... 0 215 [' Hanna-Barbera ', ' sucks ', ' ... ... Train Loop Example Now let's print a list of lengths of each sequence in a batch, to see if the BucketIterator works as promised. We can see how nicely examples of similar length are grouped together by length in a single batch. After each epoch, new batches of similar length are generated when shuffle=True . It looks like we have setup everything we need to train a model! Code Cell: # example of number of epochs epochs = 1 # loop through each epoch for epoch in range ( epochs ): # create batches - needs to be called after each epoch train_iterator . create_batches () # get each batch for batch in train_iterator . batches : # put all example.text of batch in single array batch_text = [ example . text for example in batch ] # put all example.label of batch in single array batch_label = [ example . label for example in batch ] # get maximum sequence length in batch - used for padding max_sequence_len = max ([ len ( text ) for text in batch_text ]) # CODE HERE TO FEED BATCHES TO MODEL print ([ len ( text ) for text in batch_text ]) Output: [ 200 , 200 , 200 , 201 , 201 , 203 , 203 , 203 , 203 , 203 ] [ 127 , 127 , 127 , 127 , 127 , 127 , 128 , 128 , 128 , 128 ] [ 213 , 214 , 214 , 214 , 216 , 216 , 216 , 216 , 217 , 217 ] [ 238 , 238 , 239 , 240 , 240 , 241 , 241 , 241 , 241 , 241 ] [ 197 , 197 , 198 , 198 , 198 , 198 , 199 , 199 , 200 , 200 ] [ 354 , 355 , 355 , 357 , 359 , 360 , 360 , 360 , 360 , 361 ] [ 251 , 252 , 256 , 257 , 258 , 259 , 260 , 260 , 260 , 260 ] [ 148 , 148 , 148 , 148 , 148 , 148 , 149 , 149 , 149 , 149 ] [ 122 , 122 , 122 , 122 , 123 , 123 , 123 , 123 , 123 , 123 ] [ 223 , 223 , 226 , 227 , 227 , 228 , 228 , 229 , 229 , 229 ] [ 89 , 89 , 90 , 91 , 92 , 93 , 94 , 94 , 94 , 94 ] [ 131 , 131 , 132 , 132 , 132 , 132 , 132 , 133 , 133 , 133 ] ...","title":"PytorchText"},{"location":"tutorial_notebooks/pytorchtext/#pytorchtext","text":"","title":"PyTorchText"},{"location":"tutorial_notebooks/pytorchtext/#example-on-how-to-batch-text-sequences-with-bucketiterator","text":"This notebook is an example of using pytorchtext powerful BucketIterator function which allows grouping examples of similar lengths to provide the most optimal batching method. The batching problem provides a lot of gain for recurrent models (RNN, GRU, LSTM) and transformers models (bert, roBerta, gpt2, xlnet, etc.). Basically any model that can deal with variable input batches. I will not train any model in this notebook. There are other notebooks where I use this batching method to train models. The purpose is to use an example datasets and batch it using torchtext with BucketIterator and show how it groups text sequences of similar length in batches.","title":"Example on how to batch text sequences with BucketIterator"},{"location":"tutorial_notebooks/pytorchtext/#how-to-use-this-notebook","text":"I am using the Large Movie Review Dataset v1.0 dataset which contains positive sentiments and negative sentiments of movie review. This dataset requires using Supervised Training with Binary Classification . The code is made with reusability in mind. It can be easily adapted for other text datasets and other NLP tasks. Comments should provide enough guidance to easily adapt this notebook to your needs. This code is designed mostly for classification tasks in mind, but it can be adapted for dialogue generation tasks . Notes: * This notebooks is a code adaptation of a few sources I foudn online: torchtext_translation_tutorial , pytorch/text - GitHub , torchtext documentation and A Comprehensive Introduction to Torchtext .","title":"How to use this notebook?"},{"location":"tutorial_notebooks/pytorchtext/#downloads","text":"Download the IMDB Movie Reviews sentiment dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Output:","title":"Downloads"},{"location":"tutorial_notebooks/pytorchtext/#installs","text":"I will use ftfy to fix any bad Unicode there might be in the text data files. Since we don't know for sure of anything is wrong with the text files its safer to run all text through ftfy. Code Cell: # install ftfy to fix any text encoding issues !pip install -q ftfy Output: | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 1 .9MB/s Building wheel for ftfy ( setup.py ) ... done","title":"Installs"},{"location":"tutorial_notebooks/pytorchtext/#imports","text":"Import python all needed python packages. Code Cell: import io import os from tqdm.notebook import tqdm from ftfy import fix_text from torchtext import data Output:","title":"Imports"},{"location":"tutorial_notebooks/pytorchtext/#helper-functions","text":"I've created the file_tsv function in order to concatenate all text files into a single .tsv file. Since I'll use the TabularDataset from pytorch.data I need to pass tabular format file. For text data I find the Tab Separated Values format easier to deal with - don't need to import pandas for this. Code Cell: def file_tsv ( partition_path , save_path = './' ): \"\"\"Parse each file in partition and keep track of sentiments. Create a list of paris [tag, text] \"\"\" # list of all examples in format [tag, text] examples = [] print ( partition_path ) # for each sentiment for sentiment in tqdm ([ 'pos' , 'neg' ]): # find path for sentiment sentiment_path = os . path . join ( partition_path , sentiment ) # get all files from path sentiment files_names = os . listdir ( sentiment_path ) # for each file in path sentiment for file_name in files_names : # get file content file_content = io . open ( os . path . join ( sentiment_path , file_name ), mode = 'r' , encoding = 'utf-8' ) . read () # fix any format errors file_content = fix_text ( file_content ) # append sentiment and file content examples . append ([ sentiment , file_content ]) # create a TSV file with same format `sentiment text` examples = [ \" %s \\t %s \" % ( example [ 0 ], example [ 1 ]) for example in examples ] # create file name tsv_filename = os . path . basename ( partition_path ) + '_pos_neg_ %d .tsv' % len ( examples ) # write to file io . open ( os . path . join ( save_path , tsv_filename ), mode = 'w' , encoding = 'utf-8' ) . write ( ' \\n ' . join ( examples )) return tsv_filename Output:","title":"Helper Functions"},{"location":"tutorial_notebooks/pytorchtext/#convert-data-to-tsv","text":"I will call the file_tsv function for each of the two partitions train and test . The function will return the path where the .tsv file is saved so we can use it later in pytorchtext. Code Cell: # path where to save tsv file data_path = '/content' # convert train files to tsv file train_filename = file_tsv ( partition_path = '/content/aclImdb/train' , save_path = data_path ) # convert test files to tsv file test_filename = file_tsv ( partition_path = '/content/aclImdb/test' , save_path = data_path ) Output: /content/aclImdb/train 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :39< 00 :00, 19 .80s/it ] /content/aclImdb/test 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :19< 00 :00, 9 .97s/it ]","title":"Convert data to .tsv"},{"location":"tutorial_notebooks/pytorchtext/#pytorchtext-setup","text":"Here I will setup the dataset to be processed by PyTrochText. I will try to add as many useful comments as possible to make the code very easy to adapt to other projects.","title":"PyTorchText Setup"},{"location":"tutorial_notebooks/pytorchtext/#setup-data-fields","text":"Here I setup data fields for PyTorchText. We have to tell the library how to handle each column of the .tsv file. For this we need to create data.Field objects for each column. The tokenizer for text column is a simple split on white-space tokenizer. Depending on the project, it can be changed to any tokenizer. It needs to take as input text and output a list. The label tokenizer is not actually a tokenizer. It just encodes the pos into 1 and neg into 0 . Since we have two .tsv files it's great that we can use the .split function from TabularDataset to handle two files at the same time one for train and the other one for test. Find more details about torchtext.data functionality here . Code Cell: # text tokenizer function - simple white-space split text_tokenizer = lambda x : x . split () # label tokenizer - encode labels to int 0:negative and 1:positive label_tokenizer = lambda x : 0 if x == 'neg' else 1 # data fiels for text column - invoke tokenizer TEXT = data . Field ( sequential = True , tokenize = text_tokenizer , lower = False ) # data field for labels - invoke tokenize label encoder LABEL = data . Field ( sequential = True , tokenize = label_tokenizer , use_vocab = False ) # create data fields at tuples of description variable and data fiels datafields = [( \"label\" , LABEL ), ( \"text\" , TEXT )] # since we have have tab separated data we use TabularDataset train_split , test_split = data . TabularDataset . splits ( path = data_path , # path to data train = train_filename , validation = test_filename , format = 'tsv' , skip_header = False , # important fields = datafields ) Output:","title":"Setup data fields"},{"location":"tutorial_notebooks/pytorchtext/#bucket-iterator","text":"Here is where the magic happens! We pass in the train_split and test_split TabularDatasets splits into BucketIterator to create the actual batches. It's very nice that pytorchtext can handle splits! We need to tell the BucketIterator the batch size for both our splits. The sort_key parameter is very important. It is used to order text sequences in batches. Since we want to batch sequence of text with similar length, we will use a simple function that returns the length of our text ( len(x.text) ). It is important to keep sort=False and sort_with_batch=True to sort the batches only and not the whole dataset. Find more details in the pytorchtext BucketIterator documentation here - look at the BPTTIterator because it has same parameters except the bptt_len argument. Code Cell: # batch size used for train train_batch_size = 10 # batch size used for test test_batch_size = 20 # bucket similar length text sequences together to create batches train_iterator , val_iterator = data . BucketIterator . splits ( ( train_split , test_split ), # datasets for iterator to draw data from batch_sizes = ( train_batch_size , test_batch_size ), device = 'cpu' , # if we want to load batches on specific device sort_key = lambda x : len ( x . text ), # what function should use to group batches repeat = True , # repeat the iterator for multiple epochs(DON'T TRUST) sort = False , # avoid sorting all data using sort_key shuffle = True , # if data needs to be shuffled each time batches are created sort_within_batch = True # only sort each batch using sort_key (better to use) ) Output:","title":"Bucket Iterator"},{"location":"tutorial_notebooks/pytorchtext/#sample-batch","text":"Now let's see how a batch looks like! The print format is: label sequence_length tokenized_text . We see the labels as 0 and 1 values along with the length of tokens for that text sequence and along with the list of tokens from that sequence. It looks like the lengths of sequences for this batches are very close together! This actually works! Note: I would call .create_batches() after each epoch. The repeat=True in BucketIterator should allow more epochs to run but I don't trust it! Code Cell: # create batches - needs to be called after each epoch train_iterator . create_batches () # loop through each batch for batch in train_iterator . batches : print ( len ( batch )) # print each example for example in batch : print ( example . label , len ( example . text ), example . text ) print ( ' \\n ' ) break Output: 10 0 212 [ 'Sometimes' , 'a' , 'premise' ... 1 212 [ \"I've\" , 'loved' , 'all' , 'of... 0 213 [' Apparently ', ' most ', ' view... 0 214 [ 'This' , 'movie' , 'had' , 'th... 1 215 [' This ', ' is ', ' the ', ' Neil '... 0 215 [' Hanna-Barbera ', ' sucks ', ' ... ...","title":"Sample batch"},{"location":"tutorial_notebooks/pytorchtext/#train-loop-example","text":"Now let's print a list of lengths of each sequence in a batch, to see if the BucketIterator works as promised. We can see how nicely examples of similar length are grouped together by length in a single batch. After each epoch, new batches of similar length are generated when shuffle=True . It looks like we have setup everything we need to train a model! Code Cell: # example of number of epochs epochs = 1 # loop through each epoch for epoch in range ( epochs ): # create batches - needs to be called after each epoch train_iterator . create_batches () # get each batch for batch in train_iterator . batches : # put all example.text of batch in single array batch_text = [ example . text for example in batch ] # put all example.label of batch in single array batch_label = [ example . label for example in batch ] # get maximum sequence length in batch - used for padding max_sequence_len = max ([ len ( text ) for text in batch_text ]) # CODE HERE TO FEED BATCHES TO MODEL print ([ len ( text ) for text in batch_text ]) Output: [ 200 , 200 , 200 , 201 , 201 , 203 , 203 , 203 , 203 , 203 ] [ 127 , 127 , 127 , 127 , 127 , 127 , 128 , 128 , 128 , 128 ] [ 213 , 214 , 214 , 214 , 216 , 216 , 216 , 216 , 217 , 217 ] [ 238 , 238 , 239 , 240 , 240 , 241 , 241 , 241 , 241 , 241 ] [ 197 , 197 , 198 , 198 , 198 , 198 , 199 , 199 , 200 , 200 ] [ 354 , 355 , 355 , 357 , 359 , 360 , 360 , 360 , 360 , 361 ] [ 251 , 252 , 256 , 257 , 258 , 259 , 260 , 260 , 260 , 260 ] [ 148 , 148 , 148 , 148 , 148 , 148 , 149 , 149 , 149 , 149 ] [ 122 , 122 , 122 , 122 , 123 , 123 , 123 , 123 , 123 , 123 ] [ 223 , 223 , 226 , 227 , 227 , 228 , 228 , 229 , 229 , 229 ] [ 89 , 89 , 90 , 91 , 92 , 93 , 94 , 94 , 94 , 94 ] [ 131 , 131 , 132 , 132 , 132 , 132 , 132 , 133 , 133 , 133 ] ...","title":"Train Loop Example"},{"location":"useful/useful/","text":"Useful Code Here is where I put Python code snippets that I use in my Machine Learning research work. I'm using this page to have code easily accessible and to be able to share it with others. Tip : use Table of contents on the top-right side of the page to avoid endless scrolling, and is a good idea to use Copy to clipboard button on the upper right corner of each code cell to get things done quickly. Read FIle One liner to read any file: io . open ( \"my_file.txt\" , mode = 'r' , encoding = 'utf-8' ) . read () Details: import io Write File One liner to write a string to a file: io . open ( \"my_file.txt\" , mode = 'w' , encoding = 'utf-8' ) . write ( \"Your text!\" ) Details: import io Debug Start debugging after this line: import pdb ; pdb . set_trace () Details: use dir() to see all current variables, locals() to see variables and their values and globals() to see all global variables with values. Pip Install GitHub Install library directly from GitHub using pip: pip install git+github_url Details: add @version_number at the end to use a certain version to install. Parse Argument Parse arguments given when running a .py file. parser = argparse . ArgumentParser ( description = 'Description' ) parser . add_argument ( '--argument' , help = 'Help me.' , type = str ) # parse arguments args = parser . parse_args () Details: import argparse and use python script.py --argument something when running script. Doctest How to run a simple unittesc using function documentaiton. Useful when need to do unittest inside notebook: # function to test def add ( a , b ): ''' >>> add(2, 2) 5 ''' return a + b # run doctest import doctest doctest . testmod ( verbose = True ) Details: ml_things PyTorch Device How to setup device in PyTorch to detect if GPU is available. If there is no GPU available it will default to CPU. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) Fix Text I use this package everytime I read text data from a source I don't trust. Since text data is always messy, I always use it. It is great in fixing any bad Unicode. fix_text ( text = \"Text to be fixed\" ) Details: Install it pip install ftfy and import it from ftfy import fix_text . Current Date How to get current date in Python. I use this when need to name log files: from datetime import date today = date . today () # dd/mm/YY in string format today . strftime ( \" %d /%m/%Y\" ) Details: More details here Current Time Get current time in Python: from datetime import datetime # datetime object containing current date and time now = datetime . now () # dd/mm/YY H:M:S now . strftime ( \" %d /%m/%Y %H:%M:%S\" ) Details: More details here PyTorch Dataset Code sample on how to create a PyTorch Dataset. The __len__(self) function needs to return the number of examples in your dataset and _getitem__(self,item) will use the index item to select an example from your dataset: from torch.utils.data import Dataset , DataLoader class PyTorchDataset ( Dataset ): \"\"\"PyTorch Dataset. \"\"\" def __init__ ( self ,): return def __len__ ( self ): return def __getitem__ ( self , item ): return # create pytorch dataset pytorch_dataset = PyTorchDataset () # move pytorch dataset into dataloader pytorch_dataloader = DataLoader ( pytorch_dataset , batch_size = 32 , shuffle = True ) Details: Find more details here","title":"Useful Code"},{"location":"useful/useful/#useful-code","text":"Here is where I put Python code snippets that I use in my Machine Learning research work. I'm using this page to have code easily accessible and to be able to share it with others. Tip : use Table of contents on the top-right side of the page to avoid endless scrolling, and is a good idea to use Copy to clipboard button on the upper right corner of each code cell to get things done quickly.","title":"Useful Code"},{"location":"useful/useful/#read-file","text":"One liner to read any file: io . open ( \"my_file.txt\" , mode = 'r' , encoding = 'utf-8' ) . read () Details: import io","title":"Read FIle"},{"location":"useful/useful/#write-file","text":"One liner to write a string to a file: io . open ( \"my_file.txt\" , mode = 'w' , encoding = 'utf-8' ) . write ( \"Your text!\" ) Details: import io","title":"Write File"},{"location":"useful/useful/#debug","text":"Start debugging after this line: import pdb ; pdb . set_trace () Details: use dir() to see all current variables, locals() to see variables and their values and globals() to see all global variables with values.","title":"Debug"},{"location":"useful/useful/#pip-install-github","text":"Install library directly from GitHub using pip: pip install git+github_url Details: add @version_number at the end to use a certain version to install.","title":"Pip Install GitHub"},{"location":"useful/useful/#parse-argument","text":"Parse arguments given when running a .py file. parser = argparse . ArgumentParser ( description = 'Description' ) parser . add_argument ( '--argument' , help = 'Help me.' , type = str ) # parse arguments args = parser . parse_args () Details: import argparse and use python script.py --argument something when running script.","title":"Parse Argument"},{"location":"useful/useful/#doctest","text":"How to run a simple unittesc using function documentaiton. Useful when need to do unittest inside notebook: # function to test def add ( a , b ): ''' >>> add(2, 2) 5 ''' return a + b # run doctest import doctest doctest . testmod ( verbose = True ) Details: ml_things","title":"Doctest"},{"location":"useful/useful/#pytorch-device","text":"How to setup device in PyTorch to detect if GPU is available. If there is no GPU available it will default to CPU. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' )","title":"PyTorch Device"},{"location":"useful/useful/#fix-text","text":"I use this package everytime I read text data from a source I don't trust. Since text data is always messy, I always use it. It is great in fixing any bad Unicode. fix_text ( text = \"Text to be fixed\" ) Details: Install it pip install ftfy and import it from ftfy import fix_text .","title":"Fix Text"},{"location":"useful/useful/#current-date","text":"How to get current date in Python. I use this when need to name log files: from datetime import date today = date . today () # dd/mm/YY in string format today . strftime ( \" %d /%m/%Y\" ) Details: More details here","title":"Current Date"},{"location":"useful/useful/#current-time","text":"Get current time in Python: from datetime import datetime # datetime object containing current date and time now = datetime . now () # dd/mm/YY H:M:S now . strftime ( \" %d /%m/%Y %H:%M:%S\" ) Details: More details here","title":"Current Time"},{"location":"useful/useful/#pytorch-dataset","text":"Code sample on how to create a PyTorch Dataset. The __len__(self) function needs to return the number of examples in your dataset and _getitem__(self,item) will use the index item to select an example from your dataset: from torch.utils.data import Dataset , DataLoader class PyTorchDataset ( Dataset ): \"\"\"PyTorch Dataset. \"\"\" def __init__ ( self ,): return def __len__ ( self ): return def __getitem__ ( self , item ): return # create pytorch dataset pytorch_dataset = PyTorchDataset () # move pytorch dataset into dataloader pytorch_dataloader = DataLoader ( pytorch_dataset , batch_size = 32 , shuffle = True ) Details: Find more details here","title":"PyTorch Dataset"}]}