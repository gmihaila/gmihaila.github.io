{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About me George is a PhD candidate at University of North Texas, in the Department of Computer Science. He received his bachelor\u2019s degree in Electrical Engineering in his home-country, Romania, and he got his master\u2019s degree in Computer Science in the United States. He worked three summers for State Farm as a Data Scientist (DS) and Machine Learning (ML) Engineer, and two years combined of DS and ML Engineer for the University of north Texas High Performance Computing center. He has more than 5 years of combined experience in Natural Language Processing (NLP), Computer Vision (CV), Deep Learning (DL), Reinforcement Learning (RL) and Machine Learning Operations (MLOps). He was the technical reviewer for the book Transformers for Natural Language Processing by Denis Rothman . This was the first book to be published on Transformers model architecture for NLP. He is currently working towards his doctoral thesis in casual dialog generation with persona. In his free time George likes to work on AI related tutorials and projects. Competencies: data science, machine learning, deep learning, high performance computing, mlops, Tensorflow2.0+, PyTorch, Python, R. Current Position Teaching Assistant Computer Science | University of North Texas August 2020 \u2013 Present Artificial Intelligence \u2013 Machine Learning Researcher University of North Texas (UNT) | Department of Computer Science Jan 2017 - Present Contact \ud83c\udfa3 \ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcd3 Medium: @gmihaila \ud83d\udcec Email: georgemihaila@my.unt.edu.com \ud83d\udc64 Schedule meeting: calendly.com/georgemihaila","title":"About"},{"location":"#about-me","text":"George is a PhD candidate at University of North Texas, in the Department of Computer Science. He received his bachelor\u2019s degree in Electrical Engineering in his home-country, Romania, and he got his master\u2019s degree in Computer Science in the United States. He worked three summers for State Farm as a Data Scientist (DS) and Machine Learning (ML) Engineer, and two years combined of DS and ML Engineer for the University of north Texas High Performance Computing center. He has more than 5 years of combined experience in Natural Language Processing (NLP), Computer Vision (CV), Deep Learning (DL), Reinforcement Learning (RL) and Machine Learning Operations (MLOps). He was the technical reviewer for the book Transformers for Natural Language Processing by Denis Rothman . This was the first book to be published on Transformers model architecture for NLP. He is currently working towards his doctoral thesis in casual dialog generation with persona. In his free time George likes to work on AI related tutorials and projects. Competencies: data science, machine learning, deep learning, high performance computing, mlops, Tensorflow2.0+, PyTorch, Python, R.","title":"About me"},{"location":"#current-position","text":"","title":"Current Position"},{"location":"#teaching-assistant","text":"Computer Science | University of North Texas August 2020 \u2013 Present","title":"Teaching Assistant"},{"location":"#artificial-intelligence--machine-learning-researcher","text":"University of North Texas (UNT) | Department of Computer Science Jan 2017 - Present","title":"Artificial Intelligence \u2013 Machine Learning Researcher"},{"location":"#contact-","text":"\ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcd3 Medium: @gmihaila \ud83d\udcec Email: georgemihaila@my.unt.edu.com \ud83d\udc64 Schedule meeting: calendly.com/georgemihaila","title":"Contact \ud83c\udfa3"},{"location":"stuff/","text":"Welcome to Talon Hight-Performance Computing Documentation BTW this is mobile friendly and easy to deploy on GitHub Pages http://UNT-RITS.github.io/talon_sop/_build/html/index.html Task List Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Bundle code together Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } } Python Starter Content import tensorflow as tf import os print ( \"Hello world\" ) import tensorflow as tf import os print ( \"Hello world\" ) Formulas \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons :octicons-octoface: \u2013 that's not all, we can also use GitHub's Octicons Foot Note Lorem ipsum 1 dolor sit amet R Starter Content Job Submit Content Tables Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Welcome to Talon Hight-Performance Computing Documentation"},{"location":"stuff/#welcome-to-talon-hight-performance-computing-documentation","text":"BTW this is mobile friendly and easy to deploy on GitHub Pages http://UNT-RITS.github.io/talon_sop/_build/html/index.html","title":"Welcome to Talon Hight-Performance Computing Documentation"},{"location":"stuff/#task-list","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Task List"},{"location":"stuff/#bundle-code-together","text":"Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } }","title":"Bundle code together"},{"location":"stuff/#python-starter","text":"","title":"Python Starter"},{"location":"stuff/#content","text":"import tensorflow as tf import os print ( \"Hello world\" ) import tensorflow as tf import os print ( \"Hello world\" )","title":"Content"},{"location":"stuff/#formulas","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons :octicons-octoface: \u2013 that's not all, we can also use GitHub's Octicons","title":"Formulas"},{"location":"stuff/#foot-note","text":"Lorem ipsum 1 dolor sit amet","title":"Foot Note"},{"location":"stuff/#r-starter","text":"","title":"R Starter"},{"location":"stuff/#content_1","text":"","title":"Content"},{"location":"stuff/#job-submit","text":"","title":"Job Submit"},{"location":"stuff/#content_2","text":"","title":"Content"},{"location":"stuff/#tables","text":"Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Tables"},{"location":"activities/activities/","text":"Activities Tutorial: Text Classification using GPT2 and Pytorch Code | Slides april 09, 2021 | Zoom Text classification is a very common problem that needs solving when dealing with text data. We\u2019ve all seen and know how to use Encoder Transformer models like Bert and RoBerta for text classification but did you know you can use a Decoder Transformer model like GPT2 for text classification? In this tutorial, I will walk you through on how to use GPT2 from HuggingFace for text classification. We will start with downloading customized dataset, installing required componments, selecting pre-trained models, and then train the model. we will finally evaluate the results and how to optimize further. Duration: 1 Hour 45 Minutes GPU Technology Conference (GTC) Code | Slides march 20-22, 2020 | San Jose California GPUs in Natural Language Processing Invited to participate in conference Instructor-Led Training at the GPU Technology Conference (GTC). Title: T22128: GPUs in Natural Language Processing Session Type: Instructor-Led Training Duration: 1 Hour 45 Minutes Find New Sentiments in Text Data Code | Slides march 3, 2020 | University of North Texas DP F223 Using text embedding and clustering methods to find new sentiments in text data. Use imdb movie reviews dataset and see how positive-negative sentiments can be used to find other sentiments. Use Tensorflow 2, Word Embeddings, Contextual Embeddings. We'll do clustering and find best number of cluster in data we don't know anything about. We'll do pretty plots to interpret results! We'll do all this on our UNT Talon Supercomputer! Duration: 1 Hour 15 Minutes Intro to using Git Lab Info | Slides may 3, 2019 | University of North Texas DP F285 How to use UNT HPC GitLab server Ever wondered what is Git and why every software company uses it? We'll teach some basic commands in Git Lab (platform used in almost every software company). And Yes, UNT has a Git Lab! Duration: 1 Hour 15 Minutes Intro to Word Embeddings - NLP Tools on Talon Info | Slides may 2, 2019 | University of North Texas DP F223 Natural Language Processing on Talon HPC Ever wondered or curious about natural language processing? How do you load a text file, how do you 'clean' it, how do you make use of it? We will learn all that and at the end you will build a Neural Network model that can predict a sentiment of a Movie Review. All this on Talon! Duration: 1 Hour 15 Minutes Using Python and Jupyter Notebooks on Talon Info | Slides may 1, 2019 | University of North Texas DP F223 Hands on work on Talon UNT HPC This workshop will go over some basic instruction about Python and Jupyter Notebooks. We will learn how to see what libraries we have available and how to install your own.Learn how to create a virtual environment, and most exciting build a simple Neural Network model to predict diabetes! All this on Talon! Duration: 1 Hour 15 Minutes Machine Learning - Neural Networks on Talon Info | Slides november 30, 2018 | University of North Texas GAB 535 What are Neural Networks, and why everyone is talking about them? This workshop is meant to give a general overview of why Neural Networks and Deep Learning are getting so much attention and help shed some light on how they work and what applications are possible because of them. Duration: 1 Hour 15 Minutes","title":"Activities"},{"location":"activities/activities/#activities","text":"","title":"Activities"},{"location":"activities/activities/#tutorial-text-classification-using-gpt2-and-pytorch","text":"Code | Slides april 09, 2021 | Zoom Text classification is a very common problem that needs solving when dealing with text data. We\u2019ve all seen and know how to use Encoder Transformer models like Bert and RoBerta for text classification but did you know you can use a Decoder Transformer model like GPT2 for text classification? In this tutorial, I will walk you through on how to use GPT2 from HuggingFace for text classification. We will start with downloading customized dataset, installing required componments, selecting pre-trained models, and then train the model. we will finally evaluate the results and how to optimize further. Duration: 1 Hour 45 Minutes","title":"Tutorial: Text Classification using GPT2 and Pytorch"},{"location":"activities/activities/#gpu-technology-conference-gtc","text":"Code | Slides march 20-22, 2020 | San Jose California GPUs in Natural Language Processing Invited to participate in conference Instructor-Led Training at the GPU Technology Conference (GTC). Title: T22128: GPUs in Natural Language Processing Session Type: Instructor-Led Training Duration: 1 Hour 45 Minutes","title":"GPU Technology Conference (GTC)"},{"location":"activities/activities/#find-new-sentiments-in-text-data","text":"Code | Slides march 3, 2020 | University of North Texas DP F223 Using text embedding and clustering methods to find new sentiments in text data. Use imdb movie reviews dataset and see how positive-negative sentiments can be used to find other sentiments. Use Tensorflow 2, Word Embeddings, Contextual Embeddings. We'll do clustering and find best number of cluster in data we don't know anything about. We'll do pretty plots to interpret results! We'll do all this on our UNT Talon Supercomputer! Duration: 1 Hour 15 Minutes","title":"Find New Sentiments in Text Data"},{"location":"activities/activities/#intro-to-using-git-lab","text":"Info | Slides may 3, 2019 | University of North Texas DP F285 How to use UNT HPC GitLab server Ever wondered what is Git and why every software company uses it? We'll teach some basic commands in Git Lab (platform used in almost every software company). And Yes, UNT has a Git Lab! Duration: 1 Hour 15 Minutes","title":"Intro to using Git Lab"},{"location":"activities/activities/#intro-to-word-embeddings---nlp-tools-on-talon","text":"Info | Slides may 2, 2019 | University of North Texas DP F223 Natural Language Processing on Talon HPC Ever wondered or curious about natural language processing? How do you load a text file, how do you 'clean' it, how do you make use of it? We will learn all that and at the end you will build a Neural Network model that can predict a sentiment of a Movie Review. All this on Talon! Duration: 1 Hour 15 Minutes","title":"Intro to Word Embeddings - NLP Tools on Talon"},{"location":"activities/activities/#using-python-and-jupyter-notebooks-on-talon","text":"Info | Slides may 1, 2019 | University of North Texas DP F223 Hands on work on Talon UNT HPC This workshop will go over some basic instruction about Python and Jupyter Notebooks. We will learn how to see what libraries we have available and how to install your own.Learn how to create a virtual environment, and most exciting build a simple Neural Network model to predict diabetes! All this on Talon! Duration: 1 Hour 15 Minutes","title":"Using Python and Jupyter Notebooks on Talon"},{"location":"activities/activities/#machine-learning---neural-networks-on-talon","text":"Info | Slides november 30, 2018 | University of North Texas GAB 535 What are Neural Networks, and why everyone is talking about them? This workshop is meant to give a general overview of why Neural Networks and Deep Learning are getting so much attention and help shed some light on how they work and what applications are possible because of them. Duration: 1 Hour 15 Minutes","title":"Machine Learning - Neural Networks on Talon"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/","text":"img[alt~=\"center\"] { display: block; margin: 0 auto; } Text Classification using GPT-2 George Mihaila PhD Candidate Computer Science University of North Texas Disclaimer This is not your average tutorial on GPT-2! I will not focus on the original purpose of GPT-2 - text generation. In this tutorial we'll use GPT-2 in a less conventional way - for text classification. Agenda Intro Understanding Natural Language Processing Word embeddings Text Classification The Transformer GPT-2 Architecture Inner Workings Prediction Classification Coding Session Conclusions Intro Wikipedia Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language \u2026 \u2026 The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. Understanding Natural Language Processing Understanding Natural Language Processing example = \"I love cats! Cats are very funny.\" Understanding Natural Language Processing example = \"I love cats! Cats are very funny.\" vocabulary = [ 'i' , 'love' , 'cats' , 'are' , 'funny' ] Understanding Natural Language Processing example = \"I love cats! Cats are very funny.\" vocabulary = [ 'i' , 'love' , 'cats' , 'are' , 'funny' ] word_id = { 'i' : 0 , 'love' : 1 , 'cats' : 2 , 'are' : 3 , 'funny' : 4 } Understanding Natural Language Processing example = \"I love cats! Cats are very funny.\" vocabulary = [ 'i' , 'love' , 'cats' , 'are' , 'funny' ] word_id = { 'i' : 0 , 'love' : 1 , 'cats' : 2 , 'are' : 3 , 'funny' : 4 } Encode text into numbers. Word embeddings Word embeddings Use numbers to represent words: 'love' : 1 'cats' : 2 'funny' : 4 Word embeddings Use numbers to represent words: 'love' : 1 'cats' : 2 'funny' : 4 User vectors instead of numbers 'love' : [ 0.90 , 3.10 ] 'cats' : [ 3.40 , 3.20 ] 'funny' : [ 0.45 , 1.88 ] Word embeddings Use numbers to represent words: 'love' : 1 'cats' : 2 'funny' : 4 User vectors instead of numbers 'love' : [ 0.90 , 3.10 ] 'cats' : [ 3.40 , 3.20 ] 'funny' : [ 0.45 , 1.88 ] Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. Text Classification Is the task of assigning a set of predefined categories to text data. Text classifiers can be used to organize , structure , and categorize any kind of text: documents, medical studies \u2026 For example: new articles can be organized by topics ; support tickets can be organized by urgency ; movie reviews can be organized by sentiment ; Text Classification Movie reviews sentiment classification The Transformer Is a deep neural network architecture for transforming one sequence into another one with the help of two parts ( Encoder and Decoder ). Was first introduced by Google in 2017 in the paper Attention Is All You Need . Is based solely on attention mechanisms. It brought \"the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search.\" for Google. The Transformer Is a deep neural network architecture for transforming one sequence into another one with the help of two parts ( Encoder and Decoder ). It can transform an article to a summary or translate english to another language, etc. The Transformer Is based solely on attention mechanisms. Disclaimer: I will not cover attention since this is not the intent of this tutorial. The Illustrated Transformer by Jay Alammar is great resource! GPT-2 GPT-2 Wikipedia Generative Pre-trained Transformer 2 (GPT-2) is an open-source artificial intelligence created by OpenAI in February 2019. GPT-2 translates text, answers questions, summarizes passages, and generates text output on a level that, while sometimes indistinguishable from that of humans, can become repetitive or nonsensical when generating long passages. Architecture Architecture We only have the decoder side of Transformer: Architecture There are multiple variations of GPT-2 depending on the number of decoder blocks: Inner Workings Detailed diagram Prediction Wikipedia GPT-2 translates text, answers questions, summarizes passages, and generates text output \u2026 Let's see how it make predictions. Prediction We pass the text \"a robot must\" to GPT-2. GPT-2 will output word embeddings for each of the words. The last word embedding is used to predict the next word \"obey\" . Prediction Our new text \"a robot must obey\" is fed to GPT-2. Same process is repeated to predict the next word \"orders\" . Prediction This is how GPT-2 is able to translates text, answers questions, summarizes passages, and generates text output . How can we use it for simple text classification? Classification Now we'll use GPT-2 to perform text classification. Classification Now we'll use GPT-2 to perform text classification. We'll classify a movie review as being either positive or negative sentiment. Classification Let's look at a positive movie review: Classification Now let's look at a negative movie review: Coding Session \ud83c\udfb1 GPT-2 For Text Classification using Hugging Face \ud83e\udd17 Transformers Complete tutorial on how to use GPT-2 for text classification. Disclaimer: The format of this tutorial notebook is very similar to my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format. This notebook is used to fine-tune GPT-2 model for text classification using Huggingface transformers library on a custom dataset. Conclusions I showed that GPT-2 can be used for text classification. Depending on your data and classification task GPT-2 could outperform other transformers models. It's always good to have options. Contact \ud83c\udfa3 Let's stay in touch! \ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcd3 Medium: @gmihaila \ud83d\udcec Email: georgemihaila@my.unt.edu.com \ud83d\udc64 Schedule meeting: calendly.com/georgemihaila Resources Text Classification What is a transformer Google's Search Engine biggest leap Understand transformers","title":"Markdow presentatioin"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#text-classification-using-gpt-2","text":"George Mihaila PhD Candidate Computer Science University of North Texas","title":" Text Classification using GPT-2"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#disclaimer","text":"This is not your average tutorial on GPT-2! I will not focus on the original purpose of GPT-2 - text generation. In this tutorial we'll use GPT-2 in a less conventional way - for text classification.","title":"Disclaimer"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#agenda","text":"Intro Understanding Natural Language Processing Word embeddings Text Classification The Transformer GPT-2 Architecture Inner Workings Prediction Classification Coding Session Conclusions","title":"Agenda"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#intro","text":"","title":"Intro"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#wikipedia","text":"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language \u2026 \u2026 The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.","title":"Wikipedia"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#understanding-natural-language-processing","text":"","title":"Understanding Natural Language Processing"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#understanding-natural-language-processing_1","text":"example = \"I love cats! Cats are very funny.\"","title":"Understanding Natural Language Processing"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#understanding-natural-language-processing_2","text":"example = \"I love cats! Cats are very funny.\" vocabulary = [ 'i' , 'love' , 'cats' , 'are' , 'funny' ]","title":"Understanding Natural Language Processing"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#understanding-natural-language-processing_3","text":"example = \"I love cats! Cats are very funny.\" vocabulary = [ 'i' , 'love' , 'cats' , 'are' , 'funny' ] word_id = { 'i' : 0 , 'love' : 1 , 'cats' : 2 , 'are' : 3 , 'funny' : 4 }","title":"Understanding Natural Language Processing"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#understanding-natural-language-processing_4","text":"example = \"I love cats! Cats are very funny.\" vocabulary = [ 'i' , 'love' , 'cats' , 'are' , 'funny' ] word_id = { 'i' : 0 , 'love' : 1 , 'cats' : 2 , 'are' : 3 , 'funny' : 4 } Encode text into numbers.","title":"Understanding Natural Language Processing"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#word-embeddings","text":"","title":"Word embeddings"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#word-embeddings_1","text":"Use numbers to represent words: 'love' : 1 'cats' : 2 'funny' : 4","title":"Word embeddings"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#word-embeddings_2","text":"Use numbers to represent words: 'love' : 1 'cats' : 2 'funny' : 4 User vectors instead of numbers 'love' : [ 0.90 , 3.10 ] 'cats' : [ 3.40 , 3.20 ] 'funny' : [ 0.45 , 1.88 ]","title":"Word embeddings"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#word-embeddings_3","text":"Use numbers to represent words: 'love' : 1 'cats' : 2 'funny' : 4 User vectors instead of numbers 'love' : [ 0.90 , 3.10 ] 'cats' : [ 3.40 , 3.20 ] 'funny' : [ 0.45 , 1.88 ] Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.","title":"Word embeddings"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#text-classification","text":"Is the task of assigning a set of predefined categories to text data. Text classifiers can be used to organize , structure , and categorize any kind of text: documents, medical studies \u2026 For example: new articles can be organized by topics ; support tickets can be organized by urgency ; movie reviews can be organized by sentiment ;","title":"Text Classification"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#text-classification_1","text":"","title":"Text Classification"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#movie-reviews-sentiment-classification","text":"","title":"Movie reviews sentiment classification"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#the-transformer","text":"Is a deep neural network architecture for transforming one sequence into another one with the help of two parts ( Encoder and Decoder ). Was first introduced by Google in 2017 in the paper Attention Is All You Need . Is based solely on attention mechanisms. It brought \"the biggest leap forward in the past five years, and one of the biggest leaps forward in the history of Search.\" for Google.","title":"The Transformer"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#the-transformer_1","text":"Is a deep neural network architecture for transforming one sequence into another one with the help of two parts ( Encoder and Decoder ). It can transform an article to a summary or translate english to another language, etc.","title":"The Transformer"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#the-transformer_2","text":"Is based solely on attention mechanisms. Disclaimer: I will not cover attention since this is not the intent of this tutorial. The Illustrated Transformer by Jay Alammar is great resource!","title":"The Transformer"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#gpt-2","text":"","title":"GPT-2"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#gpt-2_1","text":"","title":"GPT-2"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#wikipedia_1","text":"Generative Pre-trained Transformer 2 (GPT-2) is an open-source artificial intelligence created by OpenAI in February 2019. GPT-2 translates text, answers questions, summarizes passages, and generates text output on a level that, while sometimes indistinguishable from that of humans, can become repetitive or nonsensical when generating long passages.","title":"Wikipedia"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#architecture","text":"","title":"Architecture"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#architecture_1","text":"We only have the decoder side of Transformer:","title":"Architecture"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#architecture_2","text":"There are multiple variations of GPT-2 depending on the number of decoder blocks:","title":"Architecture"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#inner-workings","text":"Detailed diagram","title":"Inner Workings"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#prediction","text":"","title":"Prediction"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#wikipedia_2","text":"GPT-2 translates text, answers questions, summarizes passages, and generates text output \u2026 Let's see how it make predictions.","title":"Wikipedia"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#prediction_1","text":"We pass the text \"a robot must\" to GPT-2. GPT-2 will output word embeddings for each of the words. The last word embedding is used to predict the next word \"obey\" .","title":"Prediction"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#prediction_2","text":"Our new text \"a robot must obey\" is fed to GPT-2. Same process is repeated to predict the next word \"orders\" .","title":"Prediction"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#prediction_3","text":"This is how GPT-2 is able to translates text, answers questions, summarizes passages, and generates text output . How can we use it for simple text classification?","title":"Prediction"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#classification","text":"Now we'll use GPT-2 to perform text classification.","title":"Classification"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#classification_1","text":"Now we'll use GPT-2 to perform text classification. We'll classify a movie review as being either positive or negative sentiment.","title":"Classification"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#classification_2","text":"Let's look at a positive movie review:","title":"Classification"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#classification_3","text":"Now let's look at a negative movie review:","title":"Classification"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#coding-session","text":"","title":"Coding Session"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#-gpt-2-for-text-classification-using-hugging-face--transformers","text":"","title":"\ud83c\udfb1 GPT-2 For Text Classification using Hugging Face \ud83e\udd17 Transformers"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#complete-tutorial-on-how-to-use-gpt-2-for-text-classification","text":"Disclaimer: The format of this tutorial notebook is very similar to my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format. This notebook is used to fine-tune GPT-2 model for text classification using Huggingface transformers library on a custom dataset.","title":"Complete tutorial on how to use GPT-2 for text classification."},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#conclusions","text":"I showed that GPT-2 can be used for text classification. Depending on your data and classification task GPT-2 could outperform other transformers models. It's always good to have options.","title":"Conclusions"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#contact-","text":"Let's stay in touch! \ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcd3 Medium: @gmihaila \ud83d\udcec Email: georgemihaila@my.unt.edu.com \ud83d\udc64 Schedule meeting: calendly.com/georgemihaila","title":"Contact \ud83c\udfa3"},{"location":"activities/text_classification_using_gpt2_and_pytorch/slides/#resources","text":"Text Classification What is a transformer Google's Search Engine biggest leap Understand transformers","title":"Resources"},{"location":"resume/resume/","text":"Resume pdf Summary Interest areas: Reinforcement Learning, Computer Vision, Big Data, Scaling Machine Learning, Parallel Processing. Main Research areas: Deep Learning, Natural Language Processing Thesis project: Casual Dialog Generation with different personas . For example, imagine a chatbot from the TV Series Friends that talks like your favorite character (Joey, Chandler etc.) Key Assets: Being a PhD student set me up to work with little guidance and being self-taught . How I like to takle problems: always start small and with good understating of the problem, build something simple that works and keep adding and testing on that until reaching the goal. Experience Data Scientist Intern State Farm | Enterprise Data & Analytics Align machine learning and artificial inteligence research to bussiness needs. Created a Reinforcement Learning (RL) proof of concept application. Built a framework around RL that makes it expandable into other business areas. Created custom data environments to easily train and deploy RL models. May 2020 \u2013 July 2020 Data Scientist \u2013 Machine Learning Engineer University of North Texas (UNT) | Research Information Technology Supported research project data science and Machine Learning (ML) needs. Maintain latest ML frameworks for UNT\u2019s High Performance Computing (HPC). Created workshops and tutorials about cloud computing HPC services. Sep 2018 - May 2020 Machine Learning Engineer Intern State Farm | Enterprise Data & Analytics Automated model risk validation reducing deployment timeline by 90%. Deployed Python package for automated model building and data analysis. Built a user interface for automated model comparison with CI/CD pipeline. May 2019 \u2013 July 2019 Data Scientist Intern State Farm | Enterprise &Data Analytics Introduced GPUs in ML tutorials that speed up training by over 50%. Increased model performance used in claims process by 5% using text data. Built Deep Learning Optical Character Recognition with over 75% accuracy. May 2018 \u2013 August 2018 Teaching Assistant \u2013 Computer Science University of North Texas (UNT) | Department of Computer Science Built and debugged C/C++ coding assignments and exams. Assisted Computer Science students as a peer mentor and grader. Sep 2017 \u2013 May 2018 Artificial Intelligence \u2013 Machine Learning Researcher University of North Texas (UNT) | Department of Computer Science Built state of the art Deep Learning models in Natural Language Processing. Implement concepts from research papers related to state of the art chatbots. Jan 2017 - Present Education PhD in Computer Science University of North Texas (UNT) | Department of Computer Science Focus on Natural Language Processing, Deep Generative Modeling, Companionable Robots, Persona Detection. May 2019 - Present | GPA 4.0 Masters in Computer Science University of North Texas (UNT) | Department of Computer Science Specialized in Machine Learning, Natural Language Processing, Big Data, Data Mining, Computer Vision and Deep Reinforcement Learning. May 2019 | GPA 3.9 Skills Reference Dr. Rodney D. Nielsen Associate Professor - Director HiLT Lab University of North Texas \u2013 Computer Science Engineer Email: Rodney.Nielsen@UNT.edu","title":"Resume"},{"location":"resume/resume/#resume--pdf","text":"","title":"Resume  pdf"},{"location":"resume/resume/#summary","text":"Interest areas: Reinforcement Learning, Computer Vision, Big Data, Scaling Machine Learning, Parallel Processing. Main Research areas: Deep Learning, Natural Language Processing Thesis project: Casual Dialog Generation with different personas . For example, imagine a chatbot from the TV Series Friends that talks like your favorite character (Joey, Chandler etc.) Key Assets: Being a PhD student set me up to work with little guidance and being self-taught . How I like to takle problems: always start small and with good understating of the problem, build something simple that works and keep adding and testing on that until reaching the goal.","title":"Summary"},{"location":"resume/resume/#experience","text":"","title":"Experience"},{"location":"resume/resume/#data-scientist-intern","text":"State Farm | Enterprise Data & Analytics Align machine learning and artificial inteligence research to bussiness needs. Created a Reinforcement Learning (RL) proof of concept application. Built a framework around RL that makes it expandable into other business areas. Created custom data environments to easily train and deploy RL models. May 2020 \u2013 July 2020","title":"Data Scientist Intern"},{"location":"resume/resume/#data-scientist--machine-learning-engineer","text":"University of North Texas (UNT) | Research Information Technology Supported research project data science and Machine Learning (ML) needs. Maintain latest ML frameworks for UNT\u2019s High Performance Computing (HPC). Created workshops and tutorials about cloud computing HPC services. Sep 2018 - May 2020","title":"Data Scientist \u2013 Machine Learning Engineer"},{"location":"resume/resume/#machine-learning-engineer-intern","text":"State Farm | Enterprise Data & Analytics Automated model risk validation reducing deployment timeline by 90%. Deployed Python package for automated model building and data analysis. Built a user interface for automated model comparison with CI/CD pipeline. May 2019 \u2013 July 2019","title":"Machine Learning Engineer Intern"},{"location":"resume/resume/#data-scientist-intern_1","text":"State Farm | Enterprise &Data Analytics Introduced GPUs in ML tutorials that speed up training by over 50%. Increased model performance used in claims process by 5% using text data. Built Deep Learning Optical Character Recognition with over 75% accuracy. May 2018 \u2013 August 2018","title":"Data Scientist Intern"},{"location":"resume/resume/#teaching-assistant--computer-science","text":"University of North Texas (UNT) | Department of Computer Science Built and debugged C/C++ coding assignments and exams. Assisted Computer Science students as a peer mentor and grader. Sep 2017 \u2013 May 2018","title":"Teaching Assistant \u2013 Computer Science"},{"location":"resume/resume/#artificial-intelligence--machine-learning-researcher","text":"University of North Texas (UNT) | Department of Computer Science Built state of the art Deep Learning models in Natural Language Processing. Implement concepts from research papers related to state of the art chatbots. Jan 2017 - Present","title":"Artificial Intelligence \u2013 Machine Learning Researcher"},{"location":"resume/resume/#education","text":"","title":"Education"},{"location":"resume/resume/#phd-in-computer-science","text":"University of North Texas (UNT) | Department of Computer Science Focus on Natural Language Processing, Deep Generative Modeling, Companionable Robots, Persona Detection. May 2019 - Present | GPA 4.0","title":"PhD in Computer Science"},{"location":"resume/resume/#masters-in-computer-science","text":"University of North Texas (UNT) | Department of Computer Science Specialized in Machine Learning, Natural Language Processing, Big Data, Data Mining, Computer Vision and Deep Reinforcement Learning. May 2019 | GPA 3.9","title":"Masters in Computer Science"},{"location":"resume/resume/#skills","text":"","title":"Skills"},{"location":"resume/resume/#reference","text":"","title":"Reference"},{"location":"resume/resume/#dr-rodney-d-nielsen","text":"Associate Professor - Director HiLT Lab University of North Texas \u2013 Computer Science Engineer Email: Rodney.Nielsen@UNT.edu","title":"Dr. Rodney D. Nielsen"},{"location":"tutorial_notebooks/bert_inner_workings/","text":"\u2699\ufe0f Bert Inner Workings Let's look at how an input flows through Bert. Disclaimer: The format of this tutorial notebook is very similar to my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format. Main idea: I created this notebook to better understand the inner workings of Bert. I followed a lot of tutorials to try to understand the architecture, but I was never able to really understand what was happening under the hood. For me it always helps to see the actual code instead of just simple abstract diagrams that a lot of times don't match the actual implementation. If you're like me than this tutorial will help! I went as deep as you can go with Deep Learning - all the way to the tensor level. For me it helps to see the code and how the tensors move between layers. I feel like this level of abstraction is close enough to the core of the model to perfectly understand the inner workings. I will use the implementation of Bert from one of the best NLP library out there - HuggingFace Transformers . More specifically, I will show the inner working of Bert For Sequence Classification . The term forward pass is used in Neural Networks and it refers to the calculations involved from the input sequence all the way to output of the last layer. It's basically the flow of data from input to output. I will follow the code from an example input sequence all the way to the final output prediction. What should I know for this notebook? Some prior knowledge of Bert is needed. I won't go into any details of how Bert works. For this there is plenty of information out there. Since I am using the PyTorch implementation of Bert any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too. How deep are we going? I think the best way to understand such a complex model as Bert is to see the actual layer components that are used. I will dig in the code until I see the actual PyTorch layers used torch.nn . In my opinion there is no need to go deeper than the torch.nn layers. Tutorial Structure Each section contains multiple subsections. The order of each section matches the order of the model's layers from input to output. At the beginning of each section of code I created a diagram to illustrate the flow of tensors of that particular code. I created the diagrams following the model's implementation. The major section Bert For Sequence Classification starts with the Class Call that shows how we normally create the Bert model for sequence classification and perform a forward pass. Class Components contains the components of BertForSequenceClassification implementation. At the end of each major section, I assemble all components from that section and show the output and diagram. At the end of the notebook, I have all the code parts and diagrams assembled. Terminology I will use regular deep learning terminology found in most Bert tutorials. I'm using some terms in a slightly different way: Layer and layers : In this tutorial when I mention layer it can be an abstraction of a group of layers or just a single layer. When I reach torch.nn you know I refer to a single layer. torch.nn : I'm referring to any PyTorch layer module. This is the deepest I will go in this tutorial. How to use this notebook? The purpose of this notebook is purely educational. This notebook is to be used to align known information on how Bert woks with the code implementation of Bert. I used the Bert implementation from Transformers . My contribution is on arranging the code implementation and creating associated diagrams. Dataset For simplicity I will only use two sentences as our data input: I love cats! and He hates pineapple pizza. . I'll pretend to do binary sentiment classification on these two sentences. Coding Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant\u200a-\u200ashow the output. I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial, I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations. Installs transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. # install the transformers library ! pip install - q git + https : // github . com / huggingface / transformers . git Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 6.7MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 48.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 49.0MB/s Building wheel for transformers (PEP 517) ... done Building wheel for sacremoses (setup.py) ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Imports Import all needed libraries for this notebook. Declare parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. n_labels - How many labels are we using in this dataset. This is used to decide size of classification head. ACT2FN - Dictionary for special activation functions used in Bert. We'll only need the gelu activation function. BertLayerNorm - Shortcut for calling the PyTorch normalization layer torch.nn.LayerNorm . import math import torch from transformers.activations import gelu from transformers import ( BertTokenizer , BertConfig , BertForSequenceClassification , BertPreTrainedModel , apply_chunking_to_forward , set_seed , ) from transformers.modeling_outputs import ( BaseModelOutputWithPastAndCrossAttentions , BaseModelOutputWithPoolingAndCrossAttentions , SequenceClassifierOutput , ) # Set seed for reproducibility. set_seed ( 123 ) # How many labels are we using in training. # This is used to decide size of classification head. n_labels = 2 # GELU Activation function. ACT2FN = { \"gelu\" : gelu } # Define BertLayerNorm. BertLayerNorm = torch . nn . LayerNorm Define Input Let's define some text data on which we will use Bert to classify as positive or negative. We encoded our positive and negative sentiments into: * 0 - for negative sentiments. * 1 - for positive sentiments. # Array of text we want to classify input_texts = [ 'I love cats!' , \"He hates pineapple pizza.\" ] # Senitmen labels labels = [ 1 , 0 ] Bert Tokenizer Creating the tokenizer is pretty standard when using the Transformers library. Using our newly created tokenizer we'll use it on our two sentence dataset and create the input_sequence that will be used as input for our Bert model. Show Bert Tokenizer Diagram # Create BertTokenizer. tokenizer = BertTokenizer . from_pretrained ( 'bert-base-cased' ) # Create input sequence using tokenizer. input_sequences = tokenizer ( text = input_texts , add_special_tokens = True , padding = True , truncation = True , return_tensors = 'pt' ) # Since input_sequence is a dictionary we can also add the labels to it # want to make sure all values ar tensors. input_sequences . update ({ 'labels' : torch . tensor ( labels )}) # The tokenizer will return a dictionary of three: input_ids, attention_mask and token_type_ids. # Let's do a pretty print. print ( 'PRETTY PRINT OF `input_sequences` UPDATED WITH `labels`:' ) [ print ( ' %s : %s \\n ' % ( k , v )) for k , v in input_sequences . items ()]; # Lets see how the text looks like after Bert Tokenizer. # We see the special tokens added. print ( 'ORIGINAL TEXT:' ) [ print ( example ) for example in input_texts ]; print ( ' \\n TEXT AFTER USING `BertTokenizer`:' ) [ print ( tokenizer . decode ( example )) for example in input_sequences [ 'input_ids' ] . numpy ()]; Downloading: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 213k/213k [00:00<00:00, 278kB/s] PRETTY PRINT OF `input_sequences` UPDATED WITH `labels`: input_ids : tensor([[ 101, 146, 1567, 11771, 106, 102, 0, 0, 0], [ 101, 1124, 18457, 10194, 11478, 7136, 13473, 119, 102]]) token_type_ids : tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]) attention_mask : tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1]]) labels : tensor([1, 0]) ORIGINAL TEXT: I love cats! He hates pineapple pizza. TEXT AFTER USING `BertTokenizer`: [CLS] I love cats! [SEP] [PAD] [PAD] [PAD] [CLS] He hates pineapple pizza. [SEP] Bert Configuration Predefined values specific to Bert architecture already defined for us by Hugging Face. # Create the bert configuration. bert_configuraiton = BertConfig . from_pretrained ( 'bert-base-cased' ) # Let's see number of layers. print ( 'NUMBER OF LAYERS:' , bert_configuraiton . num_hidden_layers ) # We can also see the size of embeddings inside Bert. print ( 'EMBEDDING SIZE:' , bert_configuraiton . hidden_size ) # See which activation function used in hidden layers. print ( 'ACTIVATIONS:' , bert_configuraiton . hidden_act ) Downloading: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 433/433 [00:00<00:00, 15.5kB/s] NUMBER OF LAYERS: 12 EMBEDDING SIZE: 768 ACTIVATIONS: gelu Bert For Sequence Classification I will go over the Bert for Sequence Classification model. This is a Bert language model with a classification layer on top. If you plan on looking at other transformers models his tutorial will be very similar. Class Call Let's start with doing a forward pass using the whole model call from Hugging Face Transformer. # Let' start with the final model how we normally use. model = BertForSequenceClassification . from_pretrained ( 'bert-base-cased' ) # Perform a forward pass. We only care about the output and no gradients. with torch . no_grad (): output = model . forward ( ** input_sequences ) print () # Let's check how a forward pass output looks like. print ( 'FORWARD PASS OUTPUT:' , output ) Downloading: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 436M/436M [00:07<00:00, 61.3MB/s] Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. FORWARD PASS OUTPUT: SequenceClassifierOutput(loss=tensor(0.7454), logits=tensor([[ 0.2661, -0.1774], [ 0.2223, -0.0847]]), hidden_states=None, attentions=None) Class Components Now let's look at the code implementation and break down each part of the model and check the outputs. Start with the BertForSequenceClassification found in transformers/src/transformers/models/bert/modeling_bert.py#L1449 . The forward pass uses the following layers: BertModel layer: self.bert = BertModel(config) torch.nn.Dropout layer for dropout: self.dropout = nn.Dropout(config.hidden_dropout_prob) torch.nn.Linear layer used for classification: self.classifier = nn.Linear(config.hidden_size, config.num_labels) BertModel This is the core Bert model that can be found at: transformers/src/transformers/models/bert/modeling_bert.py#L815 . Hugging Face was nice enough to mention a small summary: The bare Bert Model transformer outputting raw hidden-states without any specific head on top. The forward pass uses the following layers: BertEmbeddings layer: self.embeddings = BertEmbeddings(config) BertEncoder layer: self.encoder = BertEncoder(config) BertPooler layer: self.pooler = BertPooler(config) Bert Embeddings This is where we feed the input_sequences created under Bert Tokenizer and get our first embeddings. Implementation can be found at: transformers/src/transformers/models/bert/modeling_bert.py#L165 . This layer contains actual PyTorch layers. I won't go into farther details since this is how far we need to go. The forward pass uses following layers: torch.nn.Embedding layer for word embeddings: self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id) torch.nn.Embedding layer for position embeddings: self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) torch.nn.Embedding for token type embeddings: self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size) torch.nn.LayerNorm layer for normalization: self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) torch.nn.Dropout layer for dropout: self.dropout = nn.Dropout(config.hidden_dropout_prob) Show Bert Embeddings Diagram class BertEmbeddings ( torch . nn . Module ): \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\" def __init__ ( self , config ): super () . __init__ () self . word_embeddings = torch . nn . Embedding ( config . vocab_size , config . hidden_size , padding_idx = config . pad_token_id ) self . position_embeddings = torch . nn . Embedding ( config . max_position_embeddings , config . hidden_size ) self . token_type_embeddings = torch . nn . Embedding ( config . type_vocab_size , config . hidden_size ) # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load # any TensorFlow checkpoint file self . LayerNorm = torch . nn . LayerNorm ( config . hidden_size , eps = config . layer_norm_eps ) self . dropout = torch . nn . Dropout ( config . hidden_dropout_prob ) # position_ids (1, len position emb) is contiguous in memory and exported when serialized self . register_buffer ( \"position_ids\" , torch . arange ( config . max_position_embeddings ) . expand (( 1 , - 1 ))) self . position_embedding_type = getattr ( config , \"position_embedding_type\" , \"absolute\" ) def forward ( self , input_ids = None , token_type_ids = None , position_ids = None , inputs_embeds = None , past_key_values_length = 0 ): if input_ids is not None : input_shape = input_ids . size () else : input_shape = inputs_embeds . size ()[: - 1 ] seq_length = input_shape [ 1 ] if position_ids is None : position_ids = self . position_ids [:, past_key_values_length : seq_length + past_key_values_length ] # ADDED print ( 'Created Tokens Positions IDs: \\n ' , position_ids ) if token_type_ids is None : token_type_ids = torch . zeros ( input_shape , dtype = torch . long , device = self . position_ids . device ) if inputs_embeds is None : inputs_embeds = self . word_embeddings ( input_ids ) token_type_embeddings = self . token_type_embeddings ( token_type_ids ) # ADDED print ( ' \\n Tokens IDs: \\n ' , input_ids . shape ) print ( ' \\n Tokens Type IDs: \\n ' , token_type_ids . shape ) print ( ' \\n Word Embeddings: \\n ' , inputs_embeds . shape ) embeddings = inputs_embeds + token_type_embeddings if self . position_embedding_type == \"absolute\" : position_embeddings = self . position_embeddings ( position_ids ) # ADDED print ( ' \\n Position Embeddings: \\n ' , position_embeddings . shape ) embeddings += position_embeddings # ADDED print ( ' \\n Token Types Embeddings: \\n ' , token_type_embeddings . shape ) print ( ' \\n Sum Up All Embeddings: \\n ' , embeddings . shape ) embeddings = self . LayerNorm ( embeddings ) # ADDED print ( ' \\n Embeddings Layer Nromalization: \\n ' , embeddings . shape ) embeddings = self . dropout ( embeddings ) # ADDED print ( ' \\n Embeddings Dropout Layer: \\n ' , embeddings . shape ) return embeddings # Create Bert embedding layer. bert_embeddings_block = BertEmbeddings ( bert_configuraiton ) # Perform a forward pass. embedding_output = bert_embeddings_block . forward ( input_ids = input_sequences [ 'input_ids' ], token_type_ids = input_sequences [ 'token_type_ids' ]) Created Tokens Positions IDs: tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]]) Tokens IDs: torch.Size([2, 9]) Tokens Type IDs: torch.Size([2, 9]) Word Embeddings: torch.Size([2, 9, 768]) Position Embeddings: torch.Size([1, 9, 768]) Token Types Embeddings: torch.Size([2, 9, 768]) Sum Up All Embeddings: torch.Size([2, 9, 768]) Embeddings Layer Nromalization: torch.Size([2, 9, 768]) Embeddings Dropout Layer: torch.Size([2, 9, 768]) Bert Encoder This layer contains the core of the bert model where the self-attention happens. The implementation can be found at: transformers/src/transformers/models/bert/modeling_bert.py#L512 . The forward pass uses: 12 of the BertLayer layers ( in this setup config.num_hidden_layers=12 ): self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)]) Bert Layer This layer contains basic components of the self-attention implementation. Implementation can be found at transformers/src/transformers/models/bert/modeling_bert.py#L429 . The forward pass uses: BertAttention layer: self.attention = BertAttention(config) BertIntermediate layer: self.intermediate = BertIntermediate(config) BertOutput layer: self.output = BertOutput(config) Bert Attention This layer contains basic components of the self-attention implementation. Implementation can be found at transformers/src/transformers/models/bert/modeling_bert.py#L351 . The forward pass uses: BertSelfAttention layer: self.self = BertSelfAttention(config) BertSelfOutput layer: self.output = BertSelfOutput(config) # BertSelfAttention This layer contains the torch.nn basic components of the self-attention implementation. Implementation can be found at transformers/src/transformers/models/bert/modeling_bert.py#L212 . The forward pass uses: torch.nn.Linear used for the Query layer: self.query = nn.Linear(config.hidden_size, self.all_head_size) torch.nn.Linear used for the Key layer: self.key = nn.Linear(config.hidden_size, self.all_head_size) torch.nn.Linear used for the Value layer: self.value = nn.Linear(config.hidden_size, self.all_head_size) torch.nn.Dropout layer for dropout: self.dropout = nn.Dropout(config.attention_probs_dropout_prob) Show BertSelfAttention Diagram class BertSelfAttention ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () if config . hidden_size % config . num_attention_heads != 0 and not hasattr ( config , \"embedding_size\" ): raise ValueError ( \"The hidden size ( %d ) is not a multiple of the number of attention \" \"heads ( %d )\" % ( config . hidden_size , config . num_attention_heads ) ) self . num_attention_heads = config . num_attention_heads self . attention_head_size = int ( config . hidden_size / config . num_attention_heads ) self . all_head_size = self . num_attention_heads * self . attention_head_size # ADDED print ( 'Attention Head Size: \\n ' , self . attention_head_size ) print ( ' \\n Combined Attentions Head Size: \\n ' , self . all_head_size ) self . query = torch . nn . Linear ( config . hidden_size , self . all_head_size ) self . key = torch . nn . Linear ( config . hidden_size , self . all_head_size ) self . value = torch . nn . Linear ( config . hidden_size , self . all_head_size ) self . dropout = torch . nn . Dropout ( config . attention_probs_dropout_prob ) self . position_embedding_type = getattr ( config , \"position_embedding_type\" , \"absolute\" ) if self . position_embedding_type == \"relative_key\" or self . position_embedding_type == \"relative_key_query\" : self . max_position_embeddings = config . max_position_embeddings self . distance_embedding = nn . Embedding ( 2 * config . max_position_embeddings - 1 , self . attention_head_size ) self . is_decoder = config . is_decoder def transpose_for_scores ( self , x ): new_x_shape = x . size ()[: - 1 ] + ( self . num_attention_heads , self . attention_head_size ) x = x . view ( * new_x_shape ) return x . permute ( 0 , 2 , 1 , 3 ) def forward ( self , hidden_states , attention_mask = None , head_mask = None , encoder_hidden_states = None , encoder_attention_mask = None , past_key_value = None , output_attentions = False , ): # ADDED print ( ' \\n Hidden States: \\n ' , hidden_states . shape ) mixed_query_layer = self . query ( hidden_states ) # If this is instantiated as a cross-attention module, the keys # and values come from an encoder; the attention mask needs to be # such that the encoder's padding tokens are not attended to. is_cross_attention = encoder_hidden_states is not None if is_cross_attention and past_key_value is not None : # ADDED print ( ' \\n Query Linear Layer: \\n ' , mixed_query_layer . shape ) print ( ' \\n Key Linear Layer: \\n ' , past_key_value [ 0 ] . shape ) print ( ' \\n Value Linear Layer: \\n ' , past_key_value [ 1 ] . shape ) # reuse k,v, cross_attentions key_layer = past_key_value [ 0 ] value_layer = past_key_value [ 1 ] attention_mask = encoder_attention_mask elif is_cross_attention : # ADDED print ( ' \\n Query Linear Layer: \\n ' , mixed_query_layer . shape ) print ( ' \\n Key Linear Layer: \\n ' , self . key ( encoder_hidden_states ) . shape ) print ( ' \\n Value Linear Layer: \\n ' , self . value ( encoder_hidden_states ) . shape ) key_layer = self . transpose_for_scores ( self . key ( encoder_hidden_states )) value_layer = self . transpose_for_scores ( self . value ( encoder_hidden_states )) attention_mask = encoder_attention_mask elif past_key_value is not None : # ADDED print ( ' \\n Query Linear Layer: \\n ' , mixed_query_layer . shape ) print ( ' \\n Key Linear Layer: \\n ' , self . key ( hidden_states ) . shape ) print ( ' \\n Value Linear Layer: \\n ' , self . value ( hidden_states ) . shape ) key_layer = self . transpose_for_scores ( self . key ( hidden_states )) value_layer = self . transpose_for_scores ( self . value ( hidden_states )) key_layer = torch . cat ([ past_key_value [ 0 ], key_layer ], dim = 2 ) value_layer = torch . cat ([ past_key_value [ 1 ], value_layer ], dim = 2 ) else : # ADDED print ( ' \\n Query Linear Layer: \\n ' , mixed_query_layer . shape ) print ( ' \\n Key Linear Layer: \\n ' , self . key ( hidden_states ) . shape ) print ( ' \\n Value Linear Layer: \\n ' , self . value ( hidden_states ) . shape ) key_layer = self . transpose_for_scores ( self . key ( hidden_states )) value_layer = self . transpose_for_scores ( self . value ( hidden_states )) query_layer = self . transpose_for_scores ( mixed_query_layer ) # ADDED print ( ' \\n Query: \\n ' , query_layer . shape ) print ( ' \\n Key: \\n ' , key_layer . shape ) print ( ' \\n Value: \\n ' , value_layer . shape ) if self . is_decoder : # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states. # Further calls to cross_attention layer can then reuse all cross-attention # key/value_states (first \"if\" case) # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of # all previous decoder key/value_states. Further calls to uni-directional self-attention # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case) # if encoder bi-directional self-attention `past_key_value` is always `None` past_key_value = ( key_layer , value_layer ) # ADDED print ( ' \\n Key Transposed: \\n ' , key_layer . transpose ( - 1 , - 2 ) . shape ) # Take the dot product between \"query\" and \"key\" to get the raw attention scores. attention_scores = torch . matmul ( query_layer , key_layer . transpose ( - 1 , - 2 )) # ADDED print ( ' \\n Attention Scores: \\n ' , attention_scores . shape ) if self . position_embedding_type == \"relative_key\" or self . position_embedding_type == \"relative_key_query\" : seq_length = hidden_states . size ()[ 1 ] position_ids_l = torch . arange ( seq_length , dtype = torch . long , device = hidden_states . device ) . view ( - 1 , 1 ) position_ids_r = torch . arange ( seq_length , dtype = torch . long , device = hidden_states . device ) . view ( 1 , - 1 ) distance = position_ids_l - position_ids_r positional_embedding = self . distance_embedding ( distance + self . max_position_embeddings - 1 ) positional_embedding = positional_embedding . to ( dtype = query_layer . dtype ) # fp16 compatibility if self . position_embedding_type == \"relative_key\" : relative_position_scores = torch . einsum ( \"bhld,lrd->bhlr\" , query_layer , positional_embedding ) attention_scores = attention_scores + relative_position_scores elif self . position_embedding_type == \"relative_key_query\" : relative_position_scores_query = torch . einsum ( \"bhld,lrd->bhlr\" , query_layer , positional_embedding ) relative_position_scores_key = torch . einsum ( \"bhrd,lrd->bhlr\" , key_layer , positional_embedding ) attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key attention_scores = attention_scores / math . sqrt ( self . attention_head_size ) # ADDED print ( ' \\n Attention Scores Divided by Scalar: \\n ' , attention_scores . shape ) if attention_mask is not None : # Apply the attention mask is (precomputed for all layers in BertModel forward() function) attention_scores = attention_scores + attention_mask # Normalize the attention scores to probabilities. attention_probs = torch . nn . Softmax ( dim =- 1 )( attention_scores ) # ADDED print ( ' \\n Attention Probabilities Softmax Layer: \\n ' , attention_probs . shape ) # This is actually dropping out entire tokens to attend to, which might # seem a bit unusual, but is taken from the original Transformer paper. attention_probs = self . dropout ( attention_probs ) # ADDED print ( ' \\n Attention Probabilities Dropout Layer: \\n ' , attention_probs . shape ) # Mask heads if we want to if head_mask is not None : attention_probs = attention_probs * head_mask context_layer = torch . matmul ( attention_probs , value_layer ) # ADDED print ( ' \\n Context: \\n ' , context_layer . shape ) context_layer = context_layer . permute ( 0 , 2 , 1 , 3 ) . contiguous () # ADDED print ( ' \\n Context Permute: \\n ' , context_layer . shape ) new_context_layer_shape = context_layer . size ()[: - 2 ] + ( self . all_head_size ,) context_layer = context_layer . view ( * new_context_layer_shape ) # ADDED print ( ' \\n Context Reshaped: \\n ' , context_layer . shape ) outputs = ( context_layer , attention_probs ) if output_attentions else ( context_layer ,) if self . is_decoder : outputs = outputs + ( past_key_value ,) return outputs # Create bert self attention layer. bert_selfattention_block = BertSelfAttention ( bert_configuraiton ) # Perform a forward pass. context_embedding = bert_selfattention_block . forward ( hidden_states = embedding_output ) Attention Head Size: 64 Combined Attentions Head Size: 768 Hidden States: torch.Size([2, 9, 768]) Query Linear Layer: torch.Size([2, 9, 768]) Key Linear Layer: torch.Size([2, 9, 768]) Value Linear Layer: torch.Size([2, 9, 768]) Query: torch.Size([2, 12, 9, 64]) Key: torch.Size([2, 12, 9, 64]) Value: torch.Size([2, 12, 9, 64]) Key Transposed: torch.Size([2, 12, 64, 9]) Attention Scores: torch.Size([2, 12, 9, 9]) Attention Scores Divided by Scalar: torch.Size([2, 12, 9, 9]) Attention Probabilities Softmax Layer: torch.Size([2, 12, 9, 9]) Attention Probabilities Dropout Layer: torch.Size([2, 12, 9, 9]) Context: torch.Size([2, 12, 9, 64]) Context Permute: torch.Size([2, 9, 12, 64]) Context Reshaped: torch.Size([2, 9, 768]) # BertSelfOutput This layer contains the torch.nn basic components of the self-attention implementation. Implementation can be found at transformers/src/transformers/models/bert/modeling_bert.py#L337 . The forward pass uses: torch.nn.Linear layer: self.dense = nn.Linear(config.hidden_size, config.hidden_size) torch.nn.LayerNorm layer for normalization: self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) torch.nn.Dropout layer for dropout: self.dropout = nn.Dropout(config.hidden_dropout_prob) Show BertSelfOutput Diagram class BertSelfOutput ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . dense = torch . nn . Linear ( config . hidden_size , config . hidden_size ) self . LayerNorm = BertLayerNorm ( config . hidden_size , eps = config . layer_norm_eps ) self . dropout = torch . nn . Dropout ( config . hidden_dropout_prob ) def forward ( self , hidden_states , input_tensor ): print ( 'Hidden States: \\n ' , hidden_states . shape ) hidden_states = self . dense ( hidden_states ) print ( ' \\n Hidden States Linear Layer: \\n ' , hidden_states . shape ) hidden_states = self . dropout ( hidden_states ) print ( ' \\n Hidden States Dropout Layer: \\n ' , hidden_states . shape ) hidden_states = self . LayerNorm ( hidden_states + input_tensor ) print ( ' \\n Hidden States Normalization Layer: \\n ' , hidden_states . shape ) return hidden_states # Create Bert self output layer. bert_selfoutput_block = BertSelfOutput ( bert_configuraiton ) # Perform a forward pass - context_embedding[0] because we have tuple. attention_output = bert_selfoutput_block . forward ( hidden_states = context_embedding [ 0 ], input_tensor = embedding_output ) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Normalization Layer: torch.Size([2, 9, 768]) # Assemble BertAttention Put together BertSelfAttention layer and BertSelfOutput layer to create the BertAttention layer . Now perform a forward pass using previous output layer as input. Show BertAttention Diagram class BertAttention ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . self = BertSelfAttention ( config ) self . output = BertSelfOutput ( config ) self . pruned_heads = set () def prune_heads ( self , heads ): if len ( heads ) == 0 : return heads , index = find_pruneable_heads_and_indices ( heads , self . self . num_attention_heads , self . self . attention_head_size , self . pruned_heads ) # Prune linear layers self . self . query = prune_linear_layer ( self . self . query , index ) self . self . key = prune_linear_layer ( self . self . key , index ) self . self . value = prune_linear_layer ( self . self . value , index ) self . output . dense = prune_linear_layer ( self . output . dense , index , dim = 1 ) # Update hyper params and store pruned heads self . self . num_attention_heads = self . self . num_attention_heads - len ( heads ) self . self . all_head_size = self . self . attention_head_size * self . self . num_attention_heads self . pruned_heads = self . pruned_heads . union ( heads ) def forward ( self , hidden_states , attention_mask = None , head_mask = None , encoder_hidden_states = None , encoder_attention_mask = None , past_key_value = None , output_attentions = False , ): self_outputs = self . self ( hidden_states , attention_mask , head_mask , encoder_hidden_states , encoder_attention_mask , past_key_value , output_attentions , ) attention_output = self . output ( self_outputs [ 0 ], hidden_states ) outputs = ( attention_output ,) + self_outputs [ 1 :] # add attentions if we output them return outputs # Create attention assembled layer. bert_attention_block = BertAttention ( bert_configuraiton ) # Perform a forward pass to wholte Bert Attention layer. attention_output = bert_attention_block ( hidden_states = embedding_output ) Attention Head Size: 64 Combined Attentions Head Size: 768 Hidden States: torch.Size([2, 9, 768]) Query Linear Layer: torch.Size([2, 9, 768]) Key Linear Layer: torch.Size([2, 9, 768]) Value Linear Layer: torch.Size([2, 9, 768]) Query: torch.Size([2, 12, 9, 64]) Key: torch.Size([2, 12, 9, 64]) Value: torch.Size([2, 12, 9, 64]) Key Transposed: torch.Size([2, 12, 64, 9]) Attention Scores: torch.Size([2, 12, 9, 9]) Attention Scores Divided by Scalar: torch.Size([2, 12, 9, 9]) Attention Probabilities Softmax Layer: torch.Size([2, 12, 9, 9]) Attention Probabilities Dropout Layer: torch.Size([2, 12, 9, 9]) Context: torch.Size([2, 12, 9, 64]) Context Permute: torch.Size([2, 9, 12, 64]) Context Reshaped: torch.Size([2, 9, 768]) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Normalization Layer: torch.Size([2, 9, 768]) BertIntermediate This layer contains the torch.nn basic components of the Bert model implementation. Implementation can be found at transformers/src/transformers/models/bert/modeling_bert.py#L400 . The forward pass uses: torch.nn.Linear layer: self.dense = nn.Linear(config.hidden_size, config.intermediate_size) Show BertIntermediate Diagram class BertIntermediate ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . dense = torch . nn . Linear ( config . hidden_size , config . intermediate_size ) if isinstance ( config . hidden_act , str ): self . intermediate_act_fn = ACT2FN [ config . hidden_act ] else : self . intermediate_act_fn = config . hidden_act def forward ( self , hidden_states ): print ( ' \\n Hidden States: \\n ' , hidden_states . shape ) hidden_states = self . dense ( hidden_states ) print ( ' \\n Hidden States Linear Layer: \\n ' , hidden_states . shape ) hidden_states = self . intermediate_act_fn ( hidden_states ) print ( ' \\n Hidden States Gelu Activation Function: \\n ' , hidden_states . shape ) return hidden_states # Create bert intermediate layer. bert_intermediate_block = BertIntermediate ( bert_configuraiton ) # Perform a forward pass - attention_output[0] because we have tuple. intermediate_output = bert_intermediate_block . forward ( hidden_states = attention_output [ 0 ]) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 3072]) Hidden States Gelu Activation Function: torch.Size([2, 9, 3072]) BertOutput This layer contains the torch.nn basic components of the Bert model implementation. Implementation can be found at transformers/src/transformers/models/bert/modeling_bert.py#L415 . The forward pass uses: torch.nn.Linear layer: self.dense = nn.Linear(config.intermediate_size, config.hidden_size) torch.nn.LayerNorm layer for normalization: self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) torch.nn.Dropout layer for dropout: self.dropout = nn.Dropout(config.hidden_dropout_prob) Show BertOutput Diagram class BertOutput ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . dense = torch . nn . Linear ( config . intermediate_size , config . hidden_size ) self . LayerNorm = BertLayerNorm ( config . hidden_size , eps = config . layer_norm_eps ) self . dropout = torch . nn . Dropout ( config . hidden_dropout_prob ) def forward ( self , hidden_states , input_tensor ): print ( ' \\n Hidden States: \\n ' , hidden_states . shape ) hidden_states = self . dense ( hidden_states ) print ( ' \\n Hidden States Linear Layer: \\n ' , hidden_states . shape ) hidden_states = self . dropout ( hidden_states ) print ( ' \\n Hidden States Dropout Layer: \\n ' , hidden_states . shape ) hidden_states = self . LayerNorm ( hidden_states + input_tensor ) print ( ' \\n Hidden States Layer Normalization: \\n ' , hidden_states . shape ) return hidden_states # Create bert output layer. bert_output_block = BertOutput ( bert_configuraiton ) # Perform forward pass - attention_output[0] dealing with tuple. layer_output = bert_output_block . forward ( hidden_states = intermediate_output , input_tensor = attention_output [ 0 ]) Hidden States: torch.Size([2, 9, 3072]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Layer Normalization: torch.Size([2, 9, 768]) Assemble BertLayer Put together BertAttention layer, BertIntermediate layer and BertOutput layer to create the BertLayer layer . Now perform a forward pass using previous output layer as input. Show BertLayer Diagram class BertLayer ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . chunk_size_feed_forward = config . chunk_size_feed_forward self . seq_len_dim = 1 self . attention = BertAttention ( config ) self . is_decoder = config . is_decoder self . add_cross_attention = config . add_cross_attention if self . add_cross_attention : assert self . is_decoder , f \" { self } should be used as a decoder model if cross attention is added\" self . crossattention = BertAttention ( config ) self . intermediate = BertIntermediate ( config ) self . output = BertOutput ( config ) def forward ( self , hidden_states , attention_mask = None , head_mask = None , encoder_hidden_states = None , encoder_attention_mask = None , past_key_value = None , output_attentions = False , ): # decoder uni-directional self-attention cached key/values tuple is at positions 1,2 self_attn_past_key_value = past_key_value [: 2 ] if past_key_value is not None else None self_attention_outputs = self . attention ( hidden_states , attention_mask , head_mask , output_attentions = output_attentions , past_key_value = self_attn_past_key_value , ) attention_output = self_attention_outputs [ 0 ] # if decoder, the last output is tuple of self-attn cache if self . is_decoder : outputs = self_attention_outputs [ 1 : - 1 ] present_key_value = self_attention_outputs [ - 1 ] else : outputs = self_attention_outputs [ 1 :] # add self attentions if we output attention weights cross_attn_present_key_value = None if self . is_decoder and encoder_hidden_states is not None : assert hasattr ( self , \"crossattention\" ), f \"If `encoder_hidden_states` are passed, { self } has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\" # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple cross_attn_past_key_value = past_key_value [ - 2 :] if past_key_value is not None else None cross_attention_outputs = self . crossattention ( attention_output , attention_mask , head_mask , encoder_hidden_states , encoder_attention_mask , cross_attn_past_key_value , output_attentions , ) attention_output = cross_attention_outputs [ 0 ] outputs = outputs + cross_attention_outputs [ 1 : - 1 ] # add cross attentions if we output attention weights # add cross-attn cache to positions 3,4 of present_key_value tuple cross_attn_present_key_value = cross_attention_outputs [ - 1 ] present_key_value = present_key_value + cross_attn_present_key_value layer_output = apply_chunking_to_forward ( self . feed_forward_chunk , self . chunk_size_feed_forward , self . seq_len_dim , attention_output ) outputs = ( layer_output ,) + outputs # if decoder, return the attn key/values as the last output if self . is_decoder : outputs = outputs + ( present_key_value ,) return outputs def feed_forward_chunk ( self , attention_output ): intermediate_output = self . intermediate ( attention_output ) layer_output = self . output ( intermediate_output , attention_output ) return layer_output # Assemble block to create Bert Layer. bert_layer_block = BertLayer ( bert_configuraiton ) # Perform feed forward on a whole Bert Layer. layer_output = bert_layer_block . forward ( hidden_states = embedding_output ) Attention Head Size: 64 Combined Attentions Head Size: 768 Hidden States: torch.Size([2, 9, 768]) Query Linear Layer: torch.Size([2, 9, 768]) Key Linear Layer: torch.Size([2, 9, 768]) Value Linear Layer: torch.Size([2, 9, 768]) Query: torch.Size([2, 12, 9, 64]) Key: torch.Size([2, 12, 9, 64]) Value: torch.Size([2, 12, 9, 64]) Key Transposed: torch.Size([2, 12, 64, 9]) Attention Scores: torch.Size([2, 12, 9, 9]) Attention Scores Divided by Scalar: torch.Size([2, 12, 9, 9]) Attention Probabilities Softmax Layer: torch.Size([2, 12, 9, 9]) Attention Probabilities Dropout Layer: torch.Size([2, 12, 9, 9]) Context: torch.Size([2, 12, 9, 64]) Context Permute: torch.Size([2, 9, 12, 64]) Context Reshaped: torch.Size([2, 9, 768]) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Normalization Layer: torch.Size([2, 9, 768]) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 3072]) Hidden States Gelu Activation Function: torch.Size([2, 9, 3072]) Hidden States: torch.Size([2, 9, 3072]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Layer Normalization: torch.Size([2, 9, 768]) Assemble BertEncoder Put together 12 of the BertLayer layers ( in this setup config.num_hidden_layers=12 ) to create the BertEncoder layer. Now perform a forward pass using previous output layer as input. Show BertEncoder Diagram class BertEncoder ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . config = config self . layer = torch . nn . ModuleList ([ BertLayer ( config ) for _ in range ( config . num_hidden_layers )]) def forward ( self , hidden_states , attention_mask = None , head_mask = None , encoder_hidden_states = None , encoder_attention_mask = None , past_key_values = None , use_cache = None , output_attentions = False , output_hidden_states = False , return_dict = True , ): all_hidden_states = () if output_hidden_states else None all_self_attentions = () if output_attentions else None all_cross_attentions = () if output_attentions and self . config . add_cross_attention else None next_decoder_cache = () if use_cache else None for i , layer_module in enumerate ( self . layer ): # ADDED print ( ' \\n ----------------- BERT LAYER %d -----------------' % ( i + 1 )) if output_hidden_states : all_hidden_states = all_hidden_states + ( hidden_states ,) layer_head_mask = head_mask [ i ] if head_mask is not None else None past_key_value = past_key_values [ i ] if past_key_values is not None else None if getattr ( self . config , \"gradient_checkpointing\" , False ): def create_custom_forward ( module ): def custom_forward ( * inputs ): return module ( * inputs , past_key_value , output_attentions ) return custom_forward layer_outputs = torch . utils . checkpoint . checkpoint ( create_custom_forward ( layer_module ), hidden_states , attention_mask , layer_head_mask , encoder_hidden_states , encoder_attention_mask , ) else : layer_outputs = layer_module ( hidden_states , attention_mask , layer_head_mask , encoder_hidden_states , encoder_attention_mask , past_key_value , output_attentions , ) hidden_states = layer_outputs [ 0 ] if use_cache : next_decoder_cache += ( layer_outputs [ - 1 ],) if output_attentions : all_self_attentions = all_self_attentions + ( layer_outputs [ 1 ],) if self . config . add_cross_attention : all_cross_attentions = all_cross_attentions + ( layer_outputs [ 2 ],) if output_hidden_states : all_hidden_states = all_hidden_states + ( hidden_states ,) if not return_dict : return tuple ( v for v in [ hidden_states , next_decoder_cache , all_hidden_states , all_self_attentions , all_cross_attentions , ] if v is not None ) return BaseModelOutputWithPastAndCrossAttentions ( last_hidden_state = hidden_states , past_key_values = next_decoder_cache , hidden_states = all_hidden_states , attentions = all_self_attentions , cross_attentions = all_cross_attentions , ) # create bert encoder block by stacking 12 layers bert_encoder_block = BertEncoder ( bert_configuraiton ) # perform forward pass on entire Bert Encoder encoder_embedding = bert_encoder_block . forward ( hidden_states = embedding_output ) Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 ----------------- BERT LAYER 1 ----------------- Hidden States: torch.Size([2, 9, 768]) Query Linear Layer: torch.Size([2, 9, 768]) Key Linear Layer: torch.Size([2, 9, 768]) Value Linear Layer: torch.Size([2, 9, 768]) Query: torch.Size([2, 12, 9, 64]) Key: torch.Size([2, 12, 9, 64]) Value: torch.Size([2, 12, 9, 64]) Key Transposed: torch.Size([2, 12, 64, 9]) Attention Scores: torch.Size([2, 12, 9, 9]) Attention Scores Divided by Scalar: torch.Size([2, 12, 9, 9]) Attention Probabilities Softmax Layer: torch.Size([2, 12, 9, 9]) Attention Probabilities Dropout Layer: torch.Size([2, 12, 9, 9]) Context: torch.Size([2, 12, 9, 64]) Context Permute: torch.Size([2, 9, 12, 64]) Context Reshaped: torch.Size([2, 9, 768]) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Normalization Layer: torch.Size([2, 9, 768]) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 3072]) Hidden States Gelu Activation Function: torch.Size([2, 9, 3072]) Hidden States: torch.Size([2, 9, 3072]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Layer Normalization: torch.Size([2, 9, 768]) ----------------- BERT LAYER 2 ----------------- ... ----------------- BERT LAYER 12 ----------------- ... BertPooler This layer contains the core of the bert model where the self-attention happens. The implementation can be found at: transformers/src/transformers/models/bert/modeling_bert.py#L601 . The forward pass uses: torch.nn.Linear layer: self.dense = torch.nn.Linear(config.hidden_size, config.hidden_size) torch.nn.Tanh activation function layer: self.activation = torch.nn.Tanh() Show BertPooler Diagram class BertPooler ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . dense = torch . nn . Linear ( config . hidden_size , config . hidden_size ) self . activation = torch . nn . Tanh () def forward ( self , hidden_states ): # We \"pool\" the model by simply taking the hidden state corresponding # to the first token. print ( ' \\n Hidden States: \\n ' , hidden_states . shape ) first_token_tensor = hidden_states [:, 0 ] print ( ' \\n First Token [CLS]: \\n ' , first_token_tensor . shape ) pooled_output = self . dense ( first_token_tensor ) print ( ' \\n First Token [CLS] Linear Layer: \\n ' , pooled_output . shape ) pooled_output = self . activation ( pooled_output ) print ( ' \\n First Token [CLS] Tanh Activation Function: \\n ' , pooled_output . shape ) return pooled_output # Create bert pooler block. bert_pooler_block = BertPooler ( bert_configuraiton ) # Perform forward pass - encoder_embedding[0] because it is a tuple. pooled_output = bert_pooler_block ( hidden_states = encoder_embedding [ 0 ]) Hidden States: torch.Size([2, 9, 768]) First Token [CLS]: torch.Size([2, 768]) First Token [CLS] Linear Layer: torch.Size([2, 768]) First Token [CLS] Tanh Activation Function: torch.Size([2, 768]) Assemble BertModel Put together BertEmbeddings layer, BertEncoder layer and BertPooler layer to create the BertModel layer. Now perform a forward pass using previous output layer as input. Show BertModel Diagram class BertModel ( BertPreTrainedModel ): \"\"\" The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of cross-attention is added between the self-attention layers, following the architecture described in `Attention is all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder` argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an input to the forward pass. \"\"\" def __init__ ( self , config , add_pooling_layer = True ): super () . __init__ ( config ) self . config = config self . embeddings = BertEmbeddings ( config ) self . encoder = BertEncoder ( config ) self . pooler = BertPooler ( config ) if add_pooling_layer else None self . init_weights () def get_input_embeddings ( self ): return self . embeddings . word_embeddings def set_input_embeddings ( self , value ): self . embeddings . word_embeddings = value def _prune_heads ( self , heads_to_prune ): \"\"\" Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base class PreTrainedModel \"\"\" for layer , heads in heads_to_prune . items (): self . encoder . layer [ layer ] . attention . prune_heads ( heads ) def forward ( self , input_ids = None , attention_mask = None , token_type_ids = None , position_ids = None , head_mask = None , inputs_embeds = None , encoder_hidden_states = None , encoder_attention_mask = None , past_key_values = None , use_cache = None , output_attentions = None , output_hidden_states = None , return_dict = None , ): r \"\"\" encoder_hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`): Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if the model is configured as a decoder. encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`): Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``: - 1 for tokens that are **not masked**, - 0 for tokens that are **masked**. past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding. If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids` (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)` instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`. use_cache (:obj:`bool`, `optional`): If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up decoding (see :obj:`past_key_values`). \"\"\" output_attentions = output_attentions if output_attentions is not None else self . config . output_attentions output_hidden_states = ( output_hidden_states if output_hidden_states is not None else self . config . output_hidden_states ) return_dict = return_dict if return_dict is not None else self . config . use_return_dict if self . config . is_decoder : use_cache = use_cache if use_cache is not None else self . config . use_cache else : use_cache = False if input_ids is not None and inputs_embeds is not None : raise ValueError ( \"You cannot specify both input_ids and inputs_embeds at the same time\" ) elif input_ids is not None : input_shape = input_ids . size () batch_size , seq_length = input_shape elif inputs_embeds is not None : input_shape = inputs_embeds . size ()[: - 1 ] batch_size , seq_length = input_shape else : raise ValueError ( \"You have to specify either input_ids or inputs_embeds\" ) device = input_ids . device if input_ids is not None else inputs_embeds . device # past_key_values_length past_key_values_length = past_key_values [ 0 ][ 0 ] . shape [ 2 ] if past_key_values is not None else 0 if attention_mask is None : attention_mask = torch . ones ((( batch_size , seq_length + past_key_values_length )), device = device ) if token_type_ids is None : token_type_ids = torch . zeros ( input_shape , dtype = torch . long , device = device ) # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length] # ourselves in which case we just need to make it broadcastable to all heads. extended_attention_mask : torch . Tensor = self . get_extended_attention_mask ( attention_mask , input_shape , device ) # If a 2D or 3D attention mask is provided for the cross-attention # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length] if self . config . is_decoder and encoder_hidden_states is not None : encoder_batch_size , encoder_sequence_length , _ = encoder_hidden_states . size () encoder_hidden_shape = ( encoder_batch_size , encoder_sequence_length ) if encoder_attention_mask is None : encoder_attention_mask = torch . ones ( encoder_hidden_shape , device = device ) encoder_extended_attention_mask = self . invert_attention_mask ( encoder_attention_mask ) else : encoder_extended_attention_mask = None # Prepare head mask if needed # 1.0 in head_mask indicate we keep the head # attention_probs has shape bsz x n_heads x N x N # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length] head_mask = self . get_head_mask ( head_mask , self . config . num_hidden_layers ) embedding_output = self . embeddings ( input_ids = input_ids , position_ids = position_ids , token_type_ids = token_type_ids , inputs_embeds = inputs_embeds , past_key_values_length = past_key_values_length , ) encoder_outputs = self . encoder ( embedding_output , attention_mask = extended_attention_mask , head_mask = head_mask , encoder_hidden_states = encoder_hidden_states , encoder_attention_mask = encoder_extended_attention_mask , past_key_values = past_key_values , use_cache = use_cache , output_attentions = output_attentions , output_hidden_states = output_hidden_states , return_dict = return_dict , ) sequence_output = encoder_outputs [ 0 ] pooled_output = self . pooler ( sequence_output ) if self . pooler is not None else None if not return_dict : return ( sequence_output , pooled_output ) + encoder_outputs [ 1 :] return BaseModelOutputWithPoolingAndCrossAttentions ( last_hidden_state = sequence_output , pooler_output = pooled_output , past_key_values = encoder_outputs . past_key_values , hidden_states = encoder_outputs . hidden_states , attentions = encoder_outputs . attentions , cross_attentions = encoder_outputs . cross_attentions , ) # Create bert model. bert_model = BertModel ( bert_configuraiton ) # Perform forward pass on entire model. hidden_states = bert_model . forward ( input_ids = input_sequences [ 'input_ids' ], attention_mask = input_sequences [ 'attention_mask' ], token_type_ids = input_sequences [ 'token_type_ids' ]) Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Created Tokens Positions IDs: tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]]) Tokens IDs: torch.Size([2, 9]) Tokens Type IDs: torch.Size([2, 9]) Word Embeddings: torch.Size([2, 9, 768]) Position Embeddings: torch.Size([1, 9, 768]) Token Types Embeddings: torch.Size([2, 9, 768]) Sum Up All Embeddings: torch.Size([2, 9, 768]) Embeddings Layer Nromalization: torch.Size([2, 9, 768]) Embeddings Dropout Layer: torch.Size([2, 9, 768]) ----------------- BERT LAYER 1 ----------------- ... ----------------- BERT LAYER 12 ----------------- \u2026 Hidden States: torch.Size([2, 9, 768]) First Token [CLS]: torch.Size([2, 768]) First Token [CLS] Linear Layer: torch.Size([2, 768]) First Token [CLS] Tanh Activation Function: torch.Size([2, 768]) Assemble Components Put together BertModel layer, torch.nn.Dropout layer and torch.nn.Linear layer to create the BertForSequenceClassification model. Now perform a forward pass using previous output layer as input. class BertForSequenceClassification ( BertPreTrainedModel ): def __init__ ( self , config ): super () . __init__ ( config ) self . num_labels = config . num_labels self . bert = BertModel ( config ) self . dropout = torch . nn . Dropout ( config . hidden_dropout_prob ) self . classifier = torch . nn . Linear ( config . hidden_size , config . num_labels ) self . init_weights () def forward ( self , input_ids = None , attention_mask = None , token_type_ids = None , position_ids = None , head_mask = None , inputs_embeds = None , labels = None , output_attentions = None , output_hidden_states = None , return_dict = None , ): r \"\"\" labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`): Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ..., config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss), If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy). \"\"\" return_dict = return_dict if return_dict is not None else self . config . use_return_dict outputs = self . bert ( input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids , position_ids = position_ids , head_mask = head_mask , inputs_embeds = inputs_embeds , output_attentions = output_attentions , output_hidden_states = output_hidden_states , return_dict = return_dict , ) pooled_output = outputs [ 1 ] pooled_output = self . dropout ( pooled_output ) logits = self . classifier ( pooled_output ) loss = None if labels is not None : if self . num_labels == 1 : # We are doing regression loss_fct = MSELoss () loss = loss_fct ( logits . view ( - 1 ), labels . view ( - 1 )) else : loss_fct = torch . nn . CrossEntropyLoss () loss = loss_fct ( logits . view ( - 1 , self . num_labels ), labels . view ( - 1 )) if not return_dict : output = ( logits ,) + outputs [ 2 :] return (( loss ,) + output ) if loss is not None else output return SequenceClassifierOutput ( loss = loss , logits = logits , hidden_states = outputs . hidden_states , attentions = outputs . attentions , ) # create Bert model with classification layer - BertForSequenceClassificatin bert_for_sequence_classification_model = BertForSequenceClassification ( bert_configuraiton ) # perform forward pass on entire model outputs = bert_for_sequence_classification_model ( ** input_sequences ) Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Created Tokens Positions IDs: tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]]) Tokens IDs: torch.Size([2, 9]) Tokens Type IDs: torch.Size([2, 9]) Word Embeddings: torch.Size([2, 9, 768]) Position Embeddings: torch.Size([1, 9, 768]) Token Types Embeddings: torch.Size([2, 9, 768]) Sum Up All Embeddings: torch.Size([2, 9, 768]) Embeddings Layer Nromalization: torch.Size([2, 9, 768]) Embeddings Dropout Layer: torch.Size([2, 9, 768]) ----------------- BERT LAYER 1 ----------------- ... ----------------- BERT LAYER 12 ----------------- ... Hidden States: torch.Size([2, 9, 768]) First Token [CLS]: torch.Size([2, 768]) First Token [CLS] Linear Layer: torch.Size([2, 768]) First Token [CLS] Tanh Activation Function: torch.Size([2, 768]) Complete Diagram If you want a .pdf version of this diagram: bert_inner_workings.pdf . If you want a .png version of this diagram: bert_inner_workings.png . Final Note If you made it this far Congrats! \ud83c\udf8a and Thank you! \ud83d\ude4f for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you see something wrong please let me know by opening an issue on my ml_things GitHub repository ! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can. Contact \ud83c\udfa3 \ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"Bert Inner Workings"},{"location":"tutorial_notebooks/bert_inner_workings/#-bert-inner-workings","text":"","title":"\u2699\ufe0f Bert Inner Workings"},{"location":"tutorial_notebooks/bert_inner_workings/#lets-look-at-how-an-input-flows-through-bert","text":"Disclaimer: The format of this tutorial notebook is very similar to my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format. Main idea: I created this notebook to better understand the inner workings of Bert. I followed a lot of tutorials to try to understand the architecture, but I was never able to really understand what was happening under the hood. For me it always helps to see the actual code instead of just simple abstract diagrams that a lot of times don't match the actual implementation. If you're like me than this tutorial will help! I went as deep as you can go with Deep Learning - all the way to the tensor level. For me it helps to see the code and how the tensors move between layers. I feel like this level of abstraction is close enough to the core of the model to perfectly understand the inner workings. I will use the implementation of Bert from one of the best NLP library out there - HuggingFace Transformers . More specifically, I will show the inner working of Bert For Sequence Classification . The term forward pass is used in Neural Networks and it refers to the calculations involved from the input sequence all the way to output of the last layer. It's basically the flow of data from input to output. I will follow the code from an example input sequence all the way to the final output prediction.","title":"Let's look at how an input flows through Bert."},{"location":"tutorial_notebooks/bert_inner_workings/#what-should-i-know-for-this-notebook","text":"Some prior knowledge of Bert is needed. I won't go into any details of how Bert works. For this there is plenty of information out there. Since I am using the PyTorch implementation of Bert any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too.","title":"What should I know for this notebook?"},{"location":"tutorial_notebooks/bert_inner_workings/#how-deep-are-we-going","text":"I think the best way to understand such a complex model as Bert is to see the actual layer components that are used. I will dig in the code until I see the actual PyTorch layers used torch.nn . In my opinion there is no need to go deeper than the torch.nn layers.","title":"How deep are we going?"},{"location":"tutorial_notebooks/bert_inner_workings/#tutorial-structure","text":"Each section contains multiple subsections. The order of each section matches the order of the model's layers from input to output. At the beginning of each section of code I created a diagram to illustrate the flow of tensors of that particular code. I created the diagrams following the model's implementation. The major section Bert For Sequence Classification starts with the Class Call that shows how we normally create the Bert model for sequence classification and perform a forward pass. Class Components contains the components of BertForSequenceClassification implementation. At the end of each major section, I assemble all components from that section and show the output and diagram. At the end of the notebook, I have all the code parts and diagrams assembled.","title":"Tutorial Structure"},{"location":"tutorial_notebooks/bert_inner_workings/#terminology","text":"I will use regular deep learning terminology found in most Bert tutorials. I'm using some terms in a slightly different way: Layer and layers : In this tutorial when I mention layer it can be an abstraction of a group of layers or just a single layer. When I reach torch.nn you know I refer to a single layer. torch.nn : I'm referring to any PyTorch layer module. This is the deepest I will go in this tutorial.","title":"Terminology"},{"location":"tutorial_notebooks/bert_inner_workings/#how-to-use-this-notebook","text":"The purpose of this notebook is purely educational. This notebook is to be used to align known information on how Bert woks with the code implementation of Bert. I used the Bert implementation from Transformers . My contribution is on arranging the code implementation and creating associated diagrams.","title":"How to use this notebook?"},{"location":"tutorial_notebooks/bert_inner_workings/#dataset","text":"For simplicity I will only use two sentences as our data input: I love cats! and He hates pineapple pizza. . I'll pretend to do binary sentiment classification on these two sentences.","title":"Dataset"},{"location":"tutorial_notebooks/bert_inner_workings/#coding","text":"Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant\u200a-\u200ashow the output. I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial, I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.","title":"Coding"},{"location":"tutorial_notebooks/bert_inner_workings/#installs","text":"transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. # install the transformers library ! pip install - q git + https : // github . com / huggingface / transformers . git Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 6.7MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 48.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 49.0MB/s Building wheel for transformers (PEP 517) ... done Building wheel for sacremoses (setup.py) ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s","title":"Installs"},{"location":"tutorial_notebooks/bert_inner_workings/#imports","text":"Import all needed libraries for this notebook. Declare parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. n_labels - How many labels are we using in this dataset. This is used to decide size of classification head. ACT2FN - Dictionary for special activation functions used in Bert. We'll only need the gelu activation function. BertLayerNorm - Shortcut for calling the PyTorch normalization layer torch.nn.LayerNorm . import math import torch from transformers.activations import gelu from transformers import ( BertTokenizer , BertConfig , BertForSequenceClassification , BertPreTrainedModel , apply_chunking_to_forward , set_seed , ) from transformers.modeling_outputs import ( BaseModelOutputWithPastAndCrossAttentions , BaseModelOutputWithPoolingAndCrossAttentions , SequenceClassifierOutput , ) # Set seed for reproducibility. set_seed ( 123 ) # How many labels are we using in training. # This is used to decide size of classification head. n_labels = 2 # GELU Activation function. ACT2FN = { \"gelu\" : gelu } # Define BertLayerNorm. BertLayerNorm = torch . nn . LayerNorm","title":"Imports"},{"location":"tutorial_notebooks/bert_inner_workings/#define-input","text":"Let's define some text data on which we will use Bert to classify as positive or negative. We encoded our positive and negative sentiments into: * 0 - for negative sentiments. * 1 - for positive sentiments. # Array of text we want to classify input_texts = [ 'I love cats!' , \"He hates pineapple pizza.\" ] # Senitmen labels labels = [ 1 , 0 ]","title":"Define Input"},{"location":"tutorial_notebooks/bert_inner_workings/#bert-tokenizer","text":"Creating the tokenizer is pretty standard when using the Transformers library. Using our newly created tokenizer we'll use it on our two sentence dataset and create the input_sequence that will be used as input for our Bert model. Show Bert Tokenizer Diagram # Create BertTokenizer. tokenizer = BertTokenizer . from_pretrained ( 'bert-base-cased' ) # Create input sequence using tokenizer. input_sequences = tokenizer ( text = input_texts , add_special_tokens = True , padding = True , truncation = True , return_tensors = 'pt' ) # Since input_sequence is a dictionary we can also add the labels to it # want to make sure all values ar tensors. input_sequences . update ({ 'labels' : torch . tensor ( labels )}) # The tokenizer will return a dictionary of three: input_ids, attention_mask and token_type_ids. # Let's do a pretty print. print ( 'PRETTY PRINT OF `input_sequences` UPDATED WITH `labels`:' ) [ print ( ' %s : %s \\n ' % ( k , v )) for k , v in input_sequences . items ()]; # Lets see how the text looks like after Bert Tokenizer. # We see the special tokens added. print ( 'ORIGINAL TEXT:' ) [ print ( example ) for example in input_texts ]; print ( ' \\n TEXT AFTER USING `BertTokenizer`:' ) [ print ( tokenizer . decode ( example )) for example in input_sequences [ 'input_ids' ] . numpy ()]; Downloading: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 213k/213k [00:00<00:00, 278kB/s] PRETTY PRINT OF `input_sequences` UPDATED WITH `labels`: input_ids : tensor([[ 101, 146, 1567, 11771, 106, 102, 0, 0, 0], [ 101, 1124, 18457, 10194, 11478, 7136, 13473, 119, 102]]) token_type_ids : tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]) attention_mask : tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1]]) labels : tensor([1, 0]) ORIGINAL TEXT: I love cats! He hates pineapple pizza. TEXT AFTER USING `BertTokenizer`: [CLS] I love cats! [SEP] [PAD] [PAD] [PAD] [CLS] He hates pineapple pizza. [SEP]","title":"Bert Tokenizer"},{"location":"tutorial_notebooks/bert_inner_workings/#bert-configuration","text":"Predefined values specific to Bert architecture already defined for us by Hugging Face. # Create the bert configuration. bert_configuraiton = BertConfig . from_pretrained ( 'bert-base-cased' ) # Let's see number of layers. print ( 'NUMBER OF LAYERS:' , bert_configuraiton . num_hidden_layers ) # We can also see the size of embeddings inside Bert. print ( 'EMBEDDING SIZE:' , bert_configuraiton . hidden_size ) # See which activation function used in hidden layers. print ( 'ACTIVATIONS:' , bert_configuraiton . hidden_act ) Downloading: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 433/433 [00:00<00:00, 15.5kB/s] NUMBER OF LAYERS: 12 EMBEDDING SIZE: 768 ACTIVATIONS: gelu","title":"Bert Configuration"},{"location":"tutorial_notebooks/bert_inner_workings/#bert-for-sequence-classification","text":"I will go over the Bert for Sequence Classification model. This is a Bert language model with a classification layer on top. If you plan on looking at other transformers models his tutorial will be very similar.","title":"Bert For Sequence Classification"},{"location":"tutorial_notebooks/bert_inner_workings/#class-call","text":"Let's start with doing a forward pass using the whole model call from Hugging Face Transformer. # Let' start with the final model how we normally use. model = BertForSequenceClassification . from_pretrained ( 'bert-base-cased' ) # Perform a forward pass. We only care about the output and no gradients. with torch . no_grad (): output = model . forward ( ** input_sequences ) print () # Let's check how a forward pass output looks like. print ( 'FORWARD PASS OUTPUT:' , output ) Downloading: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 436M/436M [00:07<00:00, 61.3MB/s] Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. FORWARD PASS OUTPUT: SequenceClassifierOutput(loss=tensor(0.7454), logits=tensor([[ 0.2661, -0.1774], [ 0.2223, -0.0847]]), hidden_states=None, attentions=None)","title":"Class Call"},{"location":"tutorial_notebooks/bert_inner_workings/#class-components","text":"Now let's look at the code implementation and break down each part of the model and check the outputs. Start with the BertForSequenceClassification found in transformers/src/transformers/models/bert/modeling_bert.py#L1449 . The forward pass uses the following layers: BertModel layer: self.bert = BertModel(config) torch.nn.Dropout layer for dropout: self.dropout = nn.Dropout(config.hidden_dropout_prob) torch.nn.Linear layer used for classification: self.classifier = nn.Linear(config.hidden_size, config.num_labels)","title":"Class Components"},{"location":"tutorial_notebooks/bert_inner_workings/#bertmodel","text":"This is the core Bert model that can be found at: transformers/src/transformers/models/bert/modeling_bert.py#L815 . Hugging Face was nice enough to mention a small summary: The bare Bert Model transformer outputting raw hidden-states without any specific head on top. The forward pass uses the following layers: BertEmbeddings layer: self.embeddings = BertEmbeddings(config) BertEncoder layer: self.encoder = BertEncoder(config) BertPooler layer: self.pooler = BertPooler(config)","title":"BertModel"},{"location":"tutorial_notebooks/bert_inner_workings/#bert-embeddings","text":"This is where we feed the input_sequences created under Bert Tokenizer and get our first embeddings. Implementation can be found at: transformers/src/transformers/models/bert/modeling_bert.py#L165 . This layer contains actual PyTorch layers. I won't go into farther details since this is how far we need to go. The forward pass uses following layers: torch.nn.Embedding layer for word embeddings: self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id) torch.nn.Embedding layer for position embeddings: self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size) torch.nn.Embedding for token type embeddings: self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size) torch.nn.LayerNorm layer for normalization: self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) torch.nn.Dropout layer for dropout: self.dropout = nn.Dropout(config.hidden_dropout_prob) Show Bert Embeddings Diagram class BertEmbeddings ( torch . nn . Module ): \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\" def __init__ ( self , config ): super () . __init__ () self . word_embeddings = torch . nn . Embedding ( config . vocab_size , config . hidden_size , padding_idx = config . pad_token_id ) self . position_embeddings = torch . nn . Embedding ( config . max_position_embeddings , config . hidden_size ) self . token_type_embeddings = torch . nn . Embedding ( config . type_vocab_size , config . hidden_size ) # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load # any TensorFlow checkpoint file self . LayerNorm = torch . nn . LayerNorm ( config . hidden_size , eps = config . layer_norm_eps ) self . dropout = torch . nn . Dropout ( config . hidden_dropout_prob ) # position_ids (1, len position emb) is contiguous in memory and exported when serialized self . register_buffer ( \"position_ids\" , torch . arange ( config . max_position_embeddings ) . expand (( 1 , - 1 ))) self . position_embedding_type = getattr ( config , \"position_embedding_type\" , \"absolute\" ) def forward ( self , input_ids = None , token_type_ids = None , position_ids = None , inputs_embeds = None , past_key_values_length = 0 ): if input_ids is not None : input_shape = input_ids . size () else : input_shape = inputs_embeds . size ()[: - 1 ] seq_length = input_shape [ 1 ] if position_ids is None : position_ids = self . position_ids [:, past_key_values_length : seq_length + past_key_values_length ] # ADDED print ( 'Created Tokens Positions IDs: \\n ' , position_ids ) if token_type_ids is None : token_type_ids = torch . zeros ( input_shape , dtype = torch . long , device = self . position_ids . device ) if inputs_embeds is None : inputs_embeds = self . word_embeddings ( input_ids ) token_type_embeddings = self . token_type_embeddings ( token_type_ids ) # ADDED print ( ' \\n Tokens IDs: \\n ' , input_ids . shape ) print ( ' \\n Tokens Type IDs: \\n ' , token_type_ids . shape ) print ( ' \\n Word Embeddings: \\n ' , inputs_embeds . shape ) embeddings = inputs_embeds + token_type_embeddings if self . position_embedding_type == \"absolute\" : position_embeddings = self . position_embeddings ( position_ids ) # ADDED print ( ' \\n Position Embeddings: \\n ' , position_embeddings . shape ) embeddings += position_embeddings # ADDED print ( ' \\n Token Types Embeddings: \\n ' , token_type_embeddings . shape ) print ( ' \\n Sum Up All Embeddings: \\n ' , embeddings . shape ) embeddings = self . LayerNorm ( embeddings ) # ADDED print ( ' \\n Embeddings Layer Nromalization: \\n ' , embeddings . shape ) embeddings = self . dropout ( embeddings ) # ADDED print ( ' \\n Embeddings Dropout Layer: \\n ' , embeddings . shape ) return embeddings # Create Bert embedding layer. bert_embeddings_block = BertEmbeddings ( bert_configuraiton ) # Perform a forward pass. embedding_output = bert_embeddings_block . forward ( input_ids = input_sequences [ 'input_ids' ], token_type_ids = input_sequences [ 'token_type_ids' ]) Created Tokens Positions IDs: tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]]) Tokens IDs: torch.Size([2, 9]) Tokens Type IDs: torch.Size([2, 9]) Word Embeddings: torch.Size([2, 9, 768]) Position Embeddings: torch.Size([1, 9, 768]) Token Types Embeddings: torch.Size([2, 9, 768]) Sum Up All Embeddings: torch.Size([2, 9, 768]) Embeddings Layer Nromalization: torch.Size([2, 9, 768]) Embeddings Dropout Layer: torch.Size([2, 9, 768])","title":"Bert Embeddings"},{"location":"tutorial_notebooks/bert_inner_workings/#bert-encoder","text":"This layer contains the core of the bert model where the self-attention happens. The implementation can be found at: transformers/src/transformers/models/bert/modeling_bert.py#L512 . The forward pass uses: 12 of the BertLayer layers ( in this setup config.num_hidden_layers=12 ): self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])","title":"Bert Encoder"},{"location":"tutorial_notebooks/bert_inner_workings/#bert-layer","text":"This layer contains basic components of the self-attention implementation. Implementation can be found at transformers/src/transformers/models/bert/modeling_bert.py#L429 . The forward pass uses: BertAttention layer: self.attention = BertAttention(config) BertIntermediate layer: self.intermediate = BertIntermediate(config) BertOutput layer: self.output = BertOutput(config)","title":"Bert Layer"},{"location":"tutorial_notebooks/bert_inner_workings/#bert-attention","text":"This layer contains basic components of the self-attention implementation. Implementation can be found at transformers/src/transformers/models/bert/modeling_bert.py#L351 . The forward pass uses: BertSelfAttention layer: self.self = BertSelfAttention(config) BertSelfOutput layer: self.output = BertSelfOutput(config)","title":"Bert Attention"},{"location":"tutorial_notebooks/bert_inner_workings/#-bertselfattention","text":"This layer contains the torch.nn basic components of the self-attention implementation. Implementation can be found at transformers/src/transformers/models/bert/modeling_bert.py#L212 . The forward pass uses: torch.nn.Linear used for the Query layer: self.query = nn.Linear(config.hidden_size, self.all_head_size) torch.nn.Linear used for the Key layer: self.key = nn.Linear(config.hidden_size, self.all_head_size) torch.nn.Linear used for the Value layer: self.value = nn.Linear(config.hidden_size, self.all_head_size) torch.nn.Dropout layer for dropout: self.dropout = nn.Dropout(config.attention_probs_dropout_prob) Show BertSelfAttention Diagram class BertSelfAttention ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () if config . hidden_size % config . num_attention_heads != 0 and not hasattr ( config , \"embedding_size\" ): raise ValueError ( \"The hidden size ( %d ) is not a multiple of the number of attention \" \"heads ( %d )\" % ( config . hidden_size , config . num_attention_heads ) ) self . num_attention_heads = config . num_attention_heads self . attention_head_size = int ( config . hidden_size / config . num_attention_heads ) self . all_head_size = self . num_attention_heads * self . attention_head_size # ADDED print ( 'Attention Head Size: \\n ' , self . attention_head_size ) print ( ' \\n Combined Attentions Head Size: \\n ' , self . all_head_size ) self . query = torch . nn . Linear ( config . hidden_size , self . all_head_size ) self . key = torch . nn . Linear ( config . hidden_size , self . all_head_size ) self . value = torch . nn . Linear ( config . hidden_size , self . all_head_size ) self . dropout = torch . nn . Dropout ( config . attention_probs_dropout_prob ) self . position_embedding_type = getattr ( config , \"position_embedding_type\" , \"absolute\" ) if self . position_embedding_type == \"relative_key\" or self . position_embedding_type == \"relative_key_query\" : self . max_position_embeddings = config . max_position_embeddings self . distance_embedding = nn . Embedding ( 2 * config . max_position_embeddings - 1 , self . attention_head_size ) self . is_decoder = config . is_decoder def transpose_for_scores ( self , x ): new_x_shape = x . size ()[: - 1 ] + ( self . num_attention_heads , self . attention_head_size ) x = x . view ( * new_x_shape ) return x . permute ( 0 , 2 , 1 , 3 ) def forward ( self , hidden_states , attention_mask = None , head_mask = None , encoder_hidden_states = None , encoder_attention_mask = None , past_key_value = None , output_attentions = False , ): # ADDED print ( ' \\n Hidden States: \\n ' , hidden_states . shape ) mixed_query_layer = self . query ( hidden_states ) # If this is instantiated as a cross-attention module, the keys # and values come from an encoder; the attention mask needs to be # such that the encoder's padding tokens are not attended to. is_cross_attention = encoder_hidden_states is not None if is_cross_attention and past_key_value is not None : # ADDED print ( ' \\n Query Linear Layer: \\n ' , mixed_query_layer . shape ) print ( ' \\n Key Linear Layer: \\n ' , past_key_value [ 0 ] . shape ) print ( ' \\n Value Linear Layer: \\n ' , past_key_value [ 1 ] . shape ) # reuse k,v, cross_attentions key_layer = past_key_value [ 0 ] value_layer = past_key_value [ 1 ] attention_mask = encoder_attention_mask elif is_cross_attention : # ADDED print ( ' \\n Query Linear Layer: \\n ' , mixed_query_layer . shape ) print ( ' \\n Key Linear Layer: \\n ' , self . key ( encoder_hidden_states ) . shape ) print ( ' \\n Value Linear Layer: \\n ' , self . value ( encoder_hidden_states ) . shape ) key_layer = self . transpose_for_scores ( self . key ( encoder_hidden_states )) value_layer = self . transpose_for_scores ( self . value ( encoder_hidden_states )) attention_mask = encoder_attention_mask elif past_key_value is not None : # ADDED print ( ' \\n Query Linear Layer: \\n ' , mixed_query_layer . shape ) print ( ' \\n Key Linear Layer: \\n ' , self . key ( hidden_states ) . shape ) print ( ' \\n Value Linear Layer: \\n ' , self . value ( hidden_states ) . shape ) key_layer = self . transpose_for_scores ( self . key ( hidden_states )) value_layer = self . transpose_for_scores ( self . value ( hidden_states )) key_layer = torch . cat ([ past_key_value [ 0 ], key_layer ], dim = 2 ) value_layer = torch . cat ([ past_key_value [ 1 ], value_layer ], dim = 2 ) else : # ADDED print ( ' \\n Query Linear Layer: \\n ' , mixed_query_layer . shape ) print ( ' \\n Key Linear Layer: \\n ' , self . key ( hidden_states ) . shape ) print ( ' \\n Value Linear Layer: \\n ' , self . value ( hidden_states ) . shape ) key_layer = self . transpose_for_scores ( self . key ( hidden_states )) value_layer = self . transpose_for_scores ( self . value ( hidden_states )) query_layer = self . transpose_for_scores ( mixed_query_layer ) # ADDED print ( ' \\n Query: \\n ' , query_layer . shape ) print ( ' \\n Key: \\n ' , key_layer . shape ) print ( ' \\n Value: \\n ' , value_layer . shape ) if self . is_decoder : # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states. # Further calls to cross_attention layer can then reuse all cross-attention # key/value_states (first \"if\" case) # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of # all previous decoder key/value_states. Further calls to uni-directional self-attention # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case) # if encoder bi-directional self-attention `past_key_value` is always `None` past_key_value = ( key_layer , value_layer ) # ADDED print ( ' \\n Key Transposed: \\n ' , key_layer . transpose ( - 1 , - 2 ) . shape ) # Take the dot product between \"query\" and \"key\" to get the raw attention scores. attention_scores = torch . matmul ( query_layer , key_layer . transpose ( - 1 , - 2 )) # ADDED print ( ' \\n Attention Scores: \\n ' , attention_scores . shape ) if self . position_embedding_type == \"relative_key\" or self . position_embedding_type == \"relative_key_query\" : seq_length = hidden_states . size ()[ 1 ] position_ids_l = torch . arange ( seq_length , dtype = torch . long , device = hidden_states . device ) . view ( - 1 , 1 ) position_ids_r = torch . arange ( seq_length , dtype = torch . long , device = hidden_states . device ) . view ( 1 , - 1 ) distance = position_ids_l - position_ids_r positional_embedding = self . distance_embedding ( distance + self . max_position_embeddings - 1 ) positional_embedding = positional_embedding . to ( dtype = query_layer . dtype ) # fp16 compatibility if self . position_embedding_type == \"relative_key\" : relative_position_scores = torch . einsum ( \"bhld,lrd->bhlr\" , query_layer , positional_embedding ) attention_scores = attention_scores + relative_position_scores elif self . position_embedding_type == \"relative_key_query\" : relative_position_scores_query = torch . einsum ( \"bhld,lrd->bhlr\" , query_layer , positional_embedding ) relative_position_scores_key = torch . einsum ( \"bhrd,lrd->bhlr\" , key_layer , positional_embedding ) attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key attention_scores = attention_scores / math . sqrt ( self . attention_head_size ) # ADDED print ( ' \\n Attention Scores Divided by Scalar: \\n ' , attention_scores . shape ) if attention_mask is not None : # Apply the attention mask is (precomputed for all layers in BertModel forward() function) attention_scores = attention_scores + attention_mask # Normalize the attention scores to probabilities. attention_probs = torch . nn . Softmax ( dim =- 1 )( attention_scores ) # ADDED print ( ' \\n Attention Probabilities Softmax Layer: \\n ' , attention_probs . shape ) # This is actually dropping out entire tokens to attend to, which might # seem a bit unusual, but is taken from the original Transformer paper. attention_probs = self . dropout ( attention_probs ) # ADDED print ( ' \\n Attention Probabilities Dropout Layer: \\n ' , attention_probs . shape ) # Mask heads if we want to if head_mask is not None : attention_probs = attention_probs * head_mask context_layer = torch . matmul ( attention_probs , value_layer ) # ADDED print ( ' \\n Context: \\n ' , context_layer . shape ) context_layer = context_layer . permute ( 0 , 2 , 1 , 3 ) . contiguous () # ADDED print ( ' \\n Context Permute: \\n ' , context_layer . shape ) new_context_layer_shape = context_layer . size ()[: - 2 ] + ( self . all_head_size ,) context_layer = context_layer . view ( * new_context_layer_shape ) # ADDED print ( ' \\n Context Reshaped: \\n ' , context_layer . shape ) outputs = ( context_layer , attention_probs ) if output_attentions else ( context_layer ,) if self . is_decoder : outputs = outputs + ( past_key_value ,) return outputs # Create bert self attention layer. bert_selfattention_block = BertSelfAttention ( bert_configuraiton ) # Perform a forward pass. context_embedding = bert_selfattention_block . forward ( hidden_states = embedding_output ) Attention Head Size: 64 Combined Attentions Head Size: 768 Hidden States: torch.Size([2, 9, 768]) Query Linear Layer: torch.Size([2, 9, 768]) Key Linear Layer: torch.Size([2, 9, 768]) Value Linear Layer: torch.Size([2, 9, 768]) Query: torch.Size([2, 12, 9, 64]) Key: torch.Size([2, 12, 9, 64]) Value: torch.Size([2, 12, 9, 64]) Key Transposed: torch.Size([2, 12, 64, 9]) Attention Scores: torch.Size([2, 12, 9, 9]) Attention Scores Divided by Scalar: torch.Size([2, 12, 9, 9]) Attention Probabilities Softmax Layer: torch.Size([2, 12, 9, 9]) Attention Probabilities Dropout Layer: torch.Size([2, 12, 9, 9]) Context: torch.Size([2, 12, 9, 64]) Context Permute: torch.Size([2, 9, 12, 64]) Context Reshaped: torch.Size([2, 9, 768])","title":"# BertSelfAttention"},{"location":"tutorial_notebooks/bert_inner_workings/#-bertselfoutput","text":"This layer contains the torch.nn basic components of the self-attention implementation. Implementation can be found at transformers/src/transformers/models/bert/modeling_bert.py#L337 . The forward pass uses: torch.nn.Linear layer: self.dense = nn.Linear(config.hidden_size, config.hidden_size) torch.nn.LayerNorm layer for normalization: self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) torch.nn.Dropout layer for dropout: self.dropout = nn.Dropout(config.hidden_dropout_prob) Show BertSelfOutput Diagram class BertSelfOutput ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . dense = torch . nn . Linear ( config . hidden_size , config . hidden_size ) self . LayerNorm = BertLayerNorm ( config . hidden_size , eps = config . layer_norm_eps ) self . dropout = torch . nn . Dropout ( config . hidden_dropout_prob ) def forward ( self , hidden_states , input_tensor ): print ( 'Hidden States: \\n ' , hidden_states . shape ) hidden_states = self . dense ( hidden_states ) print ( ' \\n Hidden States Linear Layer: \\n ' , hidden_states . shape ) hidden_states = self . dropout ( hidden_states ) print ( ' \\n Hidden States Dropout Layer: \\n ' , hidden_states . shape ) hidden_states = self . LayerNorm ( hidden_states + input_tensor ) print ( ' \\n Hidden States Normalization Layer: \\n ' , hidden_states . shape ) return hidden_states # Create Bert self output layer. bert_selfoutput_block = BertSelfOutput ( bert_configuraiton ) # Perform a forward pass - context_embedding[0] because we have tuple. attention_output = bert_selfoutput_block . forward ( hidden_states = context_embedding [ 0 ], input_tensor = embedding_output ) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Normalization Layer: torch.Size([2, 9, 768])","title":"# BertSelfOutput"},{"location":"tutorial_notebooks/bert_inner_workings/#-assemble-bertattention","text":"Put together BertSelfAttention layer and BertSelfOutput layer to create the BertAttention layer . Now perform a forward pass using previous output layer as input. Show BertAttention Diagram class BertAttention ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . self = BertSelfAttention ( config ) self . output = BertSelfOutput ( config ) self . pruned_heads = set () def prune_heads ( self , heads ): if len ( heads ) == 0 : return heads , index = find_pruneable_heads_and_indices ( heads , self . self . num_attention_heads , self . self . attention_head_size , self . pruned_heads ) # Prune linear layers self . self . query = prune_linear_layer ( self . self . query , index ) self . self . key = prune_linear_layer ( self . self . key , index ) self . self . value = prune_linear_layer ( self . self . value , index ) self . output . dense = prune_linear_layer ( self . output . dense , index , dim = 1 ) # Update hyper params and store pruned heads self . self . num_attention_heads = self . self . num_attention_heads - len ( heads ) self . self . all_head_size = self . self . attention_head_size * self . self . num_attention_heads self . pruned_heads = self . pruned_heads . union ( heads ) def forward ( self , hidden_states , attention_mask = None , head_mask = None , encoder_hidden_states = None , encoder_attention_mask = None , past_key_value = None , output_attentions = False , ): self_outputs = self . self ( hidden_states , attention_mask , head_mask , encoder_hidden_states , encoder_attention_mask , past_key_value , output_attentions , ) attention_output = self . output ( self_outputs [ 0 ], hidden_states ) outputs = ( attention_output ,) + self_outputs [ 1 :] # add attentions if we output them return outputs # Create attention assembled layer. bert_attention_block = BertAttention ( bert_configuraiton ) # Perform a forward pass to wholte Bert Attention layer. attention_output = bert_attention_block ( hidden_states = embedding_output ) Attention Head Size: 64 Combined Attentions Head Size: 768 Hidden States: torch.Size([2, 9, 768]) Query Linear Layer: torch.Size([2, 9, 768]) Key Linear Layer: torch.Size([2, 9, 768]) Value Linear Layer: torch.Size([2, 9, 768]) Query: torch.Size([2, 12, 9, 64]) Key: torch.Size([2, 12, 9, 64]) Value: torch.Size([2, 12, 9, 64]) Key Transposed: torch.Size([2, 12, 64, 9]) Attention Scores: torch.Size([2, 12, 9, 9]) Attention Scores Divided by Scalar: torch.Size([2, 12, 9, 9]) Attention Probabilities Softmax Layer: torch.Size([2, 12, 9, 9]) Attention Probabilities Dropout Layer: torch.Size([2, 12, 9, 9]) Context: torch.Size([2, 12, 9, 64]) Context Permute: torch.Size([2, 9, 12, 64]) Context Reshaped: torch.Size([2, 9, 768]) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Normalization Layer: torch.Size([2, 9, 768])","title":"# Assemble BertAttention"},{"location":"tutorial_notebooks/bert_inner_workings/#bertintermediate","text":"This layer contains the torch.nn basic components of the Bert model implementation. Implementation can be found at transformers/src/transformers/models/bert/modeling_bert.py#L400 . The forward pass uses: torch.nn.Linear layer: self.dense = nn.Linear(config.hidden_size, config.intermediate_size) Show BertIntermediate Diagram class BertIntermediate ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . dense = torch . nn . Linear ( config . hidden_size , config . intermediate_size ) if isinstance ( config . hidden_act , str ): self . intermediate_act_fn = ACT2FN [ config . hidden_act ] else : self . intermediate_act_fn = config . hidden_act def forward ( self , hidden_states ): print ( ' \\n Hidden States: \\n ' , hidden_states . shape ) hidden_states = self . dense ( hidden_states ) print ( ' \\n Hidden States Linear Layer: \\n ' , hidden_states . shape ) hidden_states = self . intermediate_act_fn ( hidden_states ) print ( ' \\n Hidden States Gelu Activation Function: \\n ' , hidden_states . shape ) return hidden_states # Create bert intermediate layer. bert_intermediate_block = BertIntermediate ( bert_configuraiton ) # Perform a forward pass - attention_output[0] because we have tuple. intermediate_output = bert_intermediate_block . forward ( hidden_states = attention_output [ 0 ]) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 3072]) Hidden States Gelu Activation Function: torch.Size([2, 9, 3072])","title":"BertIntermediate"},{"location":"tutorial_notebooks/bert_inner_workings/#bertoutput","text":"This layer contains the torch.nn basic components of the Bert model implementation. Implementation can be found at transformers/src/transformers/models/bert/modeling_bert.py#L415 . The forward pass uses: torch.nn.Linear layer: self.dense = nn.Linear(config.intermediate_size, config.hidden_size) torch.nn.LayerNorm layer for normalization: self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) torch.nn.Dropout layer for dropout: self.dropout = nn.Dropout(config.hidden_dropout_prob) Show BertOutput Diagram class BertOutput ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . dense = torch . nn . Linear ( config . intermediate_size , config . hidden_size ) self . LayerNorm = BertLayerNorm ( config . hidden_size , eps = config . layer_norm_eps ) self . dropout = torch . nn . Dropout ( config . hidden_dropout_prob ) def forward ( self , hidden_states , input_tensor ): print ( ' \\n Hidden States: \\n ' , hidden_states . shape ) hidden_states = self . dense ( hidden_states ) print ( ' \\n Hidden States Linear Layer: \\n ' , hidden_states . shape ) hidden_states = self . dropout ( hidden_states ) print ( ' \\n Hidden States Dropout Layer: \\n ' , hidden_states . shape ) hidden_states = self . LayerNorm ( hidden_states + input_tensor ) print ( ' \\n Hidden States Layer Normalization: \\n ' , hidden_states . shape ) return hidden_states # Create bert output layer. bert_output_block = BertOutput ( bert_configuraiton ) # Perform forward pass - attention_output[0] dealing with tuple. layer_output = bert_output_block . forward ( hidden_states = intermediate_output , input_tensor = attention_output [ 0 ]) Hidden States: torch.Size([2, 9, 3072]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Layer Normalization: torch.Size([2, 9, 768])","title":"BertOutput"},{"location":"tutorial_notebooks/bert_inner_workings/#assemble-bertlayer","text":"Put together BertAttention layer, BertIntermediate layer and BertOutput layer to create the BertLayer layer . Now perform a forward pass using previous output layer as input. Show BertLayer Diagram class BertLayer ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . chunk_size_feed_forward = config . chunk_size_feed_forward self . seq_len_dim = 1 self . attention = BertAttention ( config ) self . is_decoder = config . is_decoder self . add_cross_attention = config . add_cross_attention if self . add_cross_attention : assert self . is_decoder , f \" { self } should be used as a decoder model if cross attention is added\" self . crossattention = BertAttention ( config ) self . intermediate = BertIntermediate ( config ) self . output = BertOutput ( config ) def forward ( self , hidden_states , attention_mask = None , head_mask = None , encoder_hidden_states = None , encoder_attention_mask = None , past_key_value = None , output_attentions = False , ): # decoder uni-directional self-attention cached key/values tuple is at positions 1,2 self_attn_past_key_value = past_key_value [: 2 ] if past_key_value is not None else None self_attention_outputs = self . attention ( hidden_states , attention_mask , head_mask , output_attentions = output_attentions , past_key_value = self_attn_past_key_value , ) attention_output = self_attention_outputs [ 0 ] # if decoder, the last output is tuple of self-attn cache if self . is_decoder : outputs = self_attention_outputs [ 1 : - 1 ] present_key_value = self_attention_outputs [ - 1 ] else : outputs = self_attention_outputs [ 1 :] # add self attentions if we output attention weights cross_attn_present_key_value = None if self . is_decoder and encoder_hidden_states is not None : assert hasattr ( self , \"crossattention\" ), f \"If `encoder_hidden_states` are passed, { self } has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\" # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple cross_attn_past_key_value = past_key_value [ - 2 :] if past_key_value is not None else None cross_attention_outputs = self . crossattention ( attention_output , attention_mask , head_mask , encoder_hidden_states , encoder_attention_mask , cross_attn_past_key_value , output_attentions , ) attention_output = cross_attention_outputs [ 0 ] outputs = outputs + cross_attention_outputs [ 1 : - 1 ] # add cross attentions if we output attention weights # add cross-attn cache to positions 3,4 of present_key_value tuple cross_attn_present_key_value = cross_attention_outputs [ - 1 ] present_key_value = present_key_value + cross_attn_present_key_value layer_output = apply_chunking_to_forward ( self . feed_forward_chunk , self . chunk_size_feed_forward , self . seq_len_dim , attention_output ) outputs = ( layer_output ,) + outputs # if decoder, return the attn key/values as the last output if self . is_decoder : outputs = outputs + ( present_key_value ,) return outputs def feed_forward_chunk ( self , attention_output ): intermediate_output = self . intermediate ( attention_output ) layer_output = self . output ( intermediate_output , attention_output ) return layer_output # Assemble block to create Bert Layer. bert_layer_block = BertLayer ( bert_configuraiton ) # Perform feed forward on a whole Bert Layer. layer_output = bert_layer_block . forward ( hidden_states = embedding_output ) Attention Head Size: 64 Combined Attentions Head Size: 768 Hidden States: torch.Size([2, 9, 768]) Query Linear Layer: torch.Size([2, 9, 768]) Key Linear Layer: torch.Size([2, 9, 768]) Value Linear Layer: torch.Size([2, 9, 768]) Query: torch.Size([2, 12, 9, 64]) Key: torch.Size([2, 12, 9, 64]) Value: torch.Size([2, 12, 9, 64]) Key Transposed: torch.Size([2, 12, 64, 9]) Attention Scores: torch.Size([2, 12, 9, 9]) Attention Scores Divided by Scalar: torch.Size([2, 12, 9, 9]) Attention Probabilities Softmax Layer: torch.Size([2, 12, 9, 9]) Attention Probabilities Dropout Layer: torch.Size([2, 12, 9, 9]) Context: torch.Size([2, 12, 9, 64]) Context Permute: torch.Size([2, 9, 12, 64]) Context Reshaped: torch.Size([2, 9, 768]) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Normalization Layer: torch.Size([2, 9, 768]) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 3072]) Hidden States Gelu Activation Function: torch.Size([2, 9, 3072]) Hidden States: torch.Size([2, 9, 3072]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Layer Normalization: torch.Size([2, 9, 768])","title":"Assemble BertLayer"},{"location":"tutorial_notebooks/bert_inner_workings/#assemble-bertencoder","text":"Put together 12 of the BertLayer layers ( in this setup config.num_hidden_layers=12 ) to create the BertEncoder layer. Now perform a forward pass using previous output layer as input. Show BertEncoder Diagram class BertEncoder ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . config = config self . layer = torch . nn . ModuleList ([ BertLayer ( config ) for _ in range ( config . num_hidden_layers )]) def forward ( self , hidden_states , attention_mask = None , head_mask = None , encoder_hidden_states = None , encoder_attention_mask = None , past_key_values = None , use_cache = None , output_attentions = False , output_hidden_states = False , return_dict = True , ): all_hidden_states = () if output_hidden_states else None all_self_attentions = () if output_attentions else None all_cross_attentions = () if output_attentions and self . config . add_cross_attention else None next_decoder_cache = () if use_cache else None for i , layer_module in enumerate ( self . layer ): # ADDED print ( ' \\n ----------------- BERT LAYER %d -----------------' % ( i + 1 )) if output_hidden_states : all_hidden_states = all_hidden_states + ( hidden_states ,) layer_head_mask = head_mask [ i ] if head_mask is not None else None past_key_value = past_key_values [ i ] if past_key_values is not None else None if getattr ( self . config , \"gradient_checkpointing\" , False ): def create_custom_forward ( module ): def custom_forward ( * inputs ): return module ( * inputs , past_key_value , output_attentions ) return custom_forward layer_outputs = torch . utils . checkpoint . checkpoint ( create_custom_forward ( layer_module ), hidden_states , attention_mask , layer_head_mask , encoder_hidden_states , encoder_attention_mask , ) else : layer_outputs = layer_module ( hidden_states , attention_mask , layer_head_mask , encoder_hidden_states , encoder_attention_mask , past_key_value , output_attentions , ) hidden_states = layer_outputs [ 0 ] if use_cache : next_decoder_cache += ( layer_outputs [ - 1 ],) if output_attentions : all_self_attentions = all_self_attentions + ( layer_outputs [ 1 ],) if self . config . add_cross_attention : all_cross_attentions = all_cross_attentions + ( layer_outputs [ 2 ],) if output_hidden_states : all_hidden_states = all_hidden_states + ( hidden_states ,) if not return_dict : return tuple ( v for v in [ hidden_states , next_decoder_cache , all_hidden_states , all_self_attentions , all_cross_attentions , ] if v is not None ) return BaseModelOutputWithPastAndCrossAttentions ( last_hidden_state = hidden_states , past_key_values = next_decoder_cache , hidden_states = all_hidden_states , attentions = all_self_attentions , cross_attentions = all_cross_attentions , ) # create bert encoder block by stacking 12 layers bert_encoder_block = BertEncoder ( bert_configuraiton ) # perform forward pass on entire Bert Encoder encoder_embedding = bert_encoder_block . forward ( hidden_states = embedding_output ) Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 ----------------- BERT LAYER 1 ----------------- Hidden States: torch.Size([2, 9, 768]) Query Linear Layer: torch.Size([2, 9, 768]) Key Linear Layer: torch.Size([2, 9, 768]) Value Linear Layer: torch.Size([2, 9, 768]) Query: torch.Size([2, 12, 9, 64]) Key: torch.Size([2, 12, 9, 64]) Value: torch.Size([2, 12, 9, 64]) Key Transposed: torch.Size([2, 12, 64, 9]) Attention Scores: torch.Size([2, 12, 9, 9]) Attention Scores Divided by Scalar: torch.Size([2, 12, 9, 9]) Attention Probabilities Softmax Layer: torch.Size([2, 12, 9, 9]) Attention Probabilities Dropout Layer: torch.Size([2, 12, 9, 9]) Context: torch.Size([2, 12, 9, 64]) Context Permute: torch.Size([2, 9, 12, 64]) Context Reshaped: torch.Size([2, 9, 768]) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Normalization Layer: torch.Size([2, 9, 768]) Hidden States: torch.Size([2, 9, 768]) Hidden States Linear Layer: torch.Size([2, 9, 3072]) Hidden States Gelu Activation Function: torch.Size([2, 9, 3072]) Hidden States: torch.Size([2, 9, 3072]) Hidden States Linear Layer: torch.Size([2, 9, 768]) Hidden States Dropout Layer: torch.Size([2, 9, 768]) Hidden States Layer Normalization: torch.Size([2, 9, 768]) ----------------- BERT LAYER 2 ----------------- ... ----------------- BERT LAYER 12 ----------------- ...","title":"Assemble BertEncoder"},{"location":"tutorial_notebooks/bert_inner_workings/#bertpooler","text":"This layer contains the core of the bert model where the self-attention happens. The implementation can be found at: transformers/src/transformers/models/bert/modeling_bert.py#L601 . The forward pass uses: torch.nn.Linear layer: self.dense = torch.nn.Linear(config.hidden_size, config.hidden_size) torch.nn.Tanh activation function layer: self.activation = torch.nn.Tanh() Show BertPooler Diagram class BertPooler ( torch . nn . Module ): def __init__ ( self , config ): super () . __init__ () self . dense = torch . nn . Linear ( config . hidden_size , config . hidden_size ) self . activation = torch . nn . Tanh () def forward ( self , hidden_states ): # We \"pool\" the model by simply taking the hidden state corresponding # to the first token. print ( ' \\n Hidden States: \\n ' , hidden_states . shape ) first_token_tensor = hidden_states [:, 0 ] print ( ' \\n First Token [CLS]: \\n ' , first_token_tensor . shape ) pooled_output = self . dense ( first_token_tensor ) print ( ' \\n First Token [CLS] Linear Layer: \\n ' , pooled_output . shape ) pooled_output = self . activation ( pooled_output ) print ( ' \\n First Token [CLS] Tanh Activation Function: \\n ' , pooled_output . shape ) return pooled_output # Create bert pooler block. bert_pooler_block = BertPooler ( bert_configuraiton ) # Perform forward pass - encoder_embedding[0] because it is a tuple. pooled_output = bert_pooler_block ( hidden_states = encoder_embedding [ 0 ]) Hidden States: torch.Size([2, 9, 768]) First Token [CLS]: torch.Size([2, 768]) First Token [CLS] Linear Layer: torch.Size([2, 768]) First Token [CLS] Tanh Activation Function: torch.Size([2, 768])","title":"BertPooler"},{"location":"tutorial_notebooks/bert_inner_workings/#assemble-bertmodel","text":"Put together BertEmbeddings layer, BertEncoder layer and BertPooler layer to create the BertModel layer. Now perform a forward pass using previous output layer as input. Show BertModel Diagram class BertModel ( BertPreTrainedModel ): \"\"\" The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of cross-attention is added between the self-attention layers, following the architecture described in `Attention is all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder` argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an input to the forward pass. \"\"\" def __init__ ( self , config , add_pooling_layer = True ): super () . __init__ ( config ) self . config = config self . embeddings = BertEmbeddings ( config ) self . encoder = BertEncoder ( config ) self . pooler = BertPooler ( config ) if add_pooling_layer else None self . init_weights () def get_input_embeddings ( self ): return self . embeddings . word_embeddings def set_input_embeddings ( self , value ): self . embeddings . word_embeddings = value def _prune_heads ( self , heads_to_prune ): \"\"\" Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base class PreTrainedModel \"\"\" for layer , heads in heads_to_prune . items (): self . encoder . layer [ layer ] . attention . prune_heads ( heads ) def forward ( self , input_ids = None , attention_mask = None , token_type_ids = None , position_ids = None , head_mask = None , inputs_embeds = None , encoder_hidden_states = None , encoder_attention_mask = None , past_key_values = None , use_cache = None , output_attentions = None , output_hidden_states = None , return_dict = None , ): r \"\"\" encoder_hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`): Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if the model is configured as a decoder. encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`): Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``: - 1 for tokens that are **not masked**, - 0 for tokens that are **masked**. past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding. If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids` (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)` instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`. use_cache (:obj:`bool`, `optional`): If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up decoding (see :obj:`past_key_values`). \"\"\" output_attentions = output_attentions if output_attentions is not None else self . config . output_attentions output_hidden_states = ( output_hidden_states if output_hidden_states is not None else self . config . output_hidden_states ) return_dict = return_dict if return_dict is not None else self . config . use_return_dict if self . config . is_decoder : use_cache = use_cache if use_cache is not None else self . config . use_cache else : use_cache = False if input_ids is not None and inputs_embeds is not None : raise ValueError ( \"You cannot specify both input_ids and inputs_embeds at the same time\" ) elif input_ids is not None : input_shape = input_ids . size () batch_size , seq_length = input_shape elif inputs_embeds is not None : input_shape = inputs_embeds . size ()[: - 1 ] batch_size , seq_length = input_shape else : raise ValueError ( \"You have to specify either input_ids or inputs_embeds\" ) device = input_ids . device if input_ids is not None else inputs_embeds . device # past_key_values_length past_key_values_length = past_key_values [ 0 ][ 0 ] . shape [ 2 ] if past_key_values is not None else 0 if attention_mask is None : attention_mask = torch . ones ((( batch_size , seq_length + past_key_values_length )), device = device ) if token_type_ids is None : token_type_ids = torch . zeros ( input_shape , dtype = torch . long , device = device ) # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length] # ourselves in which case we just need to make it broadcastable to all heads. extended_attention_mask : torch . Tensor = self . get_extended_attention_mask ( attention_mask , input_shape , device ) # If a 2D or 3D attention mask is provided for the cross-attention # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length] if self . config . is_decoder and encoder_hidden_states is not None : encoder_batch_size , encoder_sequence_length , _ = encoder_hidden_states . size () encoder_hidden_shape = ( encoder_batch_size , encoder_sequence_length ) if encoder_attention_mask is None : encoder_attention_mask = torch . ones ( encoder_hidden_shape , device = device ) encoder_extended_attention_mask = self . invert_attention_mask ( encoder_attention_mask ) else : encoder_extended_attention_mask = None # Prepare head mask if needed # 1.0 in head_mask indicate we keep the head # attention_probs has shape bsz x n_heads x N x N # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length] head_mask = self . get_head_mask ( head_mask , self . config . num_hidden_layers ) embedding_output = self . embeddings ( input_ids = input_ids , position_ids = position_ids , token_type_ids = token_type_ids , inputs_embeds = inputs_embeds , past_key_values_length = past_key_values_length , ) encoder_outputs = self . encoder ( embedding_output , attention_mask = extended_attention_mask , head_mask = head_mask , encoder_hidden_states = encoder_hidden_states , encoder_attention_mask = encoder_extended_attention_mask , past_key_values = past_key_values , use_cache = use_cache , output_attentions = output_attentions , output_hidden_states = output_hidden_states , return_dict = return_dict , ) sequence_output = encoder_outputs [ 0 ] pooled_output = self . pooler ( sequence_output ) if self . pooler is not None else None if not return_dict : return ( sequence_output , pooled_output ) + encoder_outputs [ 1 :] return BaseModelOutputWithPoolingAndCrossAttentions ( last_hidden_state = sequence_output , pooler_output = pooled_output , past_key_values = encoder_outputs . past_key_values , hidden_states = encoder_outputs . hidden_states , attentions = encoder_outputs . attentions , cross_attentions = encoder_outputs . cross_attentions , ) # Create bert model. bert_model = BertModel ( bert_configuraiton ) # Perform forward pass on entire model. hidden_states = bert_model . forward ( input_ids = input_sequences [ 'input_ids' ], attention_mask = input_sequences [ 'attention_mask' ], token_type_ids = input_sequences [ 'token_type_ids' ]) Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Created Tokens Positions IDs: tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]]) Tokens IDs: torch.Size([2, 9]) Tokens Type IDs: torch.Size([2, 9]) Word Embeddings: torch.Size([2, 9, 768]) Position Embeddings: torch.Size([1, 9, 768]) Token Types Embeddings: torch.Size([2, 9, 768]) Sum Up All Embeddings: torch.Size([2, 9, 768]) Embeddings Layer Nromalization: torch.Size([2, 9, 768]) Embeddings Dropout Layer: torch.Size([2, 9, 768]) ----------------- BERT LAYER 1 ----------------- ... ----------------- BERT LAYER 12 ----------------- \u2026 Hidden States: torch.Size([2, 9, 768]) First Token [CLS]: torch.Size([2, 768]) First Token [CLS] Linear Layer: torch.Size([2, 768]) First Token [CLS] Tanh Activation Function: torch.Size([2, 768])","title":"Assemble BertModel"},{"location":"tutorial_notebooks/bert_inner_workings/#assemble-components","text":"Put together BertModel layer, torch.nn.Dropout layer and torch.nn.Linear layer to create the BertForSequenceClassification model. Now perform a forward pass using previous output layer as input. class BertForSequenceClassification ( BertPreTrainedModel ): def __init__ ( self , config ): super () . __init__ ( config ) self . num_labels = config . num_labels self . bert = BertModel ( config ) self . dropout = torch . nn . Dropout ( config . hidden_dropout_prob ) self . classifier = torch . nn . Linear ( config . hidden_size , config . num_labels ) self . init_weights () def forward ( self , input_ids = None , attention_mask = None , token_type_ids = None , position_ids = None , head_mask = None , inputs_embeds = None , labels = None , output_attentions = None , output_hidden_states = None , return_dict = None , ): r \"\"\" labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`): Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ..., config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss), If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy). \"\"\" return_dict = return_dict if return_dict is not None else self . config . use_return_dict outputs = self . bert ( input_ids , attention_mask = attention_mask , token_type_ids = token_type_ids , position_ids = position_ids , head_mask = head_mask , inputs_embeds = inputs_embeds , output_attentions = output_attentions , output_hidden_states = output_hidden_states , return_dict = return_dict , ) pooled_output = outputs [ 1 ] pooled_output = self . dropout ( pooled_output ) logits = self . classifier ( pooled_output ) loss = None if labels is not None : if self . num_labels == 1 : # We are doing regression loss_fct = MSELoss () loss = loss_fct ( logits . view ( - 1 ), labels . view ( - 1 )) else : loss_fct = torch . nn . CrossEntropyLoss () loss = loss_fct ( logits . view ( - 1 , self . num_labels ), labels . view ( - 1 )) if not return_dict : output = ( logits ,) + outputs [ 2 :] return (( loss ,) + output ) if loss is not None else output return SequenceClassifierOutput ( loss = loss , logits = logits , hidden_states = outputs . hidden_states , attentions = outputs . attentions , ) # create Bert model with classification layer - BertForSequenceClassificatin bert_for_sequence_classification_model = BertForSequenceClassification ( bert_configuraiton ) # perform forward pass on entire model outputs = bert_for_sequence_classification_model ( ** input_sequences ) Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Attention Head Size: 64 Combined Attentions Head Size: 768 Created Tokens Positions IDs: tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8]]) Tokens IDs: torch.Size([2, 9]) Tokens Type IDs: torch.Size([2, 9]) Word Embeddings: torch.Size([2, 9, 768]) Position Embeddings: torch.Size([1, 9, 768]) Token Types Embeddings: torch.Size([2, 9, 768]) Sum Up All Embeddings: torch.Size([2, 9, 768]) Embeddings Layer Nromalization: torch.Size([2, 9, 768]) Embeddings Dropout Layer: torch.Size([2, 9, 768]) ----------------- BERT LAYER 1 ----------------- ... ----------------- BERT LAYER 12 ----------------- ... Hidden States: torch.Size([2, 9, 768]) First Token [CLS]: torch.Size([2, 768]) First Token [CLS] Linear Layer: torch.Size([2, 768]) First Token [CLS] Tanh Activation Function: torch.Size([2, 768])","title":"Assemble Components"},{"location":"tutorial_notebooks/bert_inner_workings/#complete-diagram","text":"If you want a .pdf version of this diagram: bert_inner_workings.pdf . If you want a .png version of this diagram: bert_inner_workings.png .","title":"Complete Diagram"},{"location":"tutorial_notebooks/bert_inner_workings/#final-note","text":"If you made it this far Congrats! \ud83c\udf8a and Thank you! \ud83d\ude4f for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you see something wrong please let me know by opening an issue on my ml_things GitHub repository ! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.","title":"Final Note"},{"location":"tutorial_notebooks/bert_inner_workings/#contact-","text":"\ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"Contact \ud83c\udfa3"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/","text":"Fine-tune Transformers in PyTorch using Hugging Face Transformers Complete tutorial on how to fine-tune 73 transformer models for text classification \u2014 no code changes necessary! Info This notebook is designed to use a pretrained transformers model and fine-tune it on a classification task. The focus of this tutorial will be on the code itself and how to adjust it to your needs. This notebook is using the AutoClasses from transformer by Hugging Face functionality. This functionality can guess a model's configuration, tokenizer and architecture just by passing in the model's name. This allows for code reusability on a large number of transformers models! What should I know for this notebook? I provided enough instructions and comments to be able to follow along with minimum Python coding knowledge. Since I am using PyTorch to fine-tune our transformers models any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too. How to use this notebook? I built this notebook with reusability in mind. The way I load the dataset into the PyTorch Dataset class is pretty standard and can be easily reused for any other dataset. The only modifications needed to use your own dataset will be in reading in the dataset inside the MovieReviewsDataset class which uses PyTorch Dataset . The DataLoader will return a dictionary of batch inputs format so that it can be fed straight to the model using the statement: outputs = model(**batch) . As long as this statement holds, the rest of the code will work! What transformers models work with this notebook? There are rare cases where I use a different model than Bert when dealing with classification from text data. When there is a need to run a different transformer model architecture, which one would work with this code? Since the name of the notebooks is finetune_transformers it should work with more than one type of transformers. I ran this notebook across all the pretrained models found on Hugging Face Transformer. This way you know ahead of time if the model you plan to use works with this code without any modifications. The list of pretrained transformers models that work with this notebook can be found here . There are 73 models that worked and 33 models that failed to work with this notebook. Dataset This notebook will cover fine-tune transformers for binary classification task. I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with. Coding Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations. Downloads Download the Large Movie Review Dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Installs transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. Give it a try! # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 6.7MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 48.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 49.0MB/s Building wheel for transformers (PEP 517) ... done Building wheel for sacremoses (setup.py) ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Building wheel for ml-things (setup.py) ... done Building wheel for ftfy (setup.py) ... done Imports Import all needed libraries for this notebook. Declare parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. epochs - Number of training epochs (authors recommend between 2 and 4). batch_size - Number of batches - depending on the max sequence length and GPU memory. For 512 sequence length a batch of 10 USUALY works without cuda memory issues. For small sequence length can try batch of 32 or higher. max_length - Pad or truncate text sequences to a specific length. I will set it to 60 to speed up training. device - Look for gpu to use. Will use cpu by default if no gpu found. model_name_or_path - Name of transformers model - will use already pretrained model. Path of transformer model - will load your own model from local disk. I always like to start off with bert-base-cased : 12-layer, 768-hidden, 12-heads, 109M parameters. Trained on cased English text. labels_ids - Dictionary of labels and their id - this will be used to convert string labels to numbers. n_labels - How many labels are we using in this dataset. This is used to decide size of classification head. import io import os import torch from tqdm.notebook import tqdm from torch.utils.data import Dataset , DataLoader from ml_things import plot_dict , plot_confusion_matrix , fix_text from sklearn.metrics import classification_report , accuracy_score from transformers import ( AutoConfig , AutoModelForSequenceClassification , AutoTokenizer , AdamW , get_linear_schedule_with_warmup , set_seed , ) # Set seed for reproducibility, set_seed ( 123 ) # Number of training epochs (authors recommend between 2 and 4) epochs = 4 # Number of batches - depending on the max sequence length and GPU memory. # For 512 sequence length batch of 10 works without cuda memory issues. # For small sequence length can try batch of 32 or higher. batch_size = 32 # Pad or truncate text sequences to a specific length # if `None` it will use maximum sequence of word piece tokens allowed by model. max_length = 60 # Look for gpu to use. Will use `cpu` by default if no gpu found. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) # Name of transformers model - will use already pretrained model. # Path of transformer model - will load your own model from local disk. model_name_or_path = 'bert-base-cased' # Dicitonary of labels and their id - this will be used to convert. # String labels to number ids. labels_ids = { 'neg' : 0 , 'pos' : 1 } # How many labels are we using in training. # This is used to decide size of classification head. n_labels = len ( labels_ids ) Helper Functions I like to keep all Classes and functions that will be used in this notebook under this section to help maintain a clean look of the notebook: MovieReviewsDataset(Dataset) If you worked with PyTorch before this is pretty standard. We need this class to read in our dataset, parse it, use tokenizer that transforms text into numbers and get it into a nice format to be fed to the model. Lucky for use, Hugging Face thought of everything and made the tokenizer do all the heavy lifting (split text into tokens, padding, truncating, encode text into numbers) and is very easy to use! In this class I only need to read in the content of each file, use fix_text to fix any Unicode problems and keep track of positive and negative sentiments. I will append all texts and labels in lists that later I will feed to the tokenizer and to the label ids to transform everything into numbers. There are three main parts of this PyTorch Dataset class: init () where we read in the dataset and transform text and labels into numbers. len () where we need to return the number of examples we read in. This is used when calling len(MovieReviewsDataset()) . getitem() always takes as an input an int value that represents which example from our examples to return from our dataset. If a value of 3 is passed, we will return the example form our dataset at position 3. It needs to return an object with the format that can be fed to our model. Luckily our tokenizer does that for us and returns a dictionary of variables ready to be fed to the model in this way: model(**inputs) . class MovieReviewsDataset ( Dataset ): r \"\"\"PyTorch Dataset class for loading data. This is where the data parsing happens and where the text gets encoded using loaded tokenizer. This class is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: path (:obj:`str`): Path to the data partition. use_tokenizer (:obj:`transformers.tokenization_?`): Transformer type tokenizer used to process raw text into numbers. labels_ids (:obj:`dict`): Dictionary to encode any labels names into numbers. Keys map to labels names and Values map to number associated to those labels. max_sequence_len (:obj:`int`, `optional`) Value to indicate the maximum desired sequence to truncate or pad text sequences. If no value is passed it will used maximum sequence size supported by the tokenizer and model. \"\"\" def __init__ ( self , path , use_tokenizer , labels_ids , max_sequence_len = None ): # Check if path exists. if not os . path . isdir ( path ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) # Check max sequence length. max_sequence_len = use_tokenizer . max_len if max_sequence_len is None else max_sequence_len texts = [] labels = [] print ( 'Reading partitions...' ) # Since the labels are defined by folders with data we loop # through each label. for label , label_id , in tqdm ( labels_ids . items ()): sentiment_path = os . path . join ( path , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:10] # Sample for debugging. print ( 'Reading %s files...' % label ) # Go through each file and read its content. for file_name in tqdm ( files_names ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. texts . append ( content ) # Save encode labels. labels . append ( label_id ) # Number of exmaples. self . n_examples = len ( labels ) # Use tokenizer on texts. This can take a while. print ( 'Using tokenizer on all texts. This can take a while...' ) self . inputs = use_tokenizer ( texts , add_special_tokens = True , truncation = True , padding = True , return_tensors = 'pt' , max_length = max_sequence_len ) # Get maximum sequence length. self . sequence_len = self . inputs [ 'input_ids' ] . shape [ - 1 ] print ( 'Texts padded or truncated to %d length!' % self . sequence_len ) # Add labels. self . inputs . update ({ 'labels' : torch . tensor ( labels )}) print ( 'Finished! \\n ' ) return def __len__ ( self ): r \"\"\"When used `len` return the number of examples. \"\"\" return self . n_examples def __getitem__ ( self , item ): r \"\"\"Given an index return an example from the position. Arguments: item (:obj:`int`): Index position to pick an example to return. Returns: :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model. It holddes the statement `model(**Returned Dictionary)`. \"\"\" return { key : self . inputs [ key ][ item ] for key in self . inputs . keys ()} train(dataloader, optimizer_, scheduler_, device_) I created this function to perform a full pass through the DataLoader object (the DataLoader object is created from our Dataset* type object using the **MovieReviewsDataset class). This is basically one epoch train through the entire dataset. The dataloader is created from PyTorch DataLoader which takes the object created from MovieReviewsDataset class and puts each example in batches. This way we can feed our model batches of data! The optimizer_ and scheduler_ are very common in PyTorch. They are required to update the parameters of our model and update our learning rate during training. There is a lot more than that but I won't go into details. This can actually be a huge rabbit hole since A LOT happens behind these functions that we don't need to worry. Thank you PyTorch! In the process we keep track of the actual labels and the predicted labels along with the loss. def train ( dataloader , optimizer_ , scheduler_ , device_ ): r \"\"\" Train pytorch model on a single pass through the data loader. It will use the global variable `model` which is the transformer model loaded on `_device` that we want to train on. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. optimizer_ (:obj:`transformers.optimization.AdamW`): Optimizer used for training. scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`): PyTorch scheduler. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss]. \"\"\" # Use global variable for model. global model # Tracking variables. predictions_labels = [] true_labels = [] # Total loss for this epoch. total_loss = 0 # Put the model into training mode. model . train () # For each batch of training data... for batch in tqdm ( dataloader , total = len ( dataloader )): # Add original labels - use later for evaluation. true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Always clear any previously calculated gradients before performing a # backward pass. model . zero_grad () # Perform a forward pass (evaluate the model on this training batch). # This will return the loss (rather than the model output) because we # have provided the `labels`. # The documentation for this a bert model function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to calculate training accuracy. loss , logits = outputs [: 2 ] # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # Perform a backward pass to calculate the gradients. loss . backward () # Clip the norm of the gradients to 1.0. # This is to help prevent the \"exploding gradients\" problem. torch . nn . utils . clip_grad_norm_ ( model . parameters (), 1.0 ) # Update parameters and take a step using the computed gradient. # The optimizer dictates the \"update rule\"--how the parameters are # modified based on their gradients, the learning rate, etc. optimizer . step () # Update the learning rate. scheduler . step () # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Convert these logits to list of predicted labels values. predictions_labels += logits . argmax ( axis =- 1 ) . flatten () . tolist () # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediction for future evaluations. return true_labels , predictions_labels , avg_epoch_loss validation(dataloader, device_) I implemented this function in a very similar way as train but without the parameters update, backward pass and gradient decent part. We don't need to do all of those VERY computationally intensive tasks because we only care about our model's predictions. I use the DataLoader in a similar way as in train to get out batches to feed to our model. In the process I keep track of the actual labels and the predicted labels along with the loss. def validation ( dataloader , device_ ): r \"\"\"Validation function to evaluate model performance on a separate set of data. This function will return the true and predicted labels so we can use later to evaluate the model's performance. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss] \"\"\" # Use global variable for model. global model # Tracking variables predictions_labels = [] true_labels = [] #total loss for this epoch. total_loss = 0 # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model . eval () # Evaluate data for one epoch for batch in tqdm ( dataloader , total = len ( dataloader )): # add original labels true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Telling the model not to compute or store gradients, saving memory and # speeding up validation with torch . no_grad (): # Forward pass, calculate logit predictions. # This will return the logits rather than the loss because we have # not provided labels. # token_type_ids is the same as the \"segment ids\", which # differentiates sentence 1 and 2 in 2-sentence tasks. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to to calculate training accuracy. loss , logits = outputs [: 2 ] # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # get predicitons to list predict_content = logits . argmax ( axis =- 1 ) . flatten () . tolist () # update list predictions_labels += predict_content # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediciton for future evaluations. return true_labels , predictions_labels , avg_epoch_loss Load Model and Tokenizer Loading the three essential parts of the pretrained transformers: configuration , tokenizer and model . I also need to load the model on the device I'm planning to use (GPU / CPU). Since I use the AutoClass functionality from Hugging Face I only need to worry about the model's name as input and the rest is handled by the transformers library. # Get model configuration. print ( 'Loading configuraiton...' ) model_config = AutoConfig . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , num_labels = n_labels ) # Get model's tokenizer. print ( 'Loading tokenizer...' ) tokenizer = AutoTokenizer . from_pretrained ( pretrained_model_name_or_path = model_name_or_path ) # Get the actual model. print ( 'Loading model...' ) model = AutoModelForSequenceClassification . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , config = model_config ) # Load model to defined device. model . to ( device ) print ( 'Model loaded to ` %s `' % device ) Loading configuraiton... Loading tokenizer... Loading model... Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Model loaded to `cuda` Dataset and DataLoader This is wehere I create the PyTorch Dataset and DataLoader objects that will be used to feed data into our model. This is where I use the MovieReviewsDataset class and create the dataset variables. Since data is partitioned for both train and test I will create a PyTorch Dataset and PyTorch DataLoader object for train and test. ONLY for simplicity I will use the test as validation. In practice NEVER USE THE TEST DATA FOR VALIDATION! print ( 'Dealing with Train...' ) # Create pytorch dataset. train_dataset = MovieReviewsDataset ( path = '/content/aclImdb/train' , use_tokenizer = tokenizer , labels_ids = labels_ids , max_sequence_len = max_length ) print ( 'Created `train_dataset` with %d examples!' % len ( train_dataset )) # Move pytorch dataset into dataloader. train_dataloader = DataLoader ( train_dataset , batch_size = batch_size , shuffle = True ) print ( 'Created `train_dataloader` with %d batches!' % len ( train_dataloader )) print () print ( 'Dealing with ...' ) # Create pytorch dataset. valid_dataset = MovieReviewsDataset ( path = '/content/aclImdb/test' , use_tokenizer = tokenizer , labels_ids = labels_ids , max_sequence_len = max_length ) print ( 'Created `valid_dataset` with %d examples!' % len ( valid_dataset )) # Move pytorch dataset into dataloader. valid_dataloader = DataLoader ( valid_dataset , batch_size = batch_size , shuffle = False ) print ( 'Created `eval_dataloader` with %d batches!' % len ( valid_dataloader )) Dealing with Train... Reading partitions... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|2/2 [00:34<00:00, 17.28s/it] Reading neg files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:34<00:00, 362.01it/s] Reading pos files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:23<00:00, 534.34it/s] Using tokenizer on all texts. This can take a while... Texts padded or truncated to 40 length! Finished! Created `train_dataset` with 25000 examples! Created `train_dataloader` with 25000 batches! Dealing with ... Reading partitions... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|2/2 [01:28<00:00, 44.13s/it] Reading neg files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [01:28<00:00, 141.71it/s] Reading pos files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [01:17<00:00, 161.60it/s] Using tokenizer on all texts. This can take a while... Texts padded or truncated to 40 length! Finished! Created `valid_dataset` with 25000 examples! Created `eval_dataloader` with 25000 batches! Train I create an optimizer and scheduler that will be used by PyTorch in training. I loop through the number of defined epochs and call the train and validation functions. I will output similar info after each epoch as in Keras: train_loss: - val_loss: - train_acc: - valid_acc . After training, I plot the train and validation loss and accuracy curves to check how the training went. # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the 'W' stands for 'Weight Decay fix\" optimizer = AdamW ( model . parameters (), lr = 2e-5 , # args.learning_rate - default is 5e-5, our notebook had 2e-5 eps = 1e-8 # args.adam_epsilon - default is 1e-8. ) # Total number of training steps is number of batches * number of epochs. # `train_dataloader` contains batched data so `len(train_dataloader)` gives # us the number of batches. total_steps = len ( train_dataloader ) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup ( optimizer , num_warmup_steps = 0 , # Default value in run_glue.py num_training_steps = total_steps ) # Store the average loss after each epoch so we can plot them. all_loss = { 'train_loss' :[], 'val_loss' :[]} all_acc = { 'train_acc' :[], 'val_acc' :[]} # Loop through each epoch. print ( 'Epoch' ) for epoch in tqdm ( range ( epochs )): print () print ( 'Training on batches...' ) # Perform one full pass over the training set. train_labels , train_predict , train_loss = train ( train_dataloader , optimizer , scheduler , device ) train_acc = accuracy_score ( train_labels , train_predict ) # Get prediction form model on validation data. print ( 'Validation on batches...' ) valid_labels , valid_predict , val_loss = validation ( valid_dataloader , device ) val_acc = accuracy_score ( valid_labels , valid_predict ) # Print loss and accuracy values to see how training evolves. print ( \" train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f \" % ( train_loss , val_loss , train_acc , val_acc )) print () # Store the loss value for plotting the learning curve. all_loss [ 'train_loss' ] . append ( train_loss ) all_loss [ 'val_loss' ] . append ( val_loss ) all_acc [ 'train_acc' ] . append ( train_acc ) all_acc [ 'val_acc' ] . append ( val_acc ) # Plot loss curves. plot_dict ( all_loss , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) # Plot accuracy curves. plot_dict ( all_acc , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) Epoch 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|4/4 [13:49<00:00, 207.37s/it] Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.86it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [00:46<00:00, 16.80it/s] train_loss: 0.44816 - val_loss: 0.38655 - train_acc: 0.78372 - valid_acc: 0.81892 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.86it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:13<00:00, 5.88it/s] train_loss: 0.29504 - val_loss: 0.43493 - train_acc: 0.87352 - valid_acc: 0.82360 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.87it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [01:43<00:00, 7.58it/s] train_loss: 0.16901 - val_loss: 0.48433 - train_acc: 0.93544 - valid_acc: 0.82624 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.87it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [00:46<00:00, 16.79it/s] train_loss: 0.09816 - val_loss: 0.73001 - train_acc: 0.96936 - valid_acc: 0.82144 It looks like a little over one epoch is enough training for this model and dataset. Evaluate When dealing with classification it's useful to look at precision recall and f1 score. Another good thing to look at when evaluating the model is the confusion matrix. # Get prediction form model on validation data. This is where you should use # your test data. true_labels , predictions_labels , avg_epoch_loss = validation ( valid_dataloader , device ) # Create the evaluation report. evaluation_report = classification_report ( true_labels , predictions_labels , labels = list ( labels_ids . values ()), target_names = list ( labels_ids . keys ())) # Show the evaluation report. print ( evaluation_report ) # Plot confusion matrix. plot_confusion_matrix ( y_true = true_labels , y_pred = predictions_labels , classes = list ( labels_ids . keys ()), normalize = True , magnify = 3 , ); 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [00:46<00:00, 16.77it/s] precision recall f1-score support neg 0.83 0.81 0.82 12500 pos 0.81 0.83 0.82 12500 accuracy 0.82 25000 macro avg 0.82 0.82 0.82 25000 weighted avg 0.82 0.82 0.82 25000 Normalized confusion matrix Results are not great, but for this tutorial we are not interested in performance. Final Note If you made it this far Congrats and Thank you for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you have 1 minute please give me a feedback in the comments. If you see something wrong please let me know by opening an issue on my ml_things GitHub repository! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can. Contact \ud83c\udfa3 \ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"Finetune Transformers"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#fine-tune-transformers-in-pytorch-using-hugging-face-transformers","text":"","title":"Fine-tune Transformers in PyTorch using Hugging Face Transformers"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#complete-tutorial-on-how-to-fine-tune-73-transformer-models-for-text-classification--no-code-changes-necessary","text":"","title":"Complete tutorial on how to fine-tune 73 transformer models for text classification \u2014 no code changes necessary!"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#info","text":"This notebook is designed to use a pretrained transformers model and fine-tune it on a classification task. The focus of this tutorial will be on the code itself and how to adjust it to your needs. This notebook is using the AutoClasses from transformer by Hugging Face functionality. This functionality can guess a model's configuration, tokenizer and architecture just by passing in the model's name. This allows for code reusability on a large number of transformers models!","title":"Info"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#what-should-i-know-for-this-notebook","text":"I provided enough instructions and comments to be able to follow along with minimum Python coding knowledge. Since I am using PyTorch to fine-tune our transformers models any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too.","title":"What should I know for this notebook?"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#how-to-use-this-notebook","text":"I built this notebook with reusability in mind. The way I load the dataset into the PyTorch Dataset class is pretty standard and can be easily reused for any other dataset. The only modifications needed to use your own dataset will be in reading in the dataset inside the MovieReviewsDataset class which uses PyTorch Dataset . The DataLoader will return a dictionary of batch inputs format so that it can be fed straight to the model using the statement: outputs = model(**batch) . As long as this statement holds, the rest of the code will work!","title":"How to use this notebook?"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#what-transformers-models-work-with-this-notebook","text":"There are rare cases where I use a different model than Bert when dealing with classification from text data. When there is a need to run a different transformer model architecture, which one would work with this code? Since the name of the notebooks is finetune_transformers it should work with more than one type of transformers. I ran this notebook across all the pretrained models found on Hugging Face Transformer. This way you know ahead of time if the model you plan to use works with this code without any modifications. The list of pretrained transformers models that work with this notebook can be found here . There are 73 models that worked and 33 models that failed to work with this notebook.","title":"What transformers models work with this notebook?"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#dataset","text":"This notebook will cover fine-tune transformers for binary classification task. I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with.","title":"Dataset"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#coding","text":"Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.","title":"Coding"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#downloads","text":"Download the Large Movie Review Dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz","title":"Downloads"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#installs","text":"transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. Give it a try! # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 6.7MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 48.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 49.0MB/s Building wheel for transformers (PEP 517) ... done Building wheel for sacremoses (setup.py) ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Building wheel for ml-things (setup.py) ... done Building wheel for ftfy (setup.py) ... done","title":"Installs"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#imports","text":"Import all needed libraries for this notebook. Declare parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. epochs - Number of training epochs (authors recommend between 2 and 4). batch_size - Number of batches - depending on the max sequence length and GPU memory. For 512 sequence length a batch of 10 USUALY works without cuda memory issues. For small sequence length can try batch of 32 or higher. max_length - Pad or truncate text sequences to a specific length. I will set it to 60 to speed up training. device - Look for gpu to use. Will use cpu by default if no gpu found. model_name_or_path - Name of transformers model - will use already pretrained model. Path of transformer model - will load your own model from local disk. I always like to start off with bert-base-cased : 12-layer, 768-hidden, 12-heads, 109M parameters. Trained on cased English text. labels_ids - Dictionary of labels and their id - this will be used to convert string labels to numbers. n_labels - How many labels are we using in this dataset. This is used to decide size of classification head. import io import os import torch from tqdm.notebook import tqdm from torch.utils.data import Dataset , DataLoader from ml_things import plot_dict , plot_confusion_matrix , fix_text from sklearn.metrics import classification_report , accuracy_score from transformers import ( AutoConfig , AutoModelForSequenceClassification , AutoTokenizer , AdamW , get_linear_schedule_with_warmup , set_seed , ) # Set seed for reproducibility, set_seed ( 123 ) # Number of training epochs (authors recommend between 2 and 4) epochs = 4 # Number of batches - depending on the max sequence length and GPU memory. # For 512 sequence length batch of 10 works without cuda memory issues. # For small sequence length can try batch of 32 or higher. batch_size = 32 # Pad or truncate text sequences to a specific length # if `None` it will use maximum sequence of word piece tokens allowed by model. max_length = 60 # Look for gpu to use. Will use `cpu` by default if no gpu found. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) # Name of transformers model - will use already pretrained model. # Path of transformer model - will load your own model from local disk. model_name_or_path = 'bert-base-cased' # Dicitonary of labels and their id - this will be used to convert. # String labels to number ids. labels_ids = { 'neg' : 0 , 'pos' : 1 } # How many labels are we using in training. # This is used to decide size of classification head. n_labels = len ( labels_ids )","title":"Imports"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#helper-functions","text":"I like to keep all Classes and functions that will be used in this notebook under this section to help maintain a clean look of the notebook: MovieReviewsDataset(Dataset) If you worked with PyTorch before this is pretty standard. We need this class to read in our dataset, parse it, use tokenizer that transforms text into numbers and get it into a nice format to be fed to the model. Lucky for use, Hugging Face thought of everything and made the tokenizer do all the heavy lifting (split text into tokens, padding, truncating, encode text into numbers) and is very easy to use! In this class I only need to read in the content of each file, use fix_text to fix any Unicode problems and keep track of positive and negative sentiments. I will append all texts and labels in lists that later I will feed to the tokenizer and to the label ids to transform everything into numbers. There are three main parts of this PyTorch Dataset class: init () where we read in the dataset and transform text and labels into numbers. len () where we need to return the number of examples we read in. This is used when calling len(MovieReviewsDataset()) . getitem() always takes as an input an int value that represents which example from our examples to return from our dataset. If a value of 3 is passed, we will return the example form our dataset at position 3. It needs to return an object with the format that can be fed to our model. Luckily our tokenizer does that for us and returns a dictionary of variables ready to be fed to the model in this way: model(**inputs) . class MovieReviewsDataset ( Dataset ): r \"\"\"PyTorch Dataset class for loading data. This is where the data parsing happens and where the text gets encoded using loaded tokenizer. This class is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: path (:obj:`str`): Path to the data partition. use_tokenizer (:obj:`transformers.tokenization_?`): Transformer type tokenizer used to process raw text into numbers. labels_ids (:obj:`dict`): Dictionary to encode any labels names into numbers. Keys map to labels names and Values map to number associated to those labels. max_sequence_len (:obj:`int`, `optional`) Value to indicate the maximum desired sequence to truncate or pad text sequences. If no value is passed it will used maximum sequence size supported by the tokenizer and model. \"\"\" def __init__ ( self , path , use_tokenizer , labels_ids , max_sequence_len = None ): # Check if path exists. if not os . path . isdir ( path ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) # Check max sequence length. max_sequence_len = use_tokenizer . max_len if max_sequence_len is None else max_sequence_len texts = [] labels = [] print ( 'Reading partitions...' ) # Since the labels are defined by folders with data we loop # through each label. for label , label_id , in tqdm ( labels_ids . items ()): sentiment_path = os . path . join ( path , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:10] # Sample for debugging. print ( 'Reading %s files...' % label ) # Go through each file and read its content. for file_name in tqdm ( files_names ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. texts . append ( content ) # Save encode labels. labels . append ( label_id ) # Number of exmaples. self . n_examples = len ( labels ) # Use tokenizer on texts. This can take a while. print ( 'Using tokenizer on all texts. This can take a while...' ) self . inputs = use_tokenizer ( texts , add_special_tokens = True , truncation = True , padding = True , return_tensors = 'pt' , max_length = max_sequence_len ) # Get maximum sequence length. self . sequence_len = self . inputs [ 'input_ids' ] . shape [ - 1 ] print ( 'Texts padded or truncated to %d length!' % self . sequence_len ) # Add labels. self . inputs . update ({ 'labels' : torch . tensor ( labels )}) print ( 'Finished! \\n ' ) return def __len__ ( self ): r \"\"\"When used `len` return the number of examples. \"\"\" return self . n_examples def __getitem__ ( self , item ): r \"\"\"Given an index return an example from the position. Arguments: item (:obj:`int`): Index position to pick an example to return. Returns: :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model. It holddes the statement `model(**Returned Dictionary)`. \"\"\" return { key : self . inputs [ key ][ item ] for key in self . inputs . keys ()} train(dataloader, optimizer_, scheduler_, device_) I created this function to perform a full pass through the DataLoader object (the DataLoader object is created from our Dataset* type object using the **MovieReviewsDataset class). This is basically one epoch train through the entire dataset. The dataloader is created from PyTorch DataLoader which takes the object created from MovieReviewsDataset class and puts each example in batches. This way we can feed our model batches of data! The optimizer_ and scheduler_ are very common in PyTorch. They are required to update the parameters of our model and update our learning rate during training. There is a lot more than that but I won't go into details. This can actually be a huge rabbit hole since A LOT happens behind these functions that we don't need to worry. Thank you PyTorch! In the process we keep track of the actual labels and the predicted labels along with the loss. def train ( dataloader , optimizer_ , scheduler_ , device_ ): r \"\"\" Train pytorch model on a single pass through the data loader. It will use the global variable `model` which is the transformer model loaded on `_device` that we want to train on. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. optimizer_ (:obj:`transformers.optimization.AdamW`): Optimizer used for training. scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`): PyTorch scheduler. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss]. \"\"\" # Use global variable for model. global model # Tracking variables. predictions_labels = [] true_labels = [] # Total loss for this epoch. total_loss = 0 # Put the model into training mode. model . train () # For each batch of training data... for batch in tqdm ( dataloader , total = len ( dataloader )): # Add original labels - use later for evaluation. true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Always clear any previously calculated gradients before performing a # backward pass. model . zero_grad () # Perform a forward pass (evaluate the model on this training batch). # This will return the loss (rather than the model output) because we # have provided the `labels`. # The documentation for this a bert model function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to calculate training accuracy. loss , logits = outputs [: 2 ] # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # Perform a backward pass to calculate the gradients. loss . backward () # Clip the norm of the gradients to 1.0. # This is to help prevent the \"exploding gradients\" problem. torch . nn . utils . clip_grad_norm_ ( model . parameters (), 1.0 ) # Update parameters and take a step using the computed gradient. # The optimizer dictates the \"update rule\"--how the parameters are # modified based on their gradients, the learning rate, etc. optimizer . step () # Update the learning rate. scheduler . step () # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Convert these logits to list of predicted labels values. predictions_labels += logits . argmax ( axis =- 1 ) . flatten () . tolist () # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediction for future evaluations. return true_labels , predictions_labels , avg_epoch_loss validation(dataloader, device_) I implemented this function in a very similar way as train but without the parameters update, backward pass and gradient decent part. We don't need to do all of those VERY computationally intensive tasks because we only care about our model's predictions. I use the DataLoader in a similar way as in train to get out batches to feed to our model. In the process I keep track of the actual labels and the predicted labels along with the loss. def validation ( dataloader , device_ ): r \"\"\"Validation function to evaluate model performance on a separate set of data. This function will return the true and predicted labels so we can use later to evaluate the model's performance. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss] \"\"\" # Use global variable for model. global model # Tracking variables predictions_labels = [] true_labels = [] #total loss for this epoch. total_loss = 0 # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model . eval () # Evaluate data for one epoch for batch in tqdm ( dataloader , total = len ( dataloader )): # add original labels true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Telling the model not to compute or store gradients, saving memory and # speeding up validation with torch . no_grad (): # Forward pass, calculate logit predictions. # This will return the logits rather than the loss because we have # not provided labels. # token_type_ids is the same as the \"segment ids\", which # differentiates sentence 1 and 2 in 2-sentence tasks. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to to calculate training accuracy. loss , logits = outputs [: 2 ] # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # get predicitons to list predict_content = logits . argmax ( axis =- 1 ) . flatten () . tolist () # update list predictions_labels += predict_content # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediciton for future evaluations. return true_labels , predictions_labels , avg_epoch_loss","title":"Helper Functions"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#load-model-and-tokenizer","text":"Loading the three essential parts of the pretrained transformers: configuration , tokenizer and model . I also need to load the model on the device I'm planning to use (GPU / CPU). Since I use the AutoClass functionality from Hugging Face I only need to worry about the model's name as input and the rest is handled by the transformers library. # Get model configuration. print ( 'Loading configuraiton...' ) model_config = AutoConfig . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , num_labels = n_labels ) # Get model's tokenizer. print ( 'Loading tokenizer...' ) tokenizer = AutoTokenizer . from_pretrained ( pretrained_model_name_or_path = model_name_or_path ) # Get the actual model. print ( 'Loading model...' ) model = AutoModelForSequenceClassification . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , config = model_config ) # Load model to defined device. model . to ( device ) print ( 'Model loaded to ` %s `' % device ) Loading configuraiton... Loading tokenizer... Loading model... Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Model loaded to `cuda`","title":"Load Model and Tokenizer"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#dataset-and-dataloader","text":"This is wehere I create the PyTorch Dataset and DataLoader objects that will be used to feed data into our model. This is where I use the MovieReviewsDataset class and create the dataset variables. Since data is partitioned for both train and test I will create a PyTorch Dataset and PyTorch DataLoader object for train and test. ONLY for simplicity I will use the test as validation. In practice NEVER USE THE TEST DATA FOR VALIDATION! print ( 'Dealing with Train...' ) # Create pytorch dataset. train_dataset = MovieReviewsDataset ( path = '/content/aclImdb/train' , use_tokenizer = tokenizer , labels_ids = labels_ids , max_sequence_len = max_length ) print ( 'Created `train_dataset` with %d examples!' % len ( train_dataset )) # Move pytorch dataset into dataloader. train_dataloader = DataLoader ( train_dataset , batch_size = batch_size , shuffle = True ) print ( 'Created `train_dataloader` with %d batches!' % len ( train_dataloader )) print () print ( 'Dealing with ...' ) # Create pytorch dataset. valid_dataset = MovieReviewsDataset ( path = '/content/aclImdb/test' , use_tokenizer = tokenizer , labels_ids = labels_ids , max_sequence_len = max_length ) print ( 'Created `valid_dataset` with %d examples!' % len ( valid_dataset )) # Move pytorch dataset into dataloader. valid_dataloader = DataLoader ( valid_dataset , batch_size = batch_size , shuffle = False ) print ( 'Created `eval_dataloader` with %d batches!' % len ( valid_dataloader )) Dealing with Train... Reading partitions... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|2/2 [00:34<00:00, 17.28s/it] Reading neg files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:34<00:00, 362.01it/s] Reading pos files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:23<00:00, 534.34it/s] Using tokenizer on all texts. This can take a while... Texts padded or truncated to 40 length! Finished! Created `train_dataset` with 25000 examples! Created `train_dataloader` with 25000 batches! Dealing with ... Reading partitions... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|2/2 [01:28<00:00, 44.13s/it] Reading neg files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [01:28<00:00, 141.71it/s] Reading pos files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [01:17<00:00, 161.60it/s] Using tokenizer on all texts. This can take a while... Texts padded or truncated to 40 length! Finished! Created `valid_dataset` with 25000 examples! Created `eval_dataloader` with 25000 batches!","title":"Dataset and DataLoader"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#train","text":"I create an optimizer and scheduler that will be used by PyTorch in training. I loop through the number of defined epochs and call the train and validation functions. I will output similar info after each epoch as in Keras: train_loss: - val_loss: - train_acc: - valid_acc . After training, I plot the train and validation loss and accuracy curves to check how the training went. # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the 'W' stands for 'Weight Decay fix\" optimizer = AdamW ( model . parameters (), lr = 2e-5 , # args.learning_rate - default is 5e-5, our notebook had 2e-5 eps = 1e-8 # args.adam_epsilon - default is 1e-8. ) # Total number of training steps is number of batches * number of epochs. # `train_dataloader` contains batched data so `len(train_dataloader)` gives # us the number of batches. total_steps = len ( train_dataloader ) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup ( optimizer , num_warmup_steps = 0 , # Default value in run_glue.py num_training_steps = total_steps ) # Store the average loss after each epoch so we can plot them. all_loss = { 'train_loss' :[], 'val_loss' :[]} all_acc = { 'train_acc' :[], 'val_acc' :[]} # Loop through each epoch. print ( 'Epoch' ) for epoch in tqdm ( range ( epochs )): print () print ( 'Training on batches...' ) # Perform one full pass over the training set. train_labels , train_predict , train_loss = train ( train_dataloader , optimizer , scheduler , device ) train_acc = accuracy_score ( train_labels , train_predict ) # Get prediction form model on validation data. print ( 'Validation on batches...' ) valid_labels , valid_predict , val_loss = validation ( valid_dataloader , device ) val_acc = accuracy_score ( valid_labels , valid_predict ) # Print loss and accuracy values to see how training evolves. print ( \" train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f \" % ( train_loss , val_loss , train_acc , val_acc )) print () # Store the loss value for plotting the learning curve. all_loss [ 'train_loss' ] . append ( train_loss ) all_loss [ 'val_loss' ] . append ( val_loss ) all_acc [ 'train_acc' ] . append ( train_acc ) all_acc [ 'val_acc' ] . append ( val_acc ) # Plot loss curves. plot_dict ( all_loss , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) # Plot accuracy curves. plot_dict ( all_acc , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) Epoch 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|4/4 [13:49<00:00, 207.37s/it] Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.86it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [00:46<00:00, 16.80it/s] train_loss: 0.44816 - val_loss: 0.38655 - train_acc: 0.78372 - valid_acc: 0.81892 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.86it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:13<00:00, 5.88it/s] train_loss: 0.29504 - val_loss: 0.43493 - train_acc: 0.87352 - valid_acc: 0.82360 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.87it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [01:43<00:00, 7.58it/s] train_loss: 0.16901 - val_loss: 0.48433 - train_acc: 0.93544 - valid_acc: 0.82624 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.87it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [00:46<00:00, 16.79it/s] train_loss: 0.09816 - val_loss: 0.73001 - train_acc: 0.96936 - valid_acc: 0.82144 It looks like a little over one epoch is enough training for this model and dataset.","title":"Train"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#evaluate","text":"When dealing with classification it's useful to look at precision recall and f1 score. Another good thing to look at when evaluating the model is the confusion matrix. # Get prediction form model on validation data. This is where you should use # your test data. true_labels , predictions_labels , avg_epoch_loss = validation ( valid_dataloader , device ) # Create the evaluation report. evaluation_report = classification_report ( true_labels , predictions_labels , labels = list ( labels_ids . values ()), target_names = list ( labels_ids . keys ())) # Show the evaluation report. print ( evaluation_report ) # Plot confusion matrix. plot_confusion_matrix ( y_true = true_labels , y_pred = predictions_labels , classes = list ( labels_ids . keys ()), normalize = True , magnify = 3 , ); 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [00:46<00:00, 16.77it/s] precision recall f1-score support neg 0.83 0.81 0.82 12500 pos 0.81 0.83 0.82 12500 accuracy 0.82 25000 macro avg 0.82 0.82 0.82 25000 weighted avg 0.82 0.82 0.82 25000 Normalized confusion matrix Results are not great, but for this tutorial we are not interested in performance.","title":"Evaluate"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#final-note","text":"If you made it this far Congrats and Thank you for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you have 1 minute please give me a feedback in the comments. If you see something wrong please let me know by opening an issue on my ml_things GitHub repository! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.","title":"Final Note"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#contact-","text":"\ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"Contact \ud83c\udfa3"},{"location":"tutorial_notebooks/gpt2_finetune_classification/","text":"\ud83c\udfb1 GPT2 For Text Classification using Hugging Face \ud83e\udd17 Transformers Complete tutorial on how to use GPT2 for text classification. Disclaimer: The format of this tutorial notebook is very similar to my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format. This notebook is used to fine-tune GPT2 model for text classification using Huggingface transformers library on a custom dataset. Hugging Face is very nice to us to include all the functionality needed for GPT2 to be used in classification tasks. Thank you Hugging Face! I wasn't able to find much information on how to use GPT2 for classification so I decided to make this tutorial using similar structure with other transformers models. Main idea: Since GPT2 is a decoder transformer, the last token of the input sequence is used to make predictions about the next token that should follow the input. This means that the last token of the input sequence contains all the information needed in the prediction. With this in mind we can use that information to make a prediction in a classification task instead of generation task. In other words, instead of using first token embedding to make prediction like we do in Bert, we will use the last token embedding to make prediction with GPT2. Since we only cared about the first token in Bert, we were padding to the right. Now in GPT2 we are using the last token for prediction so we will need to pad on the left. Because of a nice upgrade to HuggingFace Transformers we are able to configure the GPT2 Tokenizer to do just that. What should I know for this notebook? Since I am using PyTorch to fine-tune our transformers models any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too. How to use this notebook? Like with every project, I built this notebook with reusability in mind. All changes will happen in the data processing part where you need to customize the PyTorch Dataset, Data Collator and DataLoader to fit your own data needs. All parameters that can be changed are under the Imports section. Each parameter is nicely commented and structured to be as intuitive as possible. Dataset This notebook will cover pretraining transformers on a custom dataset. I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with. Coding Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant\u200a-\u200ashow the output. I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations. Downloads Download the Large Movie Review Dataset and unzip it locally. # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Installs transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 6.7MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 48.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 49.0MB/s Building wheel for transformers (PEP 517) ... done Building wheel for sacremoses (setup.py) ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Building wheel for ml-things (setup.py) ... done Building wheel for ftfy (setup.py) ... done Imports Import all needed libraries for this notebook. Declare parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. epochs - Number of training epochs (authors recommend between 2 and 4). batch_size - Number of batches - depending on the max sequence length and GPU memory. For 512 sequence length a batch of 10 USUALY works without cuda memory issues. For small sequence length can try batch of 32 or higher. max_length - Pad or truncate text sequences to a specific length. I will set it to 60 to speed up training. device - Look for gpu to use. Will use cpu by default if no gpu found. model_name_or_path - Name of transformers model - will use already pretrained model. Path of transformer model - will load your own model from local disk. In this tutorial I will use gpt2 model. labels_ids - Dictionary of labels and their id - this will be used to convert string labels to numbers. n_labels - How many labels are we using in this dataset. This is used to decide size of classification head. import io import os import torch from tqdm.notebook import tqdm from torch.utils.data import Dataset , DataLoader from ml_things import plot_dict , plot_confusion_matrix , fix_text from sklearn.metrics import classification_report , accuracy_score from transformers import ( set_seed , TrainingArguments , Trainer , GPT2Config , GPT2Tokenizer , AdamW , get_linear_schedule_with_warmup , GPT2ForSequenceClassification ) # Set seed for reproducibility. set_seed ( 123 ) # Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4). epochs = 4 # Number of batches - depending on the max sequence length and GPU memory. # For 512 sequence length batch of 10 works without cuda memory issues. # For small sequence length can try batch of 32 or higher. batch_size = 32 # Pad or truncate text sequences to a specific length # if `None` it will use maximum sequence of word piece tokens allowed by model. max_length = 60 # Look for gpu to use. Will use `cpu` by default if no gpu found. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) # Name of transformers model - will use already pretrained model. # Path of transformer model - will load your own model from local disk. model_name_or_path = 'gpt2' # Dictionary of labels and their id - this will be used to convert. # String labels to number ids. labels_ids = { 'neg' : 0 , 'pos' : 1 } # How many labels are we using in training. # This is used to decide size of classification head. n_labels = len ( labels_ids ) Helper Functions I like to keep all Classes and functions that will be used in this notebook under this section to help maintain a clean look of the notebook: MovieReviewsDataset(Dataset) If you worked with PyTorch before, this is pretty standard. We need this class to read in our dataset, parse it and return texts with their associated labels. In this class I only need to read in the content of each file, use fix_text to fix any Unicode problems and keep track of positive and negative sentiments. I will append all texts and labels in lists. There are three main parts of this PyTorch Dataset class: init() where we read in the dataset and transform text and labels into numbers. len() where we need to return the number of examples we read in. This is used when calling len(MovieReviewsDataset()). getitem() always takes as an input an int value that represents which example from our examples to return from our dataset. If a value of 3 is passed, we will return the example form our dataset at position 3. Gpt2ClassificationCollator I use this class to create the Data Collator. This will be used in the DataLoader to create the bathes of data that get fed to the model. I use the tokenizer and label encoder on each sequence to convert texts and labels to number. Lucky for us, Hugging Face thought of everything and made the tokenizer do all the heavy lifting (split text into tokens, padding, truncating, encode text into numbers) and is very easy to use! There are two main parts of this Data Collator class: init() where we initialize the tokenizer we plan to use, how to encode our labels and if we need to set the sequence length to a different value. call () used as function collator that takes as input a batch of data examples. It needs to return an object with the format that can be fed to our model. Luckily our tokenizer does that for us and returns a dictionary of variables ready to be fed to the model in this way: model(**inputs) . Since we are fine-tuning the model I also included the labels. train(dataloader, optimizer_, scheduler_, device_) I created this function to perform a full pass through the DataLoader object (the DataLoader object is created from our Dataset* type object using the **MovieReviewsDataset class). This is basically one epoch train through the entire dataset. The dataloader is created from PyTorch DataLoader which takes the object created from MovieReviewsDataset class and puts each example in batches. This way we can feed our model batches of data! The optimizer_ and scheduler_ are very common in PyTorch. They are required to update the parameters of our model and update our learning rate during training. There is a lot more than that but I won't go into details. This can actually be a huge rabbit hole since A LOT happens behind these functions that we don't need to worry. Thank you PyTorch! In the process we keep track of the actual labels and the predicted labels along with the loss. validation(dataloader, device_) I implemented this function in a very similar way as train but without the parameters update, backward pass and gradient decent part. We don't need to do all of those VERY computationally intensive tasks because we only care about our model's predictions. I use the DataLoader in a similar way as in train to get out batches to feed to our model. In the process I keep track of the actual labels and the predicted labels along with the loss. class MovieReviewsDataset ( Dataset ): r \"\"\"PyTorch Dataset class for loading data. This is where the data parsing happens. This class is built with reusability in mind: it can be used as is as. Arguments: path (:obj:`str`): Path to the data partition. \"\"\" def __init__ ( self , path , use_tokenizer ): # Check if path exists. if not os . path . isdir ( path ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) self . texts = [] self . labels = [] # Since the labels are defined by folders with data we loop # through each label. for label in [ 'pos' , 'neg' ]: sentiment_path = os . path . join ( path , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:10] # Sample for debugging. # Go through each file and read its content. for file_name in tqdm ( files_names , desc = f ' { label } files' ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. self . texts . append ( content ) # Save encode labels. self . labels . append ( label ) # Number of exmaples. self . n_examples = len ( self . labels ) return def __len__ ( self ): r \"\"\"When used `len` return the number of examples. \"\"\" return self . n_examples def __getitem__ ( self , item ): r \"\"\"Given an index return an example from the position. Arguments: item (:obj:`int`): Index position to pick an example to return. Returns: :obj:`Dict[str, str]`: Dictionary of inputs that contain text and asociated labels. \"\"\" return { 'text' : self . texts [ item ], 'label' : self . labels [ item ]} class Gpt2ClassificationCollator ( object ): r \"\"\" Data Collator used for GPT2 in a classificaiton rask. It uses a given tokenizer and label encoder to convert any text and labels to numbers that can go straight into a GPT2 model. This class is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: use_tokenizer (:obj:`transformers.tokenization_?`): Transformer type tokenizer used to process raw text into numbers. labels_ids (:obj:`dict`): Dictionary to encode any labels names into numbers. Keys map to labels names and Values map to number associated to those labels. max_sequence_len (:obj:`int`, `optional`) Value to indicate the maximum desired sequence to truncate or pad text sequences. If no value is passed it will used maximum sequence size supported by the tokenizer and model. \"\"\" def __init__ ( self , use_tokenizer , labels_encoder , max_sequence_len = None ): # Tokenizer to be used inside the class. self . use_tokenizer = use_tokenizer # Check max sequence length. self . max_sequence_len = use_tokenizer . model_max_length if max_sequence_len is None else max_sequence_len # Label encoder used inside the class. self . labels_encoder = labels_encoder return def __call__ ( self , sequences ): r \"\"\" This function allowes the class objesct to be used as a function call. Sine the PyTorch DataLoader needs a collator function, I can use this class as a function. Arguments: item (:obj:`list`): List of texts and labels. Returns: :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model. It holddes the statement `model(**Returned Dictionary)`. \"\"\" # Get all texts from sequences list. texts = [ sequence [ 'text' ] for sequence in sequences ] # Get all labels from sequences list. labels = [ sequence [ 'label' ] for sequence in sequences ] # Encode all labels using label encoder. labels = [ self . labels_encoder [ label ] for label in labels ] # Call tokenizer on all texts to convert into tensors of numbers with # appropriate padding. inputs = self . use_tokenizer ( text = texts , return_tensors = \"pt\" , padding = True , truncation = True , max_length = self . max_sequence_len ) # Update the inputs with the associated encoded labels as tensor. inputs . update ({ 'labels' : torch . tensor ( labels )}) return inputs def train ( dataloader , optimizer_ , scheduler_ , device_ ): r \"\"\" Train pytorch model on a single pass through the data loader. It will use the global variable `model` which is the transformer model loaded on `_device` that we want to train on. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. optimizer_ (:obj:`transformers.optimization.AdamW`): Optimizer used for training. scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`): PyTorch scheduler. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss]. \"\"\" # Use global variable for model. global model # Tracking variables. predictions_labels = [] true_labels = [] # Total loss for this epoch. total_loss = 0 # Put the model into training mode. model . train () # For each batch of training data... for batch in tqdm ( dataloader , total = len ( dataloader )): # Add original labels - use later for evaluation. true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Always clear any previously calculated gradients before performing a # backward pass. model . zero_grad () # Perform a forward pass (evaluate the model on this training batch). # This will return the loss (rather than the model output) because we # have provided the `labels`. # The documentation for this a bert model function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to calculate training accuracy. loss , logits = outputs [: 2 ] # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # Perform a backward pass to calculate the gradients. loss . backward () # Clip the norm of the gradients to 1.0. # This is to help prevent the \"exploding gradients\" problem. torch . nn . utils . clip_grad_norm_ ( model . parameters (), 1.0 ) # Update parameters and take a step using the computed gradient. # The optimizer dictates the \"update rule\"--how the parameters are # modified based on their gradients, the learning rate, etc. optimizer . step () # Update the learning rate. scheduler . step () # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Convert these logits to list of predicted labels values. predictions_labels += logits . argmax ( axis =- 1 ) . flatten () . tolist () # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediction for future evaluations. return true_labels , predictions_labels , avg_epoch_loss def validation ( dataloader , device_ ): r \"\"\"Validation function to evaluate model performance on a separate set of data. This function will return the true and predicted labels so we can use later to evaluate the model's performance. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss] \"\"\" # Use global variable for model. global model # Tracking variables predictions_labels = [] true_labels = [] #total loss for this epoch. total_loss = 0 # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model . eval () # Evaluate data for one epoch for batch in tqdm ( dataloader , total = len ( dataloader )): # add original labels true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Telling the model not to compute or store gradients, saving memory and # speeding up validation with torch . no_grad (): # Forward pass, calculate logit predictions. # This will return the logits rather than the loss because we have # not provided labels. # token_type_ids is the same as the \"segment ids\", which # differentiates sentence 1 and 2 in 2-sentence tasks. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to to calculate training accuracy. loss , logits = outputs [: 2 ] # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # get predicitons to list predict_content = logits . argmax ( axis =- 1 ) . flatten () . tolist () # update list predictions_labels += predict_content # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediciton for future evaluations. return true_labels , predictions_labels , avg_epoch_loss Load Model and Tokenizer Loading the three essential parts of the pretrained GPT2 transformer: configuration, tokenizer and model. For this example I will use gpt2 from HuggingFace pretrained transformers. You can use any variations of GP2 you want. In creating the model_config I will mention the number of labels I need for my classification task. Since I only predict two sentiments: positive and negative I will only need two labels for num_labels . Creating the tokenizer is pretty standard when using the Transformers library. After creating the tokenizer it is critical for this tutorial to set padding to the left tokenizer.padding_side = \"left\" and initialize the padding token to tokenizer.eos_token which is the GPT2's original end of sequence token. This is the most essential part of this tutorial since GPT2 uses the last token for prediction so we need to pad to the left. HuggingFace already did most of the work for us and added a classification layer to the GPT2 model. In creating the model I used GPT2ForSequenceClassification . Since we have a custom padding token we need to initialize it for the model using model.config.pad_token_id . Finally we will need to move the model to the device we defined earlier. # Get model configuration. print ( 'Loading configuraiton...' ) model_config = GPT2Config . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , num_labels = n_labels ) # Get model's tokenizer. print ( 'Loading tokenizer...' ) tokenizer = GPT2Tokenizer . from_pretrained ( pretrained_model_name_or_path = model_name_or_path ) # default to left padding tokenizer . padding_side = \"left\" # Define PAD Token = EOS Token = 50256 tokenizer . pad_token = tokenizer . eos_token # Get the actual model. print ( 'Loading model...' ) model = GPT2ForSequenceClassification . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , config = model_config ) # resize model embedding to match new tokenizer model . resize_token_embeddings ( len ( tokenizer )) # fix model padding token id model . config . pad_token_id = model . config . eos_token_id # Load model to defined device. model . to ( device ) print ( 'Model loaded to ` %s `' % device ) Loading configuraiton... Loading tokenizer... Loading model... Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Model loaded to `cuda` Dataset and Collator This is where I create the PyTorch Dataset and Data Loader with Data Collator objects that will be used to feed data into our model. This is where I use the MovieReviewsDataset class to create the PyTorch Dataset that will return texts and labels. Since we need to input numbers to our model we need to convert the texts and labels to numbers. This is the purpose of a collator! It takes data outputted by the PyTorch Dataset and passed through the Data Collator function to output the sequence for our model. I'm keeping the tokenizer away from the PyTorch Dataset to make the code cleaner and better structured. You can obviously use the tokenizer inside the PyTorch Dataset and output sequences that can be used straight into the model without using a Data Collator. I strongly recommend to use a validation text file in order to determine how much training is needed in order to avoid overfitting. After you figure out what parameters yield the best results, the validation file can be incorporated in train and run a final train with the whole dataset. The data collator is used to format the PyTorch Dataset outputs to match the inputs needed for GPT2. # Create data collator to encode text and labels into numbers. gpt2_classificaiton_collator = Gpt2ClassificationCollator ( use_tokenizer = tokenizer , labels_encoder = labels_ids , max_sequence_len = max_length ) print ( 'Dealing with Train...' ) # Create pytorch dataset. train_dataset = MovieReviewsDataset ( path = '/content/aclImdb/train' , use_tokenizer = tokenizer ) print ( 'Created `train_dataset` with %d examples!' % len ( train_dataset )) # Move pytorch dataset into dataloader. train_dataloader = DataLoader ( train_dataset , batch_size = batch_size , shuffle = True , collate_fn = gpt2_classificaiton_collator ) print ( 'Created `train_dataloader` with %d batches!' % len ( train_dataloader )) print () print ( 'Dealing with Validation...' ) # Create pytorch dataset. valid_dataset = MovieReviewsDataset ( path = '/content/aclImdb/test' , use_tokenizer = tokenizer ) print ( 'Created `valid_dataset` with %d examples!' % len ( valid_dataset )) # Move pytorch dataset into dataloader. valid_dataloader = DataLoader ( valid_dataset , batch_size = batch_size , shuffle = False , collate_fn = gpt2_classificaiton_collator ) print ( 'Created `eval_dataloader` with %d batches!' % len ( valid_dataloader )) Dealing with Train... pos files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [01:17<00:00, 161.19it/s] neg files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [01:05<00:00, 190.72it/s] Created `train_dataset` with 25000 examples! Created `train_dataloader` with 782 batches! Reading pos files... pos files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:54<00:00, 230.93it/s] neg files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:42<00:00, 291.07it/s] Created `valid_dataset` with 25000 examples! Created `eval_dataloader` with 782 batches! Train I created optimizer and scheduler use by PyTorch in training. I used most common parameters used by transformers models. I looped through the number of defined epochs and call the train and validation functions. I'm trying to output similar info after each epoch as Keras: train_loss: - val_loss: - train_acc: - valid_acc . After training, plot train and validation loss and accuracy curves to check how the training went. Note: The training plots might look a little weird: The validation accuracy starts higher than training accuracy and the validation loss starts lower than the training loss. Normally this will be the opposite. I assume the data split just happen to be easier for the validation part or too hard for training part or both. Since this tutorial is about using GPT2 for classification I will not worry about the results of the model too much. # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the 'W' stands for 'Weight Decay fix\" optimizer = AdamW ( model . parameters (), lr = 2e-5 , # default is 5e-5, our notebook had 2e-5 eps = 1e-8 # default is 1e-8. ) # Total number of training steps is number of batches * number of epochs. # `train_dataloader` contains batched data so `len(train_dataloader)` gives # us the number of batches. total_steps = len ( train_dataloader ) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup ( optimizer , num_warmup_steps = 0 , # Default value in run_glue.py num_training_steps = total_steps ) # Store the average loss after each epoch so we can plot them. all_loss = { 'train_loss' :[], 'val_loss' :[]} all_acc = { 'train_acc' :[], 'val_acc' :[]} # Loop through each epoch. print ( 'Epoch' ) for epoch in tqdm ( range ( epochs )): print () print ( 'Training on batches...' ) # Perform one full pass over the training set. train_labels , train_predict , train_loss = train ( train_dataloader , optimizer , scheduler , device ) train_acc = accuracy_score ( train_labels , train_predict ) # Get prediction form model on validation data. print ( 'Validation on batches...' ) valid_labels , valid_predict , val_loss = validation ( valid_dataloader , device ) val_acc = accuracy_score ( valid_labels , valid_predict ) # Print loss and accuracy values to see how training evolves. print ( \" train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f \" % ( train_loss , val_loss , train_acc , val_acc )) print () # Store the loss value for plotting the learning curve. all_loss [ 'train_loss' ] . append ( train_loss ) all_loss [ 'val_loss' ] . append ( val_loss ) all_acc [ 'train_acc' ] . append ( train_acc ) all_acc [ 'val_acc' ] . append ( val_acc ) # Plot loss curves. plot_dict ( all_loss , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) # Plot accuracy curves. plot_dict ( all_acc , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) Epoch 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|4/4 [15:11<00:00, 227.96s/it] Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:42<00:00, 4.82it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:07<00:00, 6.13it/s] train_loss: 0.54128 - val_loss: 0.38758 - train_acc: 0.75288 - valid_acc: 0.81904 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:36<00:00, 5.00it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [01:41<00:00, 7.68it/s] train_loss: 0.36716 - val_loss: 0.37620 - train_acc: 0.83288 - valid_acc: 0.82912 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:36<00:00, 5.00it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [01:24<00:00, 9.24it/s] train_loss: 0.31409 - val_loss: 0.39384 - train_acc: 0.86304 - valid_acc: 0.83044 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:36<00:00, 4.99it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [01:09<00:00, 11.29it/s] train_loss: 0.27358 - val_loss: 0.39798 - train_acc: 0.88432 - valid_acc: 0.83292 Evaluate When dealing with classification is useful to look at precision recall and F1 score. A good gauge to have when evaluating a model is the confusion matrix. # Get prediction form model on validation data. This is where you should use # your test data. true_labels , predictions_labels , avg_epoch_loss = validation ( valid_dataloader , device ) # Create the evaluation report. evaluation_report = classification_report ( true_labels , predictions_labels , labels = list ( labels_ids . values ()), target_names = list ( labels_ids . keys ())) # Show the evaluation report. print ( evaluation_report ) # Plot confusion matrix. plot_confusion_matrix ( y_true = true_labels , y_pred = predictions_labels , classes = list ( labels_ids . keys ()), normalize = True , magnify = 0.1 , ); Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [01:09<00:00, 11.24it/s] precision recall f1-score support neg 0.84 0.83 0.83 12500 pos 0.83 0.84 0.83 12500 accuracy 0.83 25000 macro avg 0.83 0.83 0.83 25000 weighted avg 0.83 0.83 0.83 25000 Final Note If you made it this far Congrats! \ud83c\udf8a and Thank you! \ud83d\ude4f for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you see something wrong please let me know by opening an issue on my ml_things GitHub repository ! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can. Contact \ud83c\udfa3 \ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcd3 Medium: @gmihaila \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"GPT2 Finetune Classification"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#-gpt2-for-text-classification-using-hugging-face--transformers","text":"","title":"\ud83c\udfb1 GPT2 For Text Classification using Hugging Face \ud83e\udd17 Transformers"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#complete-tutorial-on-how-to-use-gpt2-for-text-classification","text":"Disclaimer: The format of this tutorial notebook is very similar to my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format. This notebook is used to fine-tune GPT2 model for text classification using Huggingface transformers library on a custom dataset. Hugging Face is very nice to us to include all the functionality needed for GPT2 to be used in classification tasks. Thank you Hugging Face! I wasn't able to find much information on how to use GPT2 for classification so I decided to make this tutorial using similar structure with other transformers models. Main idea: Since GPT2 is a decoder transformer, the last token of the input sequence is used to make predictions about the next token that should follow the input. This means that the last token of the input sequence contains all the information needed in the prediction. With this in mind we can use that information to make a prediction in a classification task instead of generation task. In other words, instead of using first token embedding to make prediction like we do in Bert, we will use the last token embedding to make prediction with GPT2. Since we only cared about the first token in Bert, we were padding to the right. Now in GPT2 we are using the last token for prediction so we will need to pad on the left. Because of a nice upgrade to HuggingFace Transformers we are able to configure the GPT2 Tokenizer to do just that.","title":"Complete tutorial on how to use GPT2 for text classification."},{"location":"tutorial_notebooks/gpt2_finetune_classification/#what-should-i-know-for-this-notebook","text":"Since I am using PyTorch to fine-tune our transformers models any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too.","title":"What should I know for this notebook?"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#how-to-use-this-notebook","text":"Like with every project, I built this notebook with reusability in mind. All changes will happen in the data processing part where you need to customize the PyTorch Dataset, Data Collator and DataLoader to fit your own data needs. All parameters that can be changed are under the Imports section. Each parameter is nicely commented and structured to be as intuitive as possible.","title":"How to use this notebook?"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#dataset","text":"This notebook will cover pretraining transformers on a custom dataset. I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with.","title":"Dataset"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#coding","text":"Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant\u200a-\u200ashow the output. I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.","title":"Coding"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#downloads","text":"Download the Large Movie Review Dataset and unzip it locally. # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz","title":"Downloads"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#installs","text":"transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 6.7MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 48.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 49.0MB/s Building wheel for transformers (PEP 517) ... done Building wheel for sacremoses (setup.py) ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Building wheel for ml-things (setup.py) ... done Building wheel for ftfy (setup.py) ... done","title":"Installs"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#imports","text":"Import all needed libraries for this notebook. Declare parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. epochs - Number of training epochs (authors recommend between 2 and 4). batch_size - Number of batches - depending on the max sequence length and GPU memory. For 512 sequence length a batch of 10 USUALY works without cuda memory issues. For small sequence length can try batch of 32 or higher. max_length - Pad or truncate text sequences to a specific length. I will set it to 60 to speed up training. device - Look for gpu to use. Will use cpu by default if no gpu found. model_name_or_path - Name of transformers model - will use already pretrained model. Path of transformer model - will load your own model from local disk. In this tutorial I will use gpt2 model. labels_ids - Dictionary of labels and their id - this will be used to convert string labels to numbers. n_labels - How many labels are we using in this dataset. This is used to decide size of classification head. import io import os import torch from tqdm.notebook import tqdm from torch.utils.data import Dataset , DataLoader from ml_things import plot_dict , plot_confusion_matrix , fix_text from sklearn.metrics import classification_report , accuracy_score from transformers import ( set_seed , TrainingArguments , Trainer , GPT2Config , GPT2Tokenizer , AdamW , get_linear_schedule_with_warmup , GPT2ForSequenceClassification ) # Set seed for reproducibility. set_seed ( 123 ) # Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4). epochs = 4 # Number of batches - depending on the max sequence length and GPU memory. # For 512 sequence length batch of 10 works without cuda memory issues. # For small sequence length can try batch of 32 or higher. batch_size = 32 # Pad or truncate text sequences to a specific length # if `None` it will use maximum sequence of word piece tokens allowed by model. max_length = 60 # Look for gpu to use. Will use `cpu` by default if no gpu found. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) # Name of transformers model - will use already pretrained model. # Path of transformer model - will load your own model from local disk. model_name_or_path = 'gpt2' # Dictionary of labels and their id - this will be used to convert. # String labels to number ids. labels_ids = { 'neg' : 0 , 'pos' : 1 } # How many labels are we using in training. # This is used to decide size of classification head. n_labels = len ( labels_ids )","title":"Imports"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#helper-functions","text":"I like to keep all Classes and functions that will be used in this notebook under this section to help maintain a clean look of the notebook: MovieReviewsDataset(Dataset) If you worked with PyTorch before, this is pretty standard. We need this class to read in our dataset, parse it and return texts with their associated labels. In this class I only need to read in the content of each file, use fix_text to fix any Unicode problems and keep track of positive and negative sentiments. I will append all texts and labels in lists. There are three main parts of this PyTorch Dataset class: init() where we read in the dataset and transform text and labels into numbers. len() where we need to return the number of examples we read in. This is used when calling len(MovieReviewsDataset()). getitem() always takes as an input an int value that represents which example from our examples to return from our dataset. If a value of 3 is passed, we will return the example form our dataset at position 3. Gpt2ClassificationCollator I use this class to create the Data Collator. This will be used in the DataLoader to create the bathes of data that get fed to the model. I use the tokenizer and label encoder on each sequence to convert texts and labels to number. Lucky for us, Hugging Face thought of everything and made the tokenizer do all the heavy lifting (split text into tokens, padding, truncating, encode text into numbers) and is very easy to use! There are two main parts of this Data Collator class: init() where we initialize the tokenizer we plan to use, how to encode our labels and if we need to set the sequence length to a different value. call () used as function collator that takes as input a batch of data examples. It needs to return an object with the format that can be fed to our model. Luckily our tokenizer does that for us and returns a dictionary of variables ready to be fed to the model in this way: model(**inputs) . Since we are fine-tuning the model I also included the labels. train(dataloader, optimizer_, scheduler_, device_) I created this function to perform a full pass through the DataLoader object (the DataLoader object is created from our Dataset* type object using the **MovieReviewsDataset class). This is basically one epoch train through the entire dataset. The dataloader is created from PyTorch DataLoader which takes the object created from MovieReviewsDataset class and puts each example in batches. This way we can feed our model batches of data! The optimizer_ and scheduler_ are very common in PyTorch. They are required to update the parameters of our model and update our learning rate during training. There is a lot more than that but I won't go into details. This can actually be a huge rabbit hole since A LOT happens behind these functions that we don't need to worry. Thank you PyTorch! In the process we keep track of the actual labels and the predicted labels along with the loss. validation(dataloader, device_) I implemented this function in a very similar way as train but without the parameters update, backward pass and gradient decent part. We don't need to do all of those VERY computationally intensive tasks because we only care about our model's predictions. I use the DataLoader in a similar way as in train to get out batches to feed to our model. In the process I keep track of the actual labels and the predicted labels along with the loss. class MovieReviewsDataset ( Dataset ): r \"\"\"PyTorch Dataset class for loading data. This is where the data parsing happens. This class is built with reusability in mind: it can be used as is as. Arguments: path (:obj:`str`): Path to the data partition. \"\"\" def __init__ ( self , path , use_tokenizer ): # Check if path exists. if not os . path . isdir ( path ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) self . texts = [] self . labels = [] # Since the labels are defined by folders with data we loop # through each label. for label in [ 'pos' , 'neg' ]: sentiment_path = os . path . join ( path , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:10] # Sample for debugging. # Go through each file and read its content. for file_name in tqdm ( files_names , desc = f ' { label } files' ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. self . texts . append ( content ) # Save encode labels. self . labels . append ( label ) # Number of exmaples. self . n_examples = len ( self . labels ) return def __len__ ( self ): r \"\"\"When used `len` return the number of examples. \"\"\" return self . n_examples def __getitem__ ( self , item ): r \"\"\"Given an index return an example from the position. Arguments: item (:obj:`int`): Index position to pick an example to return. Returns: :obj:`Dict[str, str]`: Dictionary of inputs that contain text and asociated labels. \"\"\" return { 'text' : self . texts [ item ], 'label' : self . labels [ item ]} class Gpt2ClassificationCollator ( object ): r \"\"\" Data Collator used for GPT2 in a classificaiton rask. It uses a given tokenizer and label encoder to convert any text and labels to numbers that can go straight into a GPT2 model. This class is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: use_tokenizer (:obj:`transformers.tokenization_?`): Transformer type tokenizer used to process raw text into numbers. labels_ids (:obj:`dict`): Dictionary to encode any labels names into numbers. Keys map to labels names and Values map to number associated to those labels. max_sequence_len (:obj:`int`, `optional`) Value to indicate the maximum desired sequence to truncate or pad text sequences. If no value is passed it will used maximum sequence size supported by the tokenizer and model. \"\"\" def __init__ ( self , use_tokenizer , labels_encoder , max_sequence_len = None ): # Tokenizer to be used inside the class. self . use_tokenizer = use_tokenizer # Check max sequence length. self . max_sequence_len = use_tokenizer . model_max_length if max_sequence_len is None else max_sequence_len # Label encoder used inside the class. self . labels_encoder = labels_encoder return def __call__ ( self , sequences ): r \"\"\" This function allowes the class objesct to be used as a function call. Sine the PyTorch DataLoader needs a collator function, I can use this class as a function. Arguments: item (:obj:`list`): List of texts and labels. Returns: :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model. It holddes the statement `model(**Returned Dictionary)`. \"\"\" # Get all texts from sequences list. texts = [ sequence [ 'text' ] for sequence in sequences ] # Get all labels from sequences list. labels = [ sequence [ 'label' ] for sequence in sequences ] # Encode all labels using label encoder. labels = [ self . labels_encoder [ label ] for label in labels ] # Call tokenizer on all texts to convert into tensors of numbers with # appropriate padding. inputs = self . use_tokenizer ( text = texts , return_tensors = \"pt\" , padding = True , truncation = True , max_length = self . max_sequence_len ) # Update the inputs with the associated encoded labels as tensor. inputs . update ({ 'labels' : torch . tensor ( labels )}) return inputs def train ( dataloader , optimizer_ , scheduler_ , device_ ): r \"\"\" Train pytorch model on a single pass through the data loader. It will use the global variable `model` which is the transformer model loaded on `_device` that we want to train on. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. optimizer_ (:obj:`transformers.optimization.AdamW`): Optimizer used for training. scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`): PyTorch scheduler. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss]. \"\"\" # Use global variable for model. global model # Tracking variables. predictions_labels = [] true_labels = [] # Total loss for this epoch. total_loss = 0 # Put the model into training mode. model . train () # For each batch of training data... for batch in tqdm ( dataloader , total = len ( dataloader )): # Add original labels - use later for evaluation. true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Always clear any previously calculated gradients before performing a # backward pass. model . zero_grad () # Perform a forward pass (evaluate the model on this training batch). # This will return the loss (rather than the model output) because we # have provided the `labels`. # The documentation for this a bert model function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to calculate training accuracy. loss , logits = outputs [: 2 ] # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # Perform a backward pass to calculate the gradients. loss . backward () # Clip the norm of the gradients to 1.0. # This is to help prevent the \"exploding gradients\" problem. torch . nn . utils . clip_grad_norm_ ( model . parameters (), 1.0 ) # Update parameters and take a step using the computed gradient. # The optimizer dictates the \"update rule\"--how the parameters are # modified based on their gradients, the learning rate, etc. optimizer . step () # Update the learning rate. scheduler . step () # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Convert these logits to list of predicted labels values. predictions_labels += logits . argmax ( axis =- 1 ) . flatten () . tolist () # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediction for future evaluations. return true_labels , predictions_labels , avg_epoch_loss def validation ( dataloader , device_ ): r \"\"\"Validation function to evaluate model performance on a separate set of data. This function will return the true and predicted labels so we can use later to evaluate the model's performance. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss] \"\"\" # Use global variable for model. global model # Tracking variables predictions_labels = [] true_labels = [] #total loss for this epoch. total_loss = 0 # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model . eval () # Evaluate data for one epoch for batch in tqdm ( dataloader , total = len ( dataloader )): # add original labels true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Telling the model not to compute or store gradients, saving memory and # speeding up validation with torch . no_grad (): # Forward pass, calculate logit predictions. # This will return the logits rather than the loss because we have # not provided labels. # token_type_ids is the same as the \"segment ids\", which # differentiates sentence 1 and 2 in 2-sentence tasks. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to to calculate training accuracy. loss , logits = outputs [: 2 ] # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # get predicitons to list predict_content = logits . argmax ( axis =- 1 ) . flatten () . tolist () # update list predictions_labels += predict_content # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediciton for future evaluations. return true_labels , predictions_labels , avg_epoch_loss","title":"Helper Functions"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#load-model-and-tokenizer","text":"Loading the three essential parts of the pretrained GPT2 transformer: configuration, tokenizer and model. For this example I will use gpt2 from HuggingFace pretrained transformers. You can use any variations of GP2 you want. In creating the model_config I will mention the number of labels I need for my classification task. Since I only predict two sentiments: positive and negative I will only need two labels for num_labels . Creating the tokenizer is pretty standard when using the Transformers library. After creating the tokenizer it is critical for this tutorial to set padding to the left tokenizer.padding_side = \"left\" and initialize the padding token to tokenizer.eos_token which is the GPT2's original end of sequence token. This is the most essential part of this tutorial since GPT2 uses the last token for prediction so we need to pad to the left. HuggingFace already did most of the work for us and added a classification layer to the GPT2 model. In creating the model I used GPT2ForSequenceClassification . Since we have a custom padding token we need to initialize it for the model using model.config.pad_token_id . Finally we will need to move the model to the device we defined earlier. # Get model configuration. print ( 'Loading configuraiton...' ) model_config = GPT2Config . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , num_labels = n_labels ) # Get model's tokenizer. print ( 'Loading tokenizer...' ) tokenizer = GPT2Tokenizer . from_pretrained ( pretrained_model_name_or_path = model_name_or_path ) # default to left padding tokenizer . padding_side = \"left\" # Define PAD Token = EOS Token = 50256 tokenizer . pad_token = tokenizer . eos_token # Get the actual model. print ( 'Loading model...' ) model = GPT2ForSequenceClassification . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , config = model_config ) # resize model embedding to match new tokenizer model . resize_token_embeddings ( len ( tokenizer )) # fix model padding token id model . config . pad_token_id = model . config . eos_token_id # Load model to defined device. model . to ( device ) print ( 'Model loaded to ` %s `' % device ) Loading configuraiton... Loading tokenizer... Loading model... Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Model loaded to `cuda`","title":"Load Model and Tokenizer"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#dataset-and-collator","text":"This is where I create the PyTorch Dataset and Data Loader with Data Collator objects that will be used to feed data into our model. This is where I use the MovieReviewsDataset class to create the PyTorch Dataset that will return texts and labels. Since we need to input numbers to our model we need to convert the texts and labels to numbers. This is the purpose of a collator! It takes data outputted by the PyTorch Dataset and passed through the Data Collator function to output the sequence for our model. I'm keeping the tokenizer away from the PyTorch Dataset to make the code cleaner and better structured. You can obviously use the tokenizer inside the PyTorch Dataset and output sequences that can be used straight into the model without using a Data Collator. I strongly recommend to use a validation text file in order to determine how much training is needed in order to avoid overfitting. After you figure out what parameters yield the best results, the validation file can be incorporated in train and run a final train with the whole dataset. The data collator is used to format the PyTorch Dataset outputs to match the inputs needed for GPT2. # Create data collator to encode text and labels into numbers. gpt2_classificaiton_collator = Gpt2ClassificationCollator ( use_tokenizer = tokenizer , labels_encoder = labels_ids , max_sequence_len = max_length ) print ( 'Dealing with Train...' ) # Create pytorch dataset. train_dataset = MovieReviewsDataset ( path = '/content/aclImdb/train' , use_tokenizer = tokenizer ) print ( 'Created `train_dataset` with %d examples!' % len ( train_dataset )) # Move pytorch dataset into dataloader. train_dataloader = DataLoader ( train_dataset , batch_size = batch_size , shuffle = True , collate_fn = gpt2_classificaiton_collator ) print ( 'Created `train_dataloader` with %d batches!' % len ( train_dataloader )) print () print ( 'Dealing with Validation...' ) # Create pytorch dataset. valid_dataset = MovieReviewsDataset ( path = '/content/aclImdb/test' , use_tokenizer = tokenizer ) print ( 'Created `valid_dataset` with %d examples!' % len ( valid_dataset )) # Move pytorch dataset into dataloader. valid_dataloader = DataLoader ( valid_dataset , batch_size = batch_size , shuffle = False , collate_fn = gpt2_classificaiton_collator ) print ( 'Created `eval_dataloader` with %d batches!' % len ( valid_dataloader )) Dealing with Train... pos files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [01:17<00:00, 161.19it/s] neg files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [01:05<00:00, 190.72it/s] Created `train_dataset` with 25000 examples! Created `train_dataloader` with 782 batches! Reading pos files... pos files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:54<00:00, 230.93it/s] neg files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:42<00:00, 291.07it/s] Created `valid_dataset` with 25000 examples! Created `eval_dataloader` with 782 batches!","title":"Dataset and Collator"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#train","text":"I created optimizer and scheduler use by PyTorch in training. I used most common parameters used by transformers models. I looped through the number of defined epochs and call the train and validation functions. I'm trying to output similar info after each epoch as Keras: train_loss: - val_loss: - train_acc: - valid_acc . After training, plot train and validation loss and accuracy curves to check how the training went. Note: The training plots might look a little weird: The validation accuracy starts higher than training accuracy and the validation loss starts lower than the training loss. Normally this will be the opposite. I assume the data split just happen to be easier for the validation part or too hard for training part or both. Since this tutorial is about using GPT2 for classification I will not worry about the results of the model too much. # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the 'W' stands for 'Weight Decay fix\" optimizer = AdamW ( model . parameters (), lr = 2e-5 , # default is 5e-5, our notebook had 2e-5 eps = 1e-8 # default is 1e-8. ) # Total number of training steps is number of batches * number of epochs. # `train_dataloader` contains batched data so `len(train_dataloader)` gives # us the number of batches. total_steps = len ( train_dataloader ) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup ( optimizer , num_warmup_steps = 0 , # Default value in run_glue.py num_training_steps = total_steps ) # Store the average loss after each epoch so we can plot them. all_loss = { 'train_loss' :[], 'val_loss' :[]} all_acc = { 'train_acc' :[], 'val_acc' :[]} # Loop through each epoch. print ( 'Epoch' ) for epoch in tqdm ( range ( epochs )): print () print ( 'Training on batches...' ) # Perform one full pass over the training set. train_labels , train_predict , train_loss = train ( train_dataloader , optimizer , scheduler , device ) train_acc = accuracy_score ( train_labels , train_predict ) # Get prediction form model on validation data. print ( 'Validation on batches...' ) valid_labels , valid_predict , val_loss = validation ( valid_dataloader , device ) val_acc = accuracy_score ( valid_labels , valid_predict ) # Print loss and accuracy values to see how training evolves. print ( \" train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f \" % ( train_loss , val_loss , train_acc , val_acc )) print () # Store the loss value for plotting the learning curve. all_loss [ 'train_loss' ] . append ( train_loss ) all_loss [ 'val_loss' ] . append ( val_loss ) all_acc [ 'train_acc' ] . append ( train_acc ) all_acc [ 'val_acc' ] . append ( val_acc ) # Plot loss curves. plot_dict ( all_loss , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) # Plot accuracy curves. plot_dict ( all_acc , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) Epoch 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|4/4 [15:11<00:00, 227.96s/it] Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:42<00:00, 4.82it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:07<00:00, 6.13it/s] train_loss: 0.54128 - val_loss: 0.38758 - train_acc: 0.75288 - valid_acc: 0.81904 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:36<00:00, 5.00it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [01:41<00:00, 7.68it/s] train_loss: 0.36716 - val_loss: 0.37620 - train_acc: 0.83288 - valid_acc: 0.82912 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:36<00:00, 5.00it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [01:24<00:00, 9.24it/s] train_loss: 0.31409 - val_loss: 0.39384 - train_acc: 0.86304 - valid_acc: 0.83044 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:36<00:00, 4.99it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [01:09<00:00, 11.29it/s] train_loss: 0.27358 - val_loss: 0.39798 - train_acc: 0.88432 - valid_acc: 0.83292","title":"Train"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#evaluate","text":"When dealing with classification is useful to look at precision recall and F1 score. A good gauge to have when evaluating a model is the confusion matrix. # Get prediction form model on validation data. This is where you should use # your test data. true_labels , predictions_labels , avg_epoch_loss = validation ( valid_dataloader , device ) # Create the evaluation report. evaluation_report = classification_report ( true_labels , predictions_labels , labels = list ( labels_ids . values ()), target_names = list ( labels_ids . keys ())) # Show the evaluation report. print ( evaluation_report ) # Plot confusion matrix. plot_confusion_matrix ( y_true = true_labels , y_pred = predictions_labels , classes = list ( labels_ids . keys ()), normalize = True , magnify = 0.1 , ); Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [01:09<00:00, 11.24it/s] precision recall f1-score support neg 0.84 0.83 0.83 12500 pos 0.83 0.84 0.83 12500 accuracy 0.83 25000 macro avg 0.83 0.83 0.83 25000 weighted avg 0.83 0.83 0.83 25000","title":"Evaluate"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#final-note","text":"If you made it this far Congrats! \ud83c\udf8a and Thank you! \ud83d\ude4f for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you see something wrong please let me know by opening an issue on my ml_things GitHub repository ! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.","title":"Final Note"},{"location":"tutorial_notebooks/gpt2_finetune_classification/#contact-","text":"\ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcd3 Medium: @gmihaila \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"Contact \ud83c\udfa3"},{"location":"tutorial_notebooks/pretrain_transformer/","text":"Pretrain Transformers Info This notebook is used to pretrain transformers models using Huggingface . This notebooks is part of my trusty notebooks for Machine Learning. Check out more similar content on my website gmihaila.github.io/useful/useful/ where I post useful notebooks like this one. This notebook is heavily inspired from the Huggingface script used for training language models: transformers/tree/master/examples/language-modeling . 'Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.' How to use this notebook? This notebooks is a code adaptation of the run_language_modeling.py . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . Parse the arguments needed that are split in TrainingArguments, ModelArguments and DataTrainingArguments. The only variables that need configuration depending on your needs are model_args , data_args and training_args in Parameters : model_args of type ModelArguments : These are the arguments for the model that you want to use such as the model_name_or_path, tokenizer_name etc. You'll need these to load the model and tokenizer. Minimum setup: model_args = ModelArguments ( model_name_or_path , model_type , tokenizer_name , ) model_name_or_path path to existing transformers model or name of transformer model to be used: bert-base-cased , roberta-base , gpt2 etc. More details here . model_type type of model used: bert , roberta , gpt2 . More details here . tokenizer_name tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased , roberta-base , gpt2 etc. data_args of type DataTrainingArguments : These are as the name suggests arguments needed for the dataset. Such as the directory name where your files are stored etc. You'll need these to load/process the dataset. Minimum setup: data_args = DataArgs ( train_data_file , eval_data_file , mlm , ) train_data_file path to your dataset. This is a plain file that contains all your text data to train a model. Use each line to separate examples: i.e. if you have a dataset composed of multiple text documents, create a single file with each line in the file associated to a text document. eval_data_file same story as train_data_file . This file is used to evaluate the model performance mlm is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta . training_args of type TrainingArguments : These are the training hyper-parameters such as learning rate, batch size, weight decay, gradient accumulation steps etc. See all possible arguments here . These are used by the Trainer. Minimum setup: model_args training_args = TrainingArguments ( output_dir , do_train , do_eval , ) output_dir path where to save the pre-trained model. do_train variable to signal if you're using train data or not. Set it to True if you mentioned train_data_file . do_eval variable to signal if you're using evaluate data or not. Set it to True if you mentioned eval_data_file . Example: Pre-train Bert In the Parameters section use arguments: # process model arguments. Check Info - Notes for more details model_args = ModelArguments ( model_name_or_path = 'bert-base-cased' , model_type = 'bert' , tokenizer_name = 'bert-base-cased' , ) # process data arguments. Check Info - Notes for more details data_args = DataArgs ( train_data_file = '/content/your_train_data' , eval_data_file = '/content/your_test_data, mlm = True , ) # process training arguments. Check Info - Notes for more details training_args = TrainingArguments ( output_dir = '/content/pretrained_bert' , do_train = True , do_eval = False ) Notes: Parameters details got from here . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . I plan on testing more models in the future. I used the The WikiText Long Term Dependency Language Modeling Dataset as an example. To reduce training time I used the evaluate split as training and test split as evaluation! .","title":":dog: Pretrain Transformers"},{"location":"tutorial_notebooks/pretrain_transformer/#pretrain-transformers","text":"","title":"Pretrain Transformers"},{"location":"tutorial_notebooks/pretrain_transformer/#info","text":"This notebook is used to pretrain transformers models using Huggingface . This notebooks is part of my trusty notebooks for Machine Learning. Check out more similar content on my website gmihaila.github.io/useful/useful/ where I post useful notebooks like this one. This notebook is heavily inspired from the Huggingface script used for training language models: transformers/tree/master/examples/language-modeling . 'Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.'","title":"Info"},{"location":"tutorial_notebooks/pretrain_transformer/#how-to-use-this-notebook","text":"This notebooks is a code adaptation of the run_language_modeling.py . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . Parse the arguments needed that are split in TrainingArguments, ModelArguments and DataTrainingArguments. The only variables that need configuration depending on your needs are model_args , data_args and training_args in Parameters : model_args of type ModelArguments : These are the arguments for the model that you want to use such as the model_name_or_path, tokenizer_name etc. You'll need these to load the model and tokenizer. Minimum setup: model_args = ModelArguments ( model_name_or_path , model_type , tokenizer_name , ) model_name_or_path path to existing transformers model or name of transformer model to be used: bert-base-cased , roberta-base , gpt2 etc. More details here . model_type type of model used: bert , roberta , gpt2 . More details here . tokenizer_name tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased , roberta-base , gpt2 etc. data_args of type DataTrainingArguments : These are as the name suggests arguments needed for the dataset. Such as the directory name where your files are stored etc. You'll need these to load/process the dataset. Minimum setup: data_args = DataArgs ( train_data_file , eval_data_file , mlm , ) train_data_file path to your dataset. This is a plain file that contains all your text data to train a model. Use each line to separate examples: i.e. if you have a dataset composed of multiple text documents, create a single file with each line in the file associated to a text document. eval_data_file same story as train_data_file . This file is used to evaluate the model performance mlm is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta . training_args of type TrainingArguments : These are the training hyper-parameters such as learning rate, batch size, weight decay, gradient accumulation steps etc. See all possible arguments here . These are used by the Trainer. Minimum setup: model_args training_args = TrainingArguments ( output_dir , do_train , do_eval , ) output_dir path where to save the pre-trained model. do_train variable to signal if you're using train data or not. Set it to True if you mentioned train_data_file . do_eval variable to signal if you're using evaluate data or not. Set it to True if you mentioned eval_data_file .","title":"How to use this notebook?"},{"location":"tutorial_notebooks/pretrain_transformer/#example","text":"","title":"Example:"},{"location":"tutorial_notebooks/pretrain_transformer/#pre-train-bert","text":"In the Parameters section use arguments: # process model arguments. Check Info - Notes for more details model_args = ModelArguments ( model_name_or_path = 'bert-base-cased' , model_type = 'bert' , tokenizer_name = 'bert-base-cased' , ) # process data arguments. Check Info - Notes for more details data_args = DataArgs ( train_data_file = '/content/your_train_data' , eval_data_file = '/content/your_test_data, mlm = True , ) # process training arguments. Check Info - Notes for more details training_args = TrainingArguments ( output_dir = '/content/pretrained_bert' , do_train = True , do_eval = False )","title":"Pre-train Bert"},{"location":"tutorial_notebooks/pretrain_transformer/#notes","text":"Parameters details got from here . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . I plan on testing more models in the future. I used the The WikiText Long Term Dependency Language Modeling Dataset as an example. To reduce training time I used the evaluate split as training and test split as evaluation! .","title":"Notes:"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/","text":"Pretrain Transformers Models in PyTorch using Hugging Face Transformers Pretrain 67 transformers models on your custom dataset. Disclaimer: The format of this tutorial notebook is very similar with my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format. This notebook is used to pretrain transformers models using Huggingface on your own custom dataset. What do I mean by pretrain transformers ? The definition of pretraining is to train in advance . That is exactly what I mean! Train a transformer model to use it as a pretrained transformers model which can be used to fine-tune it on a specific task! I also use the term fine-tune where I mean to continue training a pretrained model on a custom dataset. I know it is confusing and I hope I'm not making it worse. At the end of the day you are training a transformer model that was previously trained or not! With the AutoClasses functionality we can reuse the code on a large number of transformers models! This notebook is designed to: Use an already pretrained transformers model and fine-tune (continue training) it on your custom dataset. Train a transformer model from scratch on a custom dataset. This requires an already trained (pretrained) tokenizer. This notebook will use by default the pretrained tokenizer if an already trained tokenizer is no provided. This notebook is heavily inspired from the Hugging Face script used for training language models: transformers/tree/master/examples/language-modeling . I basically adapted that script to work nicely in a notebook with a lot more comments. Notes from transformers/tree/master/examples/language-modeling : Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss. What should I know for this notebook? Since I am using PyTorch to fine-tune our transformers models any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too. In this notebook I am using raw text data to pretrain / train / fine-tune transformers models . There is no need for labeled data since we are not doing classification. The Transformers library handles the text files in same way as the original implementation of each model did. How to use this notebook? Like with every project, I built this notebook with reusability in mind. This notebook uses a custom dataset from .txt files. Since the dataset does not come in a single .txt file I created a custom function movie_reviews_to_file that reads the dataset and creates the text file. The way I load the .txt files can be easily reused for any other dataset. The only modifications needed to use your own dataset will be in the paths provided to the train .txt file and evaluation .txt file. All parameters that need to be changed are under the Parameters Setup section. Each parameter is nicely commented and structured to be as intuitive as possible. What transformers models work with this notebook? A lot of people will probably use it for Bert. When there is a need to run a different transformer model architecture, which one would work with this code? Since the name of the notebooks is pretrain_transformers it should work with more than one type of transformers. I ran this notebook across all the pretrained models found on Hugging Face Transformer. This way you know ahead of time if the model you plan to use works with this code without any modifications. The list of pretrained transformers models that work with this notebook can be found here . There are 67 models that worked \ud83d\ude04 and 39 models that failed to work \ud83d\ude22 with this notebook. Remember these are pretrained models and fine-tuned on custom dataset. Dataset This notebook will cover pretraining transformers on a custom dataset. I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with. Coding Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant\u200a-\u200ashow the output. I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations. Downloads Download the Large Movie Review Dataset and unzip it locally. # Download the dataset. ! wget - q - nc http : // ai . stanford . edu /~ amaas / data / sentiment / aclImdb_v1 . tar . gz # Unzip the dataset. ! tar - zxf / content / aclImdb_v1 . tar . gz Installs transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. # Install transformers library. ! pip install - q git + https : // github . com / huggingface / transformers . git # Install helper functions. ! pip install - q git + https : // github . com / gmihaila / ml_things . git Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 6.7MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 48.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 49.0MB/s Building wheel for transformers (PEP 517) ... done Building wheel for sacremoses (setup.py) ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Building wheel for ml-things (setup.py) ... done Building wheel for ftfy (setup.py) ... done Imports Import all needed libraries for this notebook. Declare basic parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. device - Look for gpu to use. I will use cpu by default if no gpu found. import io import os import math import torch import warnings from tqdm.notebook import tqdm from ml_things import plot_dict , fix_text from transformers import ( CONFIG_MAPPING , MODEL_FOR_MASKED_LM_MAPPING , MODEL_FOR_CAUSAL_LM_MAPPING , PreTrainedTokenizer , TrainingArguments , AutoConfig , AutoTokenizer , AutoModelWithLMHead , AutoModelForCausalLM , AutoModelForMaskedLM , LineByLineTextDataset , TextDataset , DataCollatorForLanguageModeling , DataCollatorForWholeWordMask , DataCollatorForPermutationLanguageModeling , PretrainedConfig , Trainer , set_seed , ) # Set seed for reproducibility, set_seed ( 123 ) # Look for gpu to use. Will use `cpu` by default if no gpu found. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) Helper Functions I like to keep all Classes and functions that will be used in this notebook under this section to help maintain a clean look of the notebook: movie_reviews_to_file(path_data: str, path_texts_file: str) As I mentioned before, we will need .txt files to run this notebook. Since the Large Movie Review Dataset comes in multiple files with different labels I created this function to put together all data in a single .txt file. Examples are saved on each line of the file. The path_data points to the path where data files are present and path_texts_file will be the .txt file containing all data. ModelDataArguments This class follows similar format as the [transformers](( huggingface/transformers ) library. The main difference is the way I combined multiple types of arguments into one and used rules to make sure the arguments used are correctly set. Here are all argument detailed (they are also mentioned in the class documentation): train_data_file : Path to your .txt file dataset. If you have an example on each line of the file make sure to use line_by_line=True . If the data file contains all text data without any special grouping use line_by_line=False to move a block_size window across the text file. eval_data_file : Path to evaluation .txt file. It has the same format as train_data_file . line_by_line : If the train_data_file and eval_data_file contains separate examples on each line set line_by_line=True . If there is no separation between examples and train_data_file and eval_data_file contains continuous text then line_by_line=False and a window of block_size will be moved across the files to acquire examples. mlm : Is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta and set to False otherwise. There are functions that will raise ValueError if this argument is not set accordingly. whole_word_mask : Used as flag to determine if we decide to use whole word masking or not. Whole word masking means that whole words will be masked during training instead of tokens which can be chunks of words. mlm_probability : Used when training masked language models. Needs to have mlm=True . It represents the probability of masking tokens when training model. plm_probability : Flag to define the ratio of length of a span of masked tokens to surrounding context length for permutation language modeling. Used for XLNet. max_span_length : Flag may also be used to limit the length of a span of masked tokens used for permutation language modeling. Used for XLNet. block_size : It refers to the windows size that is moved across the text file. Set to -1 to use maximum allowed length. overwrite_cache : If there are any cached files, overwrite them. model_type : Type of model used: bert, roberta, gpt2. More details here . model_config_name : Config of model used: bert, roberta, gpt2. More details here . tokenizer_name : Tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased, roberta-base, gpt2 etc. model_name_or_path : Path to existing transformers model or name of transformer model to be used: bert-base-cased, roberta-base, gpt2 etc. More details here . model_cache_dir : Path to cache files. It helps to save time when re-running code. get_model_config(args: ModelDataArguments) Get model configuration. Using the ModelDataArguments to return the model configuration. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. Returns: Model transformers configuration. Raises: ValueError: If mlm=True and model_type is NOT in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"]. We need to use a masked language model in order to set mlm=True . get_tokenizer(args: ModelDataArguments) Get model tokenizer.Using the ModelDataArguments return the model tokenizer and change block_size form args if needed. Here are all argument detailed: args : Model and data configuration arugments needed to perform pretraining. Returns: Model transformers tokenizer. get_model(args: ModelDataArguments, model_config) Get model. Using the ModelDataArguments return the actual model. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. model_config : Model transformers configuration. Returns: PyTorch model. get_dataset(args: ModelDataArguments, tokenizer: PreTrainedTokenizer, evaluate: bool=False) Process dataset file into PyTorch Dataset. Using the ModelDataArguments return the actual model. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. tokenizer : Model transformers tokenizer. evaluate : If set to True the test / validation file is being handled. If set to False the train file is being handled. Returns: PyTorch Dataset that contains file's data. get_collator(args: ModelDataArguments, model_config: PretrainedConfig, tokenizer: PreTrainedTokenizer) Get appropriate collator function. Collator function will be used to collate a PyTorch Dataset object. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. model_config : Model transformers configuration. tokenizer : Model transformers tokenizer. Returns: Transformers specific data collator. def movie_reviews_to_file ( path_data : str , path_texts_file : str ): r \"\"\"Reading in all data from path and saving it into a single `.txt` file. In the pretraining process of our transformers model we require a text file. This function is designed to work for the Movie Reviews Dataset. You wil have to create your own function to move all examples into a text file if you don't already have a text file with all your unlabeled data. Arguments: path_data (:obj:`str`): Path to the Movie Review Dataset partition. We only have `\\train` and `test` partitions. path_texts_file (:obj:`str`): File path of the generated `.txt` file that contains one example / line. \"\"\" # Check if path exists. if not os . path . isdir ( path_data ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) # Check max sequence length. texts = [] print ( 'Reading ` %s ` partition...' % ( os . path . basename ( path_data ))) # Since the labels are defined by folders with data we loop # through each label. for label in [ 'neg' , 'pos' ]: sentiment_path = os . path . join ( path_data , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:30] # SAMPLE FOR DEBUGGING. # Go through each file and read its content. for file_name in tqdm ( files_names , desc = label , unit = 'files' ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. texts . append ( content ) # Move list to single string. all_texts = ' \\n ' . join ( texts ) # Send all texts string to single file. io . open ( file = path_texts_file , mode = 'w' , encoding = 'utf-8' ) . write ( all_texts ) # Print when done. print ( '`.txt` file saved in ` %s ` \\n ' % path_texts_file ) return class ModelDataArguments ( object ): r \"\"\"Define model and data configuration needed to perform pretraining. Eve though all arguments are optional there still needs to be a certain number of arguments that require values attributed. Arguments: train_data_file (:obj:`str`, `optional`): Path to your .txt file dataset. If you have an example on each line of the file make sure to use line_by_line=True. If the data file contains all text data without any special grouping use line_by_line=False to move a block_size window across the text file. This argument is optional and it will have a `None` value attributed inside the function. eval_data_file (:obj:`str`, `optional`): Path to evaluation .txt file. It has the same format as train_data_file. This argument is optional and it will have a `None` value attributed inside the function. line_by_line (:obj:`bool`, `optional`, defaults to :obj:`False`): If the train_data_file and eval_data_file contains separate examples on each line then line_by_line=True. If there is no separation between examples and train_data_file and eval_data_file contains continuous text then line_by_line=False and a window of block_size will be moved across the files to acquire examples. This argument is optional and it has a default value. mlm (:obj:`bool`, `optional`, defaults to :obj:`False`): Is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta and set to False otherwise. There are functions that will raise ValueError if this argument is not set accordingly. This argument is optional and it has a default value. whole_word_mask (:obj:`bool`, `optional`, defaults to :obj:`False`): Used as flag to determine if we decide to use whole word masking or not. Whole word masking means that whole words will be masked during training instead of tokens which can be chunks of words. This argument is optional and it has a default value. mlm_probability(:obj:`float`, `optional`, defaults to :obj:`0.15`): Used when training masked language models. Needs to have mlm set to True. It represents the probability of masking tokens when training model. This argument is optional and it has a default value. plm_probability (:obj:`float`, `optional`, defaults to :obj:`float(1/6)`): Flag to define the ratio of length of a span of masked tokens to surrounding context length for permutation language modeling. Used for XLNet. This argument is optional and it has a default value. max_span_length (:obj:`int`, `optional`, defaults to :obj:`5`): Flag may also be used to limit the length of a span of masked tokens used for permutation language modeling. Used for XLNet. This argument is optional and it has a default value. block_size (:obj:`int`, `optional`, defaults to :obj:`-1`): It refers to the windows size that is moved across the text file. Set to -1 to use maximum allowed length. This argument is optional and it has a default value. overwrite_cache (:obj:`bool`, `optional`, defaults to :obj:`False`): If there are any cached files, overwrite them. This argument is optional and it has a default value. model_type (:obj:`str`, `optional`): Type of model used: bert, roberta, gpt2. More details: https://huggingface.co/transformers/pretrained_models.html This argument is optional and it will have a `None` value attributed inside the function. model_config_name (:obj:`str`, `optional`): Config of model used: bert, roberta, gpt2. More details: https://huggingface.co/transformers/pretrained_models.html This argument is optional and it will have a `None` value attributed inside the function. tokenizer_name: (:obj:`str`, `optional`) Tokenizer used to process data for training the model. It usually has same name as model_name_or_path: bert-base-cased, roberta-base, gpt2 etc. This argument is optional and it will have a `None` value attributed inside the function. model_name_or_path (:obj:`str`, `optional`): Path to existing transformers model or name of transformer model to be used: bert-base-cased, roberta-base, gpt2 etc. More details: https://huggingface.co/transformers/pretrained_models.html This argument is optional and it will have a `None` value attributed inside the function. model_cache_dir (:obj:`str`, `optional`): Path to cache files to save time when re-running code. This argument is optional and it will have a `None` value attributed inside the function. Raises: ValueError: If `CONFIG_MAPPING` is not loaded in global variables. ValueError: If `model_type` is not present in `CONFIG_MAPPING.keys()`. ValueError: If `model_type`, `model_config_name` and `model_name_or_path` variables are all `None`. At least one of them needs to be set. warnings: If `model_config_name` and `model_name_or_path` are both `None`, the model will be trained from scratch. ValueError: If `tokenizer_name` and `model_name_or_path` are both `None`. We need at least one of them set to load tokenizer. \"\"\" def __init__ ( self , train_data_file = None , eval_data_file = None , line_by_line = False , mlm = False , mlm_probability = 0.15 , whole_word_mask = False , plm_probability = float ( 1 / 6 ), max_span_length = 5 , block_size =- 1 , overwrite_cache = False , model_type = None , model_config_name = None , tokenizer_name = None , model_name_or_path = None , model_cache_dir = None ): # Make sure CONFIG_MAPPING is imported from transformers module. if 'CONFIG_MAPPING' not in globals (): raise ValueError ( 'Could not find `CONFIG_MAPPING` imported! Make sure' \\ ' to import it from `transformers` module!' ) # Make sure model_type is valid. if ( model_type is not None ) and ( model_type not in CONFIG_MAPPING . keys ()): raise ValueError ( 'Invalid `model_type`! Use one of the following: %s ' % ( str ( list ( CONFIG_MAPPING . keys ())))) # Make sure that model_type, model_config_name and model_name_or_path # variables are not all `None`. if not any ([ model_type , model_config_name , model_name_or_path ]): raise ValueError ( 'You can`t have all `model_type`, `model_config_name`,' \\ ' `model_name_or_path` be `None`! You need to have' \\ 'at least one of them set!' ) # Check if a new model will be loaded from scratch. if not any ([ model_config_name , model_name_or_path ]): # Setup warning to show pretty. This is an overkill warnings . formatwarning = lambda message , category , * args , ** kwargs : \\ ' %s : %s \\n ' % ( category . __name__ , message ) # Display warning. warnings . warn ( 'You are planning to train a model from scratch! \ud83d\ude40' ) # Check if a new tokenizer wants to be loaded. # This feature is not supported! if not any ([ tokenizer_name , model_name_or_path ]): # Can't train tokenizer from scratch here! Raise error. raise ValueError ( 'You want to train tokenizer from scratch! ' \\ 'That is not possible yet! You can train your own ' \\ 'tokenizer separately and use path here to load it!' ) # Set all data related arguments. self . train_data_file = train_data_file self . eval_data_file = eval_data_file self . line_by_line = line_by_line self . mlm = mlm self . whole_word_mask = whole_word_mask self . mlm_probability = mlm_probability self . plm_probability = plm_probability self . max_span_length = max_span_length self . block_size = block_size self . overwrite_cache = overwrite_cache # Set all model and tokenizer arguments. self . model_type = model_type self . model_config_name = model_config_name self . tokenizer_name = tokenizer_name self . model_name_or_path = model_name_or_path self . model_cache_dir = model_cache_dir return def get_model_config ( args : ModelDataArguments ): r \"\"\" Get model configuration. Using the ModelDataArguments return the model configuration. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. Returns: :obj:`PretrainedConfig`: Model transformers configuration. Raises: ValueError: If `mlm=True` and `model_type` is NOT in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"]. We need to use a masked language model in order to set `mlm=True`. \"\"\" # Check model configuration. if args . model_config_name is not None : # Use model configure name if defined. model_config = AutoConfig . from_pretrained ( args . model_config_name , cache_dir = args . model_cache_dir ) elif args . model_name_or_path is not None : # Use model name or path if defined. model_config = AutoConfig . from_pretrained ( args . model_name_or_path , cache_dir = args . model_cache_dir ) else : # Use config mapping if building model from scratch. model_config = CONFIG_MAPPING [ args . model_type ]() # Make sure `mlm` flag is set for Masked Language Models (MLM). if ( model_config . model_type in [ \"bert\" , \"roberta\" , \"distilbert\" , \"camembert\" ]) and ( args . mlm is False ): raise ValueError ( 'BERT and RoBERTa-like models do not have LM heads ' \\ 'butmasked LM heads. They must be run setting `mlm=True`' ) # Adjust block size for xlnet. if model_config . model_type == \"xlnet\" : # xlnet used 512 tokens when training. args . block_size = 512 # setup memory length model_config . mem_len = 1024 return model_config def get_tokenizer ( args : ModelDataArguments ): r \"\"\" Get model tokenizer. Using the ModelDataArguments return the model tokenizer and change `block_size` form `args` if needed. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. Returns: :obj:`PreTrainedTokenizer`: Model transformers tokenizer. \"\"\" # Check tokenizer configuration. if args . tokenizer_name : # Use tokenizer name if define. tokenizer = AutoTokenizer . from_pretrained ( args . tokenizer_name , cache_dir = args . model_cache_dir ) elif args . model_name_or_path : # Use tokenizer name of path if defined. tokenizer = AutoTokenizer . from_pretrained ( args . model_name_or_path , cache_dir = args . model_cache_dir ) # Setp data block size. if args . block_size <= 0 : # Set block size to maximum length of tokenizer. # Input block size will be the max possible for the model. # Some max lengths are very large and will cause a args . block_size = tokenizer . model_max_length else : # Never go beyond tokenizer maximum length. args . block_size = min ( args . block_size , tokenizer . model_max_length ) return tokenizer def get_model ( args : ModelDataArguments , model_config ): r \"\"\" Get model. Using the ModelDataArguments return the actual model. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. model_config (:obj:`PretrainedConfig`): Model transformers configuration. Returns: :obj:`torch.nn.Module`: PyTorch model. \"\"\" # Make sure MODEL_FOR_MASKED_LM_MAPPING and MODEL_FOR_CAUSAL_LM_MAPPING are # imported from transformers module. if ( 'MODEL_FOR_MASKED_LM_MAPPING' not in globals ()) and \\ ( 'MODEL_FOR_CAUSAL_LM_MAPPING' not in globals ()): raise ValueError ( 'Could not find `MODEL_FOR_MASKED_LM_MAPPING` and' \\ ' `MODEL_FOR_MASKED_LM_MAPPING` imported! Make sure to' \\ ' import them from `transformers` module!' ) # Check if using pre-trained model or train from scratch. if args . model_name_or_path : # Use pre-trained model. if type ( model_config ) in MODEL_FOR_MASKED_LM_MAPPING . keys (): # Masked language modeling head. return AutoModelForMaskedLM . from_pretrained ( args . model_name_or_path , from_tf = bool ( \".ckpt\" in args . model_name_or_path ), config = model_config , cache_dir = args . model_cache_dir , ) elif type ( model_config ) in MODEL_FOR_CAUSAL_LM_MAPPING . keys (): # Causal language modeling head. return AutoModelForCausalLM . from_pretrained ( args . model_name_or_path , from_tf = bool ( \".ckpt\" in args . model_name_or_path ), config = model_config , cache_dir = args . model_cache_dir ) else : raise ValueError ( 'Invalid `model_name_or_path`! It should be in %s or %s !' % ( str ( MODEL_FOR_MASKED_LM_MAPPING . keys ()), str ( MODEL_FOR_CAUSAL_LM_MAPPING . keys ()))) else : # Use model from configuration - train from scratch. print ( \"Training new model from scratch!\" ) return AutoModelWithLMHead . from_config ( config ) def get_dataset ( args : ModelDataArguments , tokenizer : PreTrainedTokenizer , evaluate : bool = False ): r \"\"\" Process dataset file into PyTorch Dataset. Using the ModelDataArguments return the actual model. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. tokenizer (:obj:`PreTrainedTokenizer`): Model transformers tokenizer. evaluate (:obj:`bool`, `optional`, defaults to :obj:`False`): If set to `True` the test / validation file is being handled. If set to `False` the train file is being handled. Returns: :obj:`Dataset`: PyTorch Dataset that contains file's data. \"\"\" # Get file path for either train or evaluate. file_path = args . eval_data_file if evaluate else args . train_data_file # Check if `line_by_line` flag is set to `True`. if args . line_by_line : # Each example in data file is on each line. return LineByLineTextDataset ( tokenizer = tokenizer , file_path = file_path , block_size = args . block_size ) else : # All data in file is put together without any separation. return TextDataset ( tokenizer = tokenizer , file_path = file_path , block_size = args . block_size , overwrite_cache = args . overwrite_cache ) def get_collator ( args : ModelDataArguments , model_config : PretrainedConfig , tokenizer : PreTrainedTokenizer ): r \"\"\" Get appropriate collator function. Collator function will be used to collate a PyTorch Dataset object. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. model_config (:obj:`PretrainedConfig`): Model transformers configuration. tokenizer (:obj:`PreTrainedTokenizer`): Model transformers tokenizer. Returns: :obj:`data_collator`: Transformers specific data collator. \"\"\" # Special dataset handle depending on model type. if model_config . model_type == \"xlnet\" : # Configure collator for XLNET. return DataCollatorForPermutationLanguageModeling ( tokenizer = tokenizer , plm_probability = args . plm_probability , max_span_length = args . max_span_length , ) else : # Configure data for rest of model types. if args . mlm and args . whole_word_mask : # Use whole word masking. return DataCollatorForWholeWordMask ( tokenizer = tokenizer , mlm_probability = args . mlm_probability , ) else : # Regular language modeling. return DataCollatorForLanguageModeling ( tokenizer = tokenizer , mlm = args . mlm , mlm_probability = args . mlm_probability , ) Parameters Setup Declare the rest of the parameters used for this notebook: model_data_args contains all arguments needed to setup dataset, model configuration, model tokenizer and the actual model. This is created using the ModelDataArguments class. training_args contain all arguments needed to use the Trainer functionality from Transformers that allows us to train transformers models in PyTorch very easy. You can find the complete documentation here . There are a lot of parameters that can be set to allow multiple functionalities. I only used the following parameters (the comments are inspired from the HuggingFace documentation of TrainingArguments : output_dir : The output directory where the model predictions and checkpoints will be written. I set it up to pretrained_bert_model where the model and will be saved. overwrite_output_dir : Overwrite the content of the output directory. I set it to True in case I run the notebook multiple times I only care about the last run. do_train : Whether to run training or not. I set this parameter to True because I want to train the model on my custom dataset. do_eval : Whether to run evaluation on the evaluation files or not. I set it to True since I have test data file and I want to evaluate how well the model trains. per_device_train_batch_size : Batch size GPU/TPU core/CPU training. I set it to 2 for this example. I recommend setting it up as high as your GPU memory allows you. per_device_eval_batch_size : Batch size GPU/TPU core/CPU for evaluation.I set this value to 100 since it's not dealing with gradients. evaluation_strategy : Evaluation strategy to adopt during training: no : No evaluation during training; steps : Evaluate every eval_steps; epoch`: Evaluate every end of epoch. I set it to 'steps' since I want to evaluate model more often. logging_steps : How often to show logs. I will se this to plot history loss and calculate perplexity. I set this to 20 just as an example. If your evaluate data is large you might not want to run it that often because it will significantly slow down training time. eval_steps : Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set. Since I want to evaluate model ever logging_steps I will set this to None since it will inherit same value as logging_steps . prediction_loss_only : Set prediction loss to True in order to return loss for perplexity calculation. Since I want to calculate perplexity I set this to True since I want to monitor loss and perplexity (which is exp(loss)). learning_rate : The initial learning rate for Adam. Defaults is set to 5e-5 . weight_decay : The weight decay to apply (if not zero)Defaults is set to 0 . adam_epsilon : Epsilon for the Adam optimizer. Defaults to 1e-8 . max_grad_norm : Maximum gradient norm (for gradient clipping). Defaults to 0 . num_train_epochs : Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training). I set it to 2 at most. Since the custom dataset will be a lot smaller than the original dataset the model was trained on we don't want to overfit. save_steps : Number of updates steps before two checkpoint saves. Defaults to 500 . # Define arguments for data, tokenizer and model arguments. # See comments in `ModelDataArguments` class. model_data_args = ModelDataArguments ( train_data_file = '/content/train.txt' , eval_data_file = '/content/test.txt' , line_by_line = True , mlm = True , whole_word_mask = True , mlm_probability = 0.15 , plm_probability = float ( 1 / 6 ), max_span_length = 5 , block_size = 50 , overwrite_cache = False , model_type = 'bert' , model_config_name = 'bert-base-cased' , tokenizer_name = 'bert-base-cased' , model_name_or_path = 'bert-base-cased' , model_cache_dir = None , ) # Define arguments for training # Note: I only used the arguments I care about. `TrainingArguments` contains # a lot more arguments. For more details check the awesome documentation: # https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments training_args = TrainingArguments ( # The output directory where the model predictions # and checkpoints will be written. output_dir = 'pretrain_bert' , # Overwrite the content of the output directory. overwrite_output_dir = True , # Whether to run training or not. do_train = True , # Whether to run evaluation on the dev or not. do_eval = True , # Batch size GPU/TPU core/CPU training. per_device_train_batch_size = 10 , # Batch size GPU/TPU core/CPU for evaluation. per_device_eval_batch_size = 100 , # evaluation strategy to adopt during training # `no`: No evaluation during training. # `steps`: Evaluate every `eval_steps`. # `epoch`: Evaluate every end of epoch. evaluation_strategy = 'steps' , # How often to show logs. I will se this to # plot history loss and calculate perplexity. logging_steps = 700 , # Number of update steps between two # evaluations if evaluation_strategy=\"steps\". # Will default to the same value as l # logging_steps if not set. eval_steps = None , # Set prediction loss to `True` in order to # return loss for perplexity calculation. prediction_loss_only = True , # The initial learning rate for Adam. # Defaults to 5e-5. learning_rate = 5e-5 , # The weight decay to apply (if not zero). weight_decay = 0 , # Epsilon for the Adam optimizer. # Defaults to 1e-8 adam_epsilon = 1e-8 , # Maximum gradient norm (for gradient # clipping). Defaults to 0. max_grad_norm = 1.0 , # Total number of training epochs to perform # (if not an integer, will perform the # decimal part percents of # the last epoch before stopping training). num_train_epochs = 2 , # Number of updates steps before two checkpoint saves. # Defaults to 500 save_steps = - 1 , ) Load Configuration, Tokenizer and Model Loading the three essential parts of the pretrained transformers: configuration, tokenizer and model. Since I use the AutoClass functionality from Hugging Face I only need to worry about the model's name as input and the rest is handled by the transformers library. I will be calling each three functions created in the Helper Functions tab that help return config of the model, tokenizer of the model and the actual PyTorch model . After model is loaded is always good practice to resize the model depending on the tokenizer size. This means that the tokenizer's vocabulary will be aligned with the models embedding layer. This is very useful when we have a different tokenizer that the pretrained one or we train a transformer model from scratch. # Load model configuration. print ( 'Loading model configuration...' ) config = get_model_config ( model_data_args ) # Load model tokenizer. print ( 'Loading model`s tokenizer...' ) tokenizer = get_tokenizer ( model_data_args ) # Loading model. print ( 'Loading actual model...' ) model = get_model ( model_data_args , config ) # Resize model to fit all tokens in tokenizer. model . resize_token_embeddings ( len ( tokenizer )) Loading model configuration... Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|433/433 [00:01<00:00, 285B/s] Loading model`s tokenizer... Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|433/433 [00:01<00:00, 285B/s] Loading actual model... Downloading: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|436M/436M [00:36<00:00, 11.9MB/s] Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias'] - This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Embedding(28996, 768, padding_idx=0) Dataset and Collator This is where I create the PyTorch Dataset and data collator objects that will be used to feed data into our model. This is where I use the MovieReviewsDataset text files created with the movie_reviews_to_file function. Since data is partitioned for both train and test I will create two text files: one used for train and one used for evaluation. I strongly recommend to use a validation text file in order to determine how much training is needed in order to avoid overfitting. After you figure out what parameters yield the best results, the validation file can be incorporated in train and run a final train with the whole dataset. The data collator is used to format the PyTorch Dataset outputs to match the output of our specific transformers model: i.e. for Bert it will created the masked tokens needed to train. # Create texts file from train data. movie_reviews_to_file ( path_data = '/content/aclImdb/train' , path_texts_file = '/content/train.txt' ) # Create texts file from test data. movie_reviews_to_file ( path_data = '/content/aclImdb/test' , path_texts_file = '/content/test.txt' ) # Setup train dataset if `do_train` is set. print ( 'Creating train dataset...' ) train_dataset = get_dataset ( model_data_args , tokenizer = tokenizer , evaluate = False ) if training_args . do_train else None # Setup evaluation dataset if `do_eval` is set. print ( 'Creating evaluate dataset...' ) eval_dataset = get_dataset ( model_data_args , tokenizer = tokenizer , evaluate = True ) if training_args . do_eval else None # Get data collator to modify data format depending on type of model used. data_collator = get_collator ( model_data_args , config , tokenizer ) # Check how many logging prints you'll have. This is to avoid overflowing the # notebook with a lot of prints. Display warning to user if the logging steps # that will be displayed is larger than 100. if ( len ( train_dataset ) // training_args . per_device_train_batch_size \\ // training_args . logging_steps * training_args . num_train_epochs ) > 100 : # Display warning. warnings . warn ( 'Your `logging_steps` value will will do a lot of printing!' \\ ' Consider increasing `logging_steps` to avoid overflowing' \\ ' the notebook with a lot of prints!' ) Reading `train` partition... neg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] pos: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] `.txt` file saved in `/content/train.txt` Reading `test` partition... neg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] pos: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] `.txt` file saved in `/content/train.txt` Creating train dataset... Creating evaluate dataset... Train Hugging Face was very nice to us for creating the Trainer class. This helps make PyTorch model training of transformers very easy! We just need to make sure we loaded the proper parameters and everything else is taking care of! At the end of the training the tokenizer is saved along with the model so you can easily re-use it later or even load in on Hugging Face Models. I configured the arguments to display both train and validation loss at every logging_steps . It gives us a sense of how well the model is trained. # Initialize Trainer. print ( 'Loading `trainer`...' ) trainer = Trainer ( model = model , args = training_args , data_collator = data_collator , train_dataset = train_dataset , eval_dataset = eval_dataset , ) # Check model path to save. if training_args . do_train : print ( 'Start training...' ) # Setup model path if the model to train loaded from a local path. model_path = ( model_data_args . model_name_or_path if model_data_args . model_name_or_path is not None and os . path . isdir ( model_data_args . model_name_or_path ) else None ) # Run training. trainer . train ( model_path = model_path ) # Save model. trainer . save_model () # For convenience, we also re-save the tokenizer to the same directory, # so that you can share your model easily on huggingface.co/models =). if trainer . is_world_process_zero (): tokenizer . save_pretrained ( training_args . output_dir ) Loading `trainer`... Start training... |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|[5000/5000 09:43, Epoch 2/2] Step Training Loss Validation Loss 700 2.804672 2.600590 1400 2.666996 2.548267 2100 2.625075 2.502431 2800 2.545872 2.485056 3500 2.470102 2.444808 4200 2.464950 2.420487 4900 2.436973 2.410310 Plot Train The Trainer class is so useful that it will record the log history for us. I use this to access the train and validation losses recorded at each logging_steps during training. Since we are training / fine-tuning / extended training or pretraining (depending what terminology you use) a language model, we want to compute the perplexity. This is what Wikipedia says about perplexity: In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample. We can look at the perplexity plot in the same way we look at the loss plot: the lower the better and if the validation perplexity starts to increase we are starting to overfit the model. Note: It looks from the plots that the train loss is higher than validation loss. That means that our validation data is too easy for the model and we should use a different validation dataset. Since the purpose of this notebook is to show how to train transformers models and provide tools to evaluate such process I will leave the results as is . # Keep track of train and evaluate loss. loss_history = { 'train_loss' :[], 'eval_loss' :[]} # Keep track of train and evaluate perplexity. # This is a metric useful to track for language models. perplexity_history = { 'train_perplexity' :[], 'eval_perplexity' :[]} # Loop through each log history. for log_history in trainer . state . log_history : if 'loss' in log_history . keys (): # Deal with trianing loss. loss_history [ 'train_loss' ] . append ( log_history [ 'loss' ]) perplexity_history [ 'train_perplexity' ] . append ( math . exp ( log_history [ 'loss' ])) elif 'eval_loss' in log_history . keys (): # Deal with eval loss. loss_history [ 'eval_loss' ] . append ( log_history [ 'eval_loss' ]) perplexity_history [ 'eval_perplexity' ] . append ( math . exp ( log_history [ 'eval_loss' ])) # Plot Losses. plot_dict ( loss_history , start_step = training_args . logging_steps , step_size = training_args . logging_steps , use_title = 'Loss' , use_xlabel = 'Train Steps' , use_ylabel = 'Values' , magnify = 2 ) print () # Plot Perplexities. plot_dict ( perplexity_history , start_step = training_args . logging_steps , step_size = training_args . logging_steps , use_title = 'Perplexity' , use_xlabel = 'Train Steps' , use_ylabel = 'Values' , magnify = 2 ) Evaluate For the final evaluation we can have a separate test set that we use to do our final perplexity evaluation. For simplicity I used the same validation text file for the final evaluation. That is the reason I get the same results as the last validation perplexity plot value. # check if `do_eval` flag is set. if training_args . do_eval : # capture output if trainer evaluate. eval_output = trainer . evaluate () # compute perplexity from model loss. perplexity = math . exp ( eval_output [ \"eval_loss\" ]) print ( ' \\n Evaluate Perplexity: {:10,.2f} ' . format ( perplexity )) else : print ( 'No evaluation needed. No evaluation data provided, `do_eval=False`!' ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|[250/250 00:25] Evaluate Perplexity: 11.01 Final Note If you made it this far Congrats! \ud83c\udf8a and Thank you! \ud83d\ude4f for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you see something wrong please let me know by opening an issue on my ml_things GitHub repository ! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can. Contact \ud83c\udfa3 \ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"Pretrain Transformers"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#pretrain-transformers-models-in-pytorch-using-hugging-face-transformers","text":"","title":"Pretrain Transformers Models in PyTorch using Hugging Face Transformers"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#pretrain-67-transformers-models-on-your-custom-dataset","text":"Disclaimer: The format of this tutorial notebook is very similar with my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format. This notebook is used to pretrain transformers models using Huggingface on your own custom dataset. What do I mean by pretrain transformers ? The definition of pretraining is to train in advance . That is exactly what I mean! Train a transformer model to use it as a pretrained transformers model which can be used to fine-tune it on a specific task! I also use the term fine-tune where I mean to continue training a pretrained model on a custom dataset. I know it is confusing and I hope I'm not making it worse. At the end of the day you are training a transformer model that was previously trained or not! With the AutoClasses functionality we can reuse the code on a large number of transformers models! This notebook is designed to: Use an already pretrained transformers model and fine-tune (continue training) it on your custom dataset. Train a transformer model from scratch on a custom dataset. This requires an already trained (pretrained) tokenizer. This notebook will use by default the pretrained tokenizer if an already trained tokenizer is no provided. This notebook is heavily inspired from the Hugging Face script used for training language models: transformers/tree/master/examples/language-modeling . I basically adapted that script to work nicely in a notebook with a lot more comments. Notes from transformers/tree/master/examples/language-modeling : Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.","title":"Pretrain 67 transformers models on your custom dataset."},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#what-should-i-know-for-this-notebook","text":"Since I am using PyTorch to fine-tune our transformers models any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too. In this notebook I am using raw text data to pretrain / train / fine-tune transformers models . There is no need for labeled data since we are not doing classification. The Transformers library handles the text files in same way as the original implementation of each model did.","title":"What should I know for this notebook?"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#how-to-use-this-notebook","text":"Like with every project, I built this notebook with reusability in mind. This notebook uses a custom dataset from .txt files. Since the dataset does not come in a single .txt file I created a custom function movie_reviews_to_file that reads the dataset and creates the text file. The way I load the .txt files can be easily reused for any other dataset. The only modifications needed to use your own dataset will be in the paths provided to the train .txt file and evaluation .txt file. All parameters that need to be changed are under the Parameters Setup section. Each parameter is nicely commented and structured to be as intuitive as possible.","title":"How to use this notebook?"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#what-transformers-models-work-with-this-notebook","text":"A lot of people will probably use it for Bert. When there is a need to run a different transformer model architecture, which one would work with this code? Since the name of the notebooks is pretrain_transformers it should work with more than one type of transformers. I ran this notebook across all the pretrained models found on Hugging Face Transformer. This way you know ahead of time if the model you plan to use works with this code without any modifications. The list of pretrained transformers models that work with this notebook can be found here . There are 67 models that worked \ud83d\ude04 and 39 models that failed to work \ud83d\ude22 with this notebook. Remember these are pretrained models and fine-tuned on custom dataset.","title":"What transformers models work with this notebook?"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#dataset","text":"This notebook will cover pretraining transformers on a custom dataset. I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with.","title":"Dataset"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#coding","text":"Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant\u200a-\u200ashow the output. I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.","title":"Coding"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#downloads","text":"Download the Large Movie Review Dataset and unzip it locally. # Download the dataset. ! wget - q - nc http : // ai . stanford . edu /~ amaas / data / sentiment / aclImdb_v1 . tar . gz # Unzip the dataset. ! tar - zxf / content / aclImdb_v1 . tar . gz","title":"Downloads"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#installs","text":"transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. # Install transformers library. ! pip install - q git + https : // github . com / huggingface / transformers . git # Install helper functions. ! pip install - q git + https : // github . com / gmihaila / ml_things . git Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 6.7MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 48.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 49.0MB/s Building wheel for transformers (PEP 517) ... done Building wheel for sacremoses (setup.py) ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Building wheel for ml-things (setup.py) ... done Building wheel for ftfy (setup.py) ... done","title":"Installs"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#imports","text":"Import all needed libraries for this notebook. Declare basic parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. device - Look for gpu to use. I will use cpu by default if no gpu found. import io import os import math import torch import warnings from tqdm.notebook import tqdm from ml_things import plot_dict , fix_text from transformers import ( CONFIG_MAPPING , MODEL_FOR_MASKED_LM_MAPPING , MODEL_FOR_CAUSAL_LM_MAPPING , PreTrainedTokenizer , TrainingArguments , AutoConfig , AutoTokenizer , AutoModelWithLMHead , AutoModelForCausalLM , AutoModelForMaskedLM , LineByLineTextDataset , TextDataset , DataCollatorForLanguageModeling , DataCollatorForWholeWordMask , DataCollatorForPermutationLanguageModeling , PretrainedConfig , Trainer , set_seed , ) # Set seed for reproducibility, set_seed ( 123 ) # Look for gpu to use. Will use `cpu` by default if no gpu found. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' )","title":"Imports"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#helper-functions","text":"I like to keep all Classes and functions that will be used in this notebook under this section to help maintain a clean look of the notebook: movie_reviews_to_file(path_data: str, path_texts_file: str) As I mentioned before, we will need .txt files to run this notebook. Since the Large Movie Review Dataset comes in multiple files with different labels I created this function to put together all data in a single .txt file. Examples are saved on each line of the file. The path_data points to the path where data files are present and path_texts_file will be the .txt file containing all data. ModelDataArguments This class follows similar format as the [transformers](( huggingface/transformers ) library. The main difference is the way I combined multiple types of arguments into one and used rules to make sure the arguments used are correctly set. Here are all argument detailed (they are also mentioned in the class documentation): train_data_file : Path to your .txt file dataset. If you have an example on each line of the file make sure to use line_by_line=True . If the data file contains all text data without any special grouping use line_by_line=False to move a block_size window across the text file. eval_data_file : Path to evaluation .txt file. It has the same format as train_data_file . line_by_line : If the train_data_file and eval_data_file contains separate examples on each line set line_by_line=True . If there is no separation between examples and train_data_file and eval_data_file contains continuous text then line_by_line=False and a window of block_size will be moved across the files to acquire examples. mlm : Is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta and set to False otherwise. There are functions that will raise ValueError if this argument is not set accordingly. whole_word_mask : Used as flag to determine if we decide to use whole word masking or not. Whole word masking means that whole words will be masked during training instead of tokens which can be chunks of words. mlm_probability : Used when training masked language models. Needs to have mlm=True . It represents the probability of masking tokens when training model. plm_probability : Flag to define the ratio of length of a span of masked tokens to surrounding context length for permutation language modeling. Used for XLNet. max_span_length : Flag may also be used to limit the length of a span of masked tokens used for permutation language modeling. Used for XLNet. block_size : It refers to the windows size that is moved across the text file. Set to -1 to use maximum allowed length. overwrite_cache : If there are any cached files, overwrite them. model_type : Type of model used: bert, roberta, gpt2. More details here . model_config_name : Config of model used: bert, roberta, gpt2. More details here . tokenizer_name : Tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased, roberta-base, gpt2 etc. model_name_or_path : Path to existing transformers model or name of transformer model to be used: bert-base-cased, roberta-base, gpt2 etc. More details here . model_cache_dir : Path to cache files. It helps to save time when re-running code. get_model_config(args: ModelDataArguments) Get model configuration. Using the ModelDataArguments to return the model configuration. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. Returns: Model transformers configuration. Raises: ValueError: If mlm=True and model_type is NOT in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"]. We need to use a masked language model in order to set mlm=True . get_tokenizer(args: ModelDataArguments) Get model tokenizer.Using the ModelDataArguments return the model tokenizer and change block_size form args if needed. Here are all argument detailed: args : Model and data configuration arugments needed to perform pretraining. Returns: Model transformers tokenizer. get_model(args: ModelDataArguments, model_config) Get model. Using the ModelDataArguments return the actual model. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. model_config : Model transformers configuration. Returns: PyTorch model. get_dataset(args: ModelDataArguments, tokenizer: PreTrainedTokenizer, evaluate: bool=False) Process dataset file into PyTorch Dataset. Using the ModelDataArguments return the actual model. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. tokenizer : Model transformers tokenizer. evaluate : If set to True the test / validation file is being handled. If set to False the train file is being handled. Returns: PyTorch Dataset that contains file's data. get_collator(args: ModelDataArguments, model_config: PretrainedConfig, tokenizer: PreTrainedTokenizer) Get appropriate collator function. Collator function will be used to collate a PyTorch Dataset object. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. model_config : Model transformers configuration. tokenizer : Model transformers tokenizer. Returns: Transformers specific data collator. def movie_reviews_to_file ( path_data : str , path_texts_file : str ): r \"\"\"Reading in all data from path and saving it into a single `.txt` file. In the pretraining process of our transformers model we require a text file. This function is designed to work for the Movie Reviews Dataset. You wil have to create your own function to move all examples into a text file if you don't already have a text file with all your unlabeled data. Arguments: path_data (:obj:`str`): Path to the Movie Review Dataset partition. We only have `\\train` and `test` partitions. path_texts_file (:obj:`str`): File path of the generated `.txt` file that contains one example / line. \"\"\" # Check if path exists. if not os . path . isdir ( path_data ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) # Check max sequence length. texts = [] print ( 'Reading ` %s ` partition...' % ( os . path . basename ( path_data ))) # Since the labels are defined by folders with data we loop # through each label. for label in [ 'neg' , 'pos' ]: sentiment_path = os . path . join ( path_data , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:30] # SAMPLE FOR DEBUGGING. # Go through each file and read its content. for file_name in tqdm ( files_names , desc = label , unit = 'files' ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. texts . append ( content ) # Move list to single string. all_texts = ' \\n ' . join ( texts ) # Send all texts string to single file. io . open ( file = path_texts_file , mode = 'w' , encoding = 'utf-8' ) . write ( all_texts ) # Print when done. print ( '`.txt` file saved in ` %s ` \\n ' % path_texts_file ) return class ModelDataArguments ( object ): r \"\"\"Define model and data configuration needed to perform pretraining. Eve though all arguments are optional there still needs to be a certain number of arguments that require values attributed. Arguments: train_data_file (:obj:`str`, `optional`): Path to your .txt file dataset. If you have an example on each line of the file make sure to use line_by_line=True. If the data file contains all text data without any special grouping use line_by_line=False to move a block_size window across the text file. This argument is optional and it will have a `None` value attributed inside the function. eval_data_file (:obj:`str`, `optional`): Path to evaluation .txt file. It has the same format as train_data_file. This argument is optional and it will have a `None` value attributed inside the function. line_by_line (:obj:`bool`, `optional`, defaults to :obj:`False`): If the train_data_file and eval_data_file contains separate examples on each line then line_by_line=True. If there is no separation between examples and train_data_file and eval_data_file contains continuous text then line_by_line=False and a window of block_size will be moved across the files to acquire examples. This argument is optional and it has a default value. mlm (:obj:`bool`, `optional`, defaults to :obj:`False`): Is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta and set to False otherwise. There are functions that will raise ValueError if this argument is not set accordingly. This argument is optional and it has a default value. whole_word_mask (:obj:`bool`, `optional`, defaults to :obj:`False`): Used as flag to determine if we decide to use whole word masking or not. Whole word masking means that whole words will be masked during training instead of tokens which can be chunks of words. This argument is optional and it has a default value. mlm_probability(:obj:`float`, `optional`, defaults to :obj:`0.15`): Used when training masked language models. Needs to have mlm set to True. It represents the probability of masking tokens when training model. This argument is optional and it has a default value. plm_probability (:obj:`float`, `optional`, defaults to :obj:`float(1/6)`): Flag to define the ratio of length of a span of masked tokens to surrounding context length for permutation language modeling. Used for XLNet. This argument is optional and it has a default value. max_span_length (:obj:`int`, `optional`, defaults to :obj:`5`): Flag may also be used to limit the length of a span of masked tokens used for permutation language modeling. Used for XLNet. This argument is optional and it has a default value. block_size (:obj:`int`, `optional`, defaults to :obj:`-1`): It refers to the windows size that is moved across the text file. Set to -1 to use maximum allowed length. This argument is optional and it has a default value. overwrite_cache (:obj:`bool`, `optional`, defaults to :obj:`False`): If there are any cached files, overwrite them. This argument is optional and it has a default value. model_type (:obj:`str`, `optional`): Type of model used: bert, roberta, gpt2. More details: https://huggingface.co/transformers/pretrained_models.html This argument is optional and it will have a `None` value attributed inside the function. model_config_name (:obj:`str`, `optional`): Config of model used: bert, roberta, gpt2. More details: https://huggingface.co/transformers/pretrained_models.html This argument is optional and it will have a `None` value attributed inside the function. tokenizer_name: (:obj:`str`, `optional`) Tokenizer used to process data for training the model. It usually has same name as model_name_or_path: bert-base-cased, roberta-base, gpt2 etc. This argument is optional and it will have a `None` value attributed inside the function. model_name_or_path (:obj:`str`, `optional`): Path to existing transformers model or name of transformer model to be used: bert-base-cased, roberta-base, gpt2 etc. More details: https://huggingface.co/transformers/pretrained_models.html This argument is optional and it will have a `None` value attributed inside the function. model_cache_dir (:obj:`str`, `optional`): Path to cache files to save time when re-running code. This argument is optional and it will have a `None` value attributed inside the function. Raises: ValueError: If `CONFIG_MAPPING` is not loaded in global variables. ValueError: If `model_type` is not present in `CONFIG_MAPPING.keys()`. ValueError: If `model_type`, `model_config_name` and `model_name_or_path` variables are all `None`. At least one of them needs to be set. warnings: If `model_config_name` and `model_name_or_path` are both `None`, the model will be trained from scratch. ValueError: If `tokenizer_name` and `model_name_or_path` are both `None`. We need at least one of them set to load tokenizer. \"\"\" def __init__ ( self , train_data_file = None , eval_data_file = None , line_by_line = False , mlm = False , mlm_probability = 0.15 , whole_word_mask = False , plm_probability = float ( 1 / 6 ), max_span_length = 5 , block_size =- 1 , overwrite_cache = False , model_type = None , model_config_name = None , tokenizer_name = None , model_name_or_path = None , model_cache_dir = None ): # Make sure CONFIG_MAPPING is imported from transformers module. if 'CONFIG_MAPPING' not in globals (): raise ValueError ( 'Could not find `CONFIG_MAPPING` imported! Make sure' \\ ' to import it from `transformers` module!' ) # Make sure model_type is valid. if ( model_type is not None ) and ( model_type not in CONFIG_MAPPING . keys ()): raise ValueError ( 'Invalid `model_type`! Use one of the following: %s ' % ( str ( list ( CONFIG_MAPPING . keys ())))) # Make sure that model_type, model_config_name and model_name_or_path # variables are not all `None`. if not any ([ model_type , model_config_name , model_name_or_path ]): raise ValueError ( 'You can`t have all `model_type`, `model_config_name`,' \\ ' `model_name_or_path` be `None`! You need to have' \\ 'at least one of them set!' ) # Check if a new model will be loaded from scratch. if not any ([ model_config_name , model_name_or_path ]): # Setup warning to show pretty. This is an overkill warnings . formatwarning = lambda message , category , * args , ** kwargs : \\ ' %s : %s \\n ' % ( category . __name__ , message ) # Display warning. warnings . warn ( 'You are planning to train a model from scratch! \ud83d\ude40' ) # Check if a new tokenizer wants to be loaded. # This feature is not supported! if not any ([ tokenizer_name , model_name_or_path ]): # Can't train tokenizer from scratch here! Raise error. raise ValueError ( 'You want to train tokenizer from scratch! ' \\ 'That is not possible yet! You can train your own ' \\ 'tokenizer separately and use path here to load it!' ) # Set all data related arguments. self . train_data_file = train_data_file self . eval_data_file = eval_data_file self . line_by_line = line_by_line self . mlm = mlm self . whole_word_mask = whole_word_mask self . mlm_probability = mlm_probability self . plm_probability = plm_probability self . max_span_length = max_span_length self . block_size = block_size self . overwrite_cache = overwrite_cache # Set all model and tokenizer arguments. self . model_type = model_type self . model_config_name = model_config_name self . tokenizer_name = tokenizer_name self . model_name_or_path = model_name_or_path self . model_cache_dir = model_cache_dir return def get_model_config ( args : ModelDataArguments ): r \"\"\" Get model configuration. Using the ModelDataArguments return the model configuration. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. Returns: :obj:`PretrainedConfig`: Model transformers configuration. Raises: ValueError: If `mlm=True` and `model_type` is NOT in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"]. We need to use a masked language model in order to set `mlm=True`. \"\"\" # Check model configuration. if args . model_config_name is not None : # Use model configure name if defined. model_config = AutoConfig . from_pretrained ( args . model_config_name , cache_dir = args . model_cache_dir ) elif args . model_name_or_path is not None : # Use model name or path if defined. model_config = AutoConfig . from_pretrained ( args . model_name_or_path , cache_dir = args . model_cache_dir ) else : # Use config mapping if building model from scratch. model_config = CONFIG_MAPPING [ args . model_type ]() # Make sure `mlm` flag is set for Masked Language Models (MLM). if ( model_config . model_type in [ \"bert\" , \"roberta\" , \"distilbert\" , \"camembert\" ]) and ( args . mlm is False ): raise ValueError ( 'BERT and RoBERTa-like models do not have LM heads ' \\ 'butmasked LM heads. They must be run setting `mlm=True`' ) # Adjust block size for xlnet. if model_config . model_type == \"xlnet\" : # xlnet used 512 tokens when training. args . block_size = 512 # setup memory length model_config . mem_len = 1024 return model_config def get_tokenizer ( args : ModelDataArguments ): r \"\"\" Get model tokenizer. Using the ModelDataArguments return the model tokenizer and change `block_size` form `args` if needed. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. Returns: :obj:`PreTrainedTokenizer`: Model transformers tokenizer. \"\"\" # Check tokenizer configuration. if args . tokenizer_name : # Use tokenizer name if define. tokenizer = AutoTokenizer . from_pretrained ( args . tokenizer_name , cache_dir = args . model_cache_dir ) elif args . model_name_or_path : # Use tokenizer name of path if defined. tokenizer = AutoTokenizer . from_pretrained ( args . model_name_or_path , cache_dir = args . model_cache_dir ) # Setp data block size. if args . block_size <= 0 : # Set block size to maximum length of tokenizer. # Input block size will be the max possible for the model. # Some max lengths are very large and will cause a args . block_size = tokenizer . model_max_length else : # Never go beyond tokenizer maximum length. args . block_size = min ( args . block_size , tokenizer . model_max_length ) return tokenizer def get_model ( args : ModelDataArguments , model_config ): r \"\"\" Get model. Using the ModelDataArguments return the actual model. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. model_config (:obj:`PretrainedConfig`): Model transformers configuration. Returns: :obj:`torch.nn.Module`: PyTorch model. \"\"\" # Make sure MODEL_FOR_MASKED_LM_MAPPING and MODEL_FOR_CAUSAL_LM_MAPPING are # imported from transformers module. if ( 'MODEL_FOR_MASKED_LM_MAPPING' not in globals ()) and \\ ( 'MODEL_FOR_CAUSAL_LM_MAPPING' not in globals ()): raise ValueError ( 'Could not find `MODEL_FOR_MASKED_LM_MAPPING` and' \\ ' `MODEL_FOR_MASKED_LM_MAPPING` imported! Make sure to' \\ ' import them from `transformers` module!' ) # Check if using pre-trained model or train from scratch. if args . model_name_or_path : # Use pre-trained model. if type ( model_config ) in MODEL_FOR_MASKED_LM_MAPPING . keys (): # Masked language modeling head. return AutoModelForMaskedLM . from_pretrained ( args . model_name_or_path , from_tf = bool ( \".ckpt\" in args . model_name_or_path ), config = model_config , cache_dir = args . model_cache_dir , ) elif type ( model_config ) in MODEL_FOR_CAUSAL_LM_MAPPING . keys (): # Causal language modeling head. return AutoModelForCausalLM . from_pretrained ( args . model_name_or_path , from_tf = bool ( \".ckpt\" in args . model_name_or_path ), config = model_config , cache_dir = args . model_cache_dir ) else : raise ValueError ( 'Invalid `model_name_or_path`! It should be in %s or %s !' % ( str ( MODEL_FOR_MASKED_LM_MAPPING . keys ()), str ( MODEL_FOR_CAUSAL_LM_MAPPING . keys ()))) else : # Use model from configuration - train from scratch. print ( \"Training new model from scratch!\" ) return AutoModelWithLMHead . from_config ( config ) def get_dataset ( args : ModelDataArguments , tokenizer : PreTrainedTokenizer , evaluate : bool = False ): r \"\"\" Process dataset file into PyTorch Dataset. Using the ModelDataArguments return the actual model. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. tokenizer (:obj:`PreTrainedTokenizer`): Model transformers tokenizer. evaluate (:obj:`bool`, `optional`, defaults to :obj:`False`): If set to `True` the test / validation file is being handled. If set to `False` the train file is being handled. Returns: :obj:`Dataset`: PyTorch Dataset that contains file's data. \"\"\" # Get file path for either train or evaluate. file_path = args . eval_data_file if evaluate else args . train_data_file # Check if `line_by_line` flag is set to `True`. if args . line_by_line : # Each example in data file is on each line. return LineByLineTextDataset ( tokenizer = tokenizer , file_path = file_path , block_size = args . block_size ) else : # All data in file is put together without any separation. return TextDataset ( tokenizer = tokenizer , file_path = file_path , block_size = args . block_size , overwrite_cache = args . overwrite_cache ) def get_collator ( args : ModelDataArguments , model_config : PretrainedConfig , tokenizer : PreTrainedTokenizer ): r \"\"\" Get appropriate collator function. Collator function will be used to collate a PyTorch Dataset object. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. model_config (:obj:`PretrainedConfig`): Model transformers configuration. tokenizer (:obj:`PreTrainedTokenizer`): Model transformers tokenizer. Returns: :obj:`data_collator`: Transformers specific data collator. \"\"\" # Special dataset handle depending on model type. if model_config . model_type == \"xlnet\" : # Configure collator for XLNET. return DataCollatorForPermutationLanguageModeling ( tokenizer = tokenizer , plm_probability = args . plm_probability , max_span_length = args . max_span_length , ) else : # Configure data for rest of model types. if args . mlm and args . whole_word_mask : # Use whole word masking. return DataCollatorForWholeWordMask ( tokenizer = tokenizer , mlm_probability = args . mlm_probability , ) else : # Regular language modeling. return DataCollatorForLanguageModeling ( tokenizer = tokenizer , mlm = args . mlm , mlm_probability = args . mlm_probability , )","title":"Helper Functions"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#parameters-setup","text":"Declare the rest of the parameters used for this notebook: model_data_args contains all arguments needed to setup dataset, model configuration, model tokenizer and the actual model. This is created using the ModelDataArguments class. training_args contain all arguments needed to use the Trainer functionality from Transformers that allows us to train transformers models in PyTorch very easy. You can find the complete documentation here . There are a lot of parameters that can be set to allow multiple functionalities. I only used the following parameters (the comments are inspired from the HuggingFace documentation of TrainingArguments : output_dir : The output directory where the model predictions and checkpoints will be written. I set it up to pretrained_bert_model where the model and will be saved. overwrite_output_dir : Overwrite the content of the output directory. I set it to True in case I run the notebook multiple times I only care about the last run. do_train : Whether to run training or not. I set this parameter to True because I want to train the model on my custom dataset. do_eval : Whether to run evaluation on the evaluation files or not. I set it to True since I have test data file and I want to evaluate how well the model trains. per_device_train_batch_size : Batch size GPU/TPU core/CPU training. I set it to 2 for this example. I recommend setting it up as high as your GPU memory allows you. per_device_eval_batch_size : Batch size GPU/TPU core/CPU for evaluation.I set this value to 100 since it's not dealing with gradients. evaluation_strategy : Evaluation strategy to adopt during training: no : No evaluation during training; steps : Evaluate every eval_steps; epoch`: Evaluate every end of epoch. I set it to 'steps' since I want to evaluate model more often. logging_steps : How often to show logs. I will se this to plot history loss and calculate perplexity. I set this to 20 just as an example. If your evaluate data is large you might not want to run it that often because it will significantly slow down training time. eval_steps : Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set. Since I want to evaluate model ever logging_steps I will set this to None since it will inherit same value as logging_steps . prediction_loss_only : Set prediction loss to True in order to return loss for perplexity calculation. Since I want to calculate perplexity I set this to True since I want to monitor loss and perplexity (which is exp(loss)). learning_rate : The initial learning rate for Adam. Defaults is set to 5e-5 . weight_decay : The weight decay to apply (if not zero)Defaults is set to 0 . adam_epsilon : Epsilon for the Adam optimizer. Defaults to 1e-8 . max_grad_norm : Maximum gradient norm (for gradient clipping). Defaults to 0 . num_train_epochs : Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training). I set it to 2 at most. Since the custom dataset will be a lot smaller than the original dataset the model was trained on we don't want to overfit. save_steps : Number of updates steps before two checkpoint saves. Defaults to 500 . # Define arguments for data, tokenizer and model arguments. # See comments in `ModelDataArguments` class. model_data_args = ModelDataArguments ( train_data_file = '/content/train.txt' , eval_data_file = '/content/test.txt' , line_by_line = True , mlm = True , whole_word_mask = True , mlm_probability = 0.15 , plm_probability = float ( 1 / 6 ), max_span_length = 5 , block_size = 50 , overwrite_cache = False , model_type = 'bert' , model_config_name = 'bert-base-cased' , tokenizer_name = 'bert-base-cased' , model_name_or_path = 'bert-base-cased' , model_cache_dir = None , ) # Define arguments for training # Note: I only used the arguments I care about. `TrainingArguments` contains # a lot more arguments. For more details check the awesome documentation: # https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments training_args = TrainingArguments ( # The output directory where the model predictions # and checkpoints will be written. output_dir = 'pretrain_bert' , # Overwrite the content of the output directory. overwrite_output_dir = True , # Whether to run training or not. do_train = True , # Whether to run evaluation on the dev or not. do_eval = True , # Batch size GPU/TPU core/CPU training. per_device_train_batch_size = 10 , # Batch size GPU/TPU core/CPU for evaluation. per_device_eval_batch_size = 100 , # evaluation strategy to adopt during training # `no`: No evaluation during training. # `steps`: Evaluate every `eval_steps`. # `epoch`: Evaluate every end of epoch. evaluation_strategy = 'steps' , # How often to show logs. I will se this to # plot history loss and calculate perplexity. logging_steps = 700 , # Number of update steps between two # evaluations if evaluation_strategy=\"steps\". # Will default to the same value as l # logging_steps if not set. eval_steps = None , # Set prediction loss to `True` in order to # return loss for perplexity calculation. prediction_loss_only = True , # The initial learning rate for Adam. # Defaults to 5e-5. learning_rate = 5e-5 , # The weight decay to apply (if not zero). weight_decay = 0 , # Epsilon for the Adam optimizer. # Defaults to 1e-8 adam_epsilon = 1e-8 , # Maximum gradient norm (for gradient # clipping). Defaults to 0. max_grad_norm = 1.0 , # Total number of training epochs to perform # (if not an integer, will perform the # decimal part percents of # the last epoch before stopping training). num_train_epochs = 2 , # Number of updates steps before two checkpoint saves. # Defaults to 500 save_steps = - 1 , )","title":"Parameters Setup"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#load-configuration-tokenizer-and-model","text":"Loading the three essential parts of the pretrained transformers: configuration, tokenizer and model. Since I use the AutoClass functionality from Hugging Face I only need to worry about the model's name as input and the rest is handled by the transformers library. I will be calling each three functions created in the Helper Functions tab that help return config of the model, tokenizer of the model and the actual PyTorch model . After model is loaded is always good practice to resize the model depending on the tokenizer size. This means that the tokenizer's vocabulary will be aligned with the models embedding layer. This is very useful when we have a different tokenizer that the pretrained one or we train a transformer model from scratch. # Load model configuration. print ( 'Loading model configuration...' ) config = get_model_config ( model_data_args ) # Load model tokenizer. print ( 'Loading model`s tokenizer...' ) tokenizer = get_tokenizer ( model_data_args ) # Loading model. print ( 'Loading actual model...' ) model = get_model ( model_data_args , config ) # Resize model to fit all tokens in tokenizer. model . resize_token_embeddings ( len ( tokenizer )) Loading model configuration... Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|433/433 [00:01<00:00, 285B/s] Loading model`s tokenizer... Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|433/433 [00:01<00:00, 285B/s] Loading actual model... Downloading: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|436M/436M [00:36<00:00, 11.9MB/s] Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias'] - This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Embedding(28996, 768, padding_idx=0)","title":"Load Configuration, Tokenizer and Model"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#dataset-and-collator","text":"This is where I create the PyTorch Dataset and data collator objects that will be used to feed data into our model. This is where I use the MovieReviewsDataset text files created with the movie_reviews_to_file function. Since data is partitioned for both train and test I will create two text files: one used for train and one used for evaluation. I strongly recommend to use a validation text file in order to determine how much training is needed in order to avoid overfitting. After you figure out what parameters yield the best results, the validation file can be incorporated in train and run a final train with the whole dataset. The data collator is used to format the PyTorch Dataset outputs to match the output of our specific transformers model: i.e. for Bert it will created the masked tokens needed to train. # Create texts file from train data. movie_reviews_to_file ( path_data = '/content/aclImdb/train' , path_texts_file = '/content/train.txt' ) # Create texts file from test data. movie_reviews_to_file ( path_data = '/content/aclImdb/test' , path_texts_file = '/content/test.txt' ) # Setup train dataset if `do_train` is set. print ( 'Creating train dataset...' ) train_dataset = get_dataset ( model_data_args , tokenizer = tokenizer , evaluate = False ) if training_args . do_train else None # Setup evaluation dataset if `do_eval` is set. print ( 'Creating evaluate dataset...' ) eval_dataset = get_dataset ( model_data_args , tokenizer = tokenizer , evaluate = True ) if training_args . do_eval else None # Get data collator to modify data format depending on type of model used. data_collator = get_collator ( model_data_args , config , tokenizer ) # Check how many logging prints you'll have. This is to avoid overflowing the # notebook with a lot of prints. Display warning to user if the logging steps # that will be displayed is larger than 100. if ( len ( train_dataset ) // training_args . per_device_train_batch_size \\ // training_args . logging_steps * training_args . num_train_epochs ) > 100 : # Display warning. warnings . warn ( 'Your `logging_steps` value will will do a lot of printing!' \\ ' Consider increasing `logging_steps` to avoid overflowing' \\ ' the notebook with a lot of prints!' ) Reading `train` partition... neg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] pos: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] `.txt` file saved in `/content/train.txt` Reading `test` partition... neg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] pos: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] `.txt` file saved in `/content/train.txt` Creating train dataset... Creating evaluate dataset...","title":"Dataset and Collator"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#train","text":"Hugging Face was very nice to us for creating the Trainer class. This helps make PyTorch model training of transformers very easy! We just need to make sure we loaded the proper parameters and everything else is taking care of! At the end of the training the tokenizer is saved along with the model so you can easily re-use it later or even load in on Hugging Face Models. I configured the arguments to display both train and validation loss at every logging_steps . It gives us a sense of how well the model is trained. # Initialize Trainer. print ( 'Loading `trainer`...' ) trainer = Trainer ( model = model , args = training_args , data_collator = data_collator , train_dataset = train_dataset , eval_dataset = eval_dataset , ) # Check model path to save. if training_args . do_train : print ( 'Start training...' ) # Setup model path if the model to train loaded from a local path. model_path = ( model_data_args . model_name_or_path if model_data_args . model_name_or_path is not None and os . path . isdir ( model_data_args . model_name_or_path ) else None ) # Run training. trainer . train ( model_path = model_path ) # Save model. trainer . save_model () # For convenience, we also re-save the tokenizer to the same directory, # so that you can share your model easily on huggingface.co/models =). if trainer . is_world_process_zero (): tokenizer . save_pretrained ( training_args . output_dir ) Loading `trainer`... Start training... |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|[5000/5000 09:43, Epoch 2/2] Step Training Loss Validation Loss 700 2.804672 2.600590 1400 2.666996 2.548267 2100 2.625075 2.502431 2800 2.545872 2.485056 3500 2.470102 2.444808 4200 2.464950 2.420487 4900 2.436973 2.410310","title":"Train"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#plot-train","text":"The Trainer class is so useful that it will record the log history for us. I use this to access the train and validation losses recorded at each logging_steps during training. Since we are training / fine-tuning / extended training or pretraining (depending what terminology you use) a language model, we want to compute the perplexity. This is what Wikipedia says about perplexity: In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample. We can look at the perplexity plot in the same way we look at the loss plot: the lower the better and if the validation perplexity starts to increase we are starting to overfit the model. Note: It looks from the plots that the train loss is higher than validation loss. That means that our validation data is too easy for the model and we should use a different validation dataset. Since the purpose of this notebook is to show how to train transformers models and provide tools to evaluate such process I will leave the results as is . # Keep track of train and evaluate loss. loss_history = { 'train_loss' :[], 'eval_loss' :[]} # Keep track of train and evaluate perplexity. # This is a metric useful to track for language models. perplexity_history = { 'train_perplexity' :[], 'eval_perplexity' :[]} # Loop through each log history. for log_history in trainer . state . log_history : if 'loss' in log_history . keys (): # Deal with trianing loss. loss_history [ 'train_loss' ] . append ( log_history [ 'loss' ]) perplexity_history [ 'train_perplexity' ] . append ( math . exp ( log_history [ 'loss' ])) elif 'eval_loss' in log_history . keys (): # Deal with eval loss. loss_history [ 'eval_loss' ] . append ( log_history [ 'eval_loss' ]) perplexity_history [ 'eval_perplexity' ] . append ( math . exp ( log_history [ 'eval_loss' ])) # Plot Losses. plot_dict ( loss_history , start_step = training_args . logging_steps , step_size = training_args . logging_steps , use_title = 'Loss' , use_xlabel = 'Train Steps' , use_ylabel = 'Values' , magnify = 2 ) print () # Plot Perplexities. plot_dict ( perplexity_history , start_step = training_args . logging_steps , step_size = training_args . logging_steps , use_title = 'Perplexity' , use_xlabel = 'Train Steps' , use_ylabel = 'Values' , magnify = 2 )","title":"Plot Train"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#evaluate","text":"For the final evaluation we can have a separate test set that we use to do our final perplexity evaluation. For simplicity I used the same validation text file for the final evaluation. That is the reason I get the same results as the last validation perplexity plot value. # check if `do_eval` flag is set. if training_args . do_eval : # capture output if trainer evaluate. eval_output = trainer . evaluate () # compute perplexity from model loss. perplexity = math . exp ( eval_output [ \"eval_loss\" ]) print ( ' \\n Evaluate Perplexity: {:10,.2f} ' . format ( perplexity )) else : print ( 'No evaluation needed. No evaluation data provided, `do_eval=False`!' ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|[250/250 00:25] Evaluate Perplexity: 11.01","title":"Evaluate"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#final-note","text":"If you made it this far Congrats! \ud83c\udf8a and Thank you! \ud83d\ude4f for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you see something wrong please let me know by opening an issue on my ml_things GitHub repository ! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.","title":"Final Note"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#contact-","text":"\ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"Contact \ud83c\udfa3"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/","text":"Better Batches with PyTorchText BucketIterator How to use PyTorchText BucketIterator to sort text data for better batching. Disclaimer: The format of this tutorial notebook is very similar with my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format. This notebook is a simple tutorial on how to use the powerful PytorchText BucketIterator functionality to group examples ( I use examples and sequences interchangeably ) of similar lengths into batches. This allows us to provide the most optimal batches when training models with text data. Having batches with similar length examples provides a lot of gain for recurrent models (RNN, GRU, LSTM) and transformers models (bert, roBerta, gpt2, xlnet, etc.) where padding will be minimal. Basically any model that takes as input variable text data sequences will benefit from this tutorial. I will not train any models in this notebook! I will release a tutorial where I use this implementation to train a transformer model. The purpose is to use an example text datasets and batch it using PyTorchText with BucketIterator and show how it groups text sequences of similar length in batches. This tutorial has two main parts: Using PyTorch Dataset with PyTorchText Bucket Iterator : Here I implemented a standard PyTorch Dataset class that reads in the example text datasets and use PyTorch Bucket Iterator to group similar length examples in same batches. I want to show how easy it is to use this powerful functionality form PyTorchText on a regular PyTorch Dataset workflow which you already have setup. Using PyTorch Text TabularDataset with PyTorchText Bucket Iterator : Here I use the built-in PyTorchText TabularDataset that reads data straight from local files without the need to create a PyTorch Dataset class. Then I follow same steps as in the previous part to show how nicely text examples are grouped together. This notebooks is a code adaptation and implementation inspired from a few sources: torchtext_translation_tutorial , pytorch/text - GitHub , torchtext documentation and A Comprehensive Introduction to Torchtext . What should I know for this notebook? Some basic PyTorch regarding Dataset class and using DataLoaders. Some knowledge of PyTorchText is helpful but not critical in understanding this tutorial. The BucketIterator is similar in applying Dataloader to a PyTorch Dataset. How to use this notebook? The code is made with reusability in mind. It can be easily adapted for other text datasets and other NLP tasks in order to achieve optimal batching. Comments should provide enough guidance to easily adapt this notebook to your needs. This code is designed mostly for classification tasks in mind, but it can be adapted for any other Natural Language Processing tasks where batching text data is needed. Dataset I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with. Coding Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant\u200a-\u200ashow the output. I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations. Downloads Download the IMDB Movie Reviews sentiment dataset and unzip it locally. # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Installs ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Building wheel for ml-things (setup.py) ... done Building wheel for ftfy (setup.py) ... done Imports Import all needed libraries for this notebook. Declare basic parameters used for this notebook: device - Device to use by torch: GPU/CPU. I use CPU as default since I will not perform any costly operations. train_batch_size - Batch size used on train data. valid_batch_size - Batch size used for validation data. It usually is greater than train_batch_size since the model would only need to make prediction and no gradient calculations is needed. import io import os import torchtext from tqdm.notebook import tqdm from ml_things import fix_text from torch.utils.data import Dataset , DataLoader # Will use `cpu` for simplicity. device = 'cpu' # Number of batches for training train_batch_size = 10 # Number of batches for validation. Use a larger value than training. # It helps speed up the validation process. valid_batch_size = 20 Using PyTorch Dataset This is where I create the PyTorch Dataset objects for training and validation that can be used to feed data into a model. This is standard procedure when using PyTorch. Dataset Class Implementation of the PyTorch Dataset class. Most important components in a PyTorch Dataset class are: __len__(self, ) where it returns the number of examples in our dataset that we read in __init__(self, ) . This will ensure that len() will return the number of examples. __getitem__(self, item) where given an index item will return the example corresponding to the item position. class MovieReviewsTextDataset ( Dataset ): r \"\"\"PyTorch Dataset class for loading data. This is where the data parsing happens. This class is built with reusability in mind. Arguments: path (:obj:`str`): Path to the data partition. \"\"\" def __init__ ( self , path ): # Check if path exists. if not os . path . isdir ( path ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) self . texts = [] self . labels = [] # Since the labels are defined by folders with data we loop # through each label. for label in [ 'pos' , 'neg' ]: sentiment_path = os . path . join ( path , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:10] # Sample for debugging. # Go through each file and read its content. for file_name in tqdm ( files_names , desc = f ' { label } Files' ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. self . texts . append ( content ) # Save labels. self . labels . append ( label ) # Number of examples. self . n_examples = len ( self . labels ) return def __len__ ( self ): r \"\"\"When used `len` return the number of examples. \"\"\" return self . n_examples def __getitem__ ( self , item ): r \"\"\"Given an index return an example from the position. Arguments: item (:obj:`int`): Index position to pick an example to return. Returns: :obj:`Dict[str, str]`: Dictionary of inputs that are used to feed to a model. \"\"\" return { 'text' : self . texts [ item ], 'label' : self . labels [ item ]} Train - Validation Datasets Create PyTorch Dataset for train and validation partitions. print ( 'Dealing with Train...' ) # Create pytorch dataset. train_dataset = MovieReviewsTextDataset ( path = '/content/aclImdb/train' ) print ( f 'Created `train_dataset` with { len ( train_dataset ) } examples!' ) print () print ( 'Dealing with Validation...' ) # Create pytorch dataset. valid_dataset = MovieReviewsTextDataset ( path = '/content/aclImdb/test' ) print ( f 'Created `valid_dataset` with { len ( valid_dataset ) } examples!' ) Dealing with Train... pos Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [01:22<00:00, 151.34it/s] neg Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [01:10<00:00, 178.52it/s] Created `train_dataset` with 25000 examples! Dealing with Validation... pos Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [01:22<00:00, 151.34it/s] neg Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [01:10<00:00, 178.52it/s] Created `valid_dataset` with 25000 examples! PyTorch DataLoader In order to group examples from the PyTorch Dataset into batches we use PyTorch DataLoader. This is standard when using PyTorch. # Move pytorch dataset into dataloader. torch_train_dataloader = DataLoader ( train_dataset , batch_size = train_batch_size , shuffle = True ) print ( f 'Created `torch_train_dataloader` with { len ( torch_train_dataloader ) } batches!' ) # Move pytorch dataset into dataloader. torch_valid_dataloader = DataLoader ( valid_dataset , batch_size = valid_batch_size , shuffle = False ) print ( f 'Created `torch_valid_dataloader` with { len ( torch_valid_dataloader ) } batches!' ) Created `torch_train_dataloader` with 2500 batches! Created `torch_valid_dataloader` with 1250 batches! PyTorchText Bucket Iterator Dataloader Here is where the magic happens! We pass in the train_dataset and valid_dataset PyTorch Dataset splits into BucketIterator to create the actual batches. It's very nice that PyTorchText can handle splits! No need to write same line of code again for train and validation split. The sort_key parameter is very important! It is used to order text sequences in batches. Since we want to batch sequences of text with similar length, we will use a simple function that returns the length of an data example ( len(x['text') ). This function needs to follow the format of the PyTorch Dataset we created in order to return the length of an example, in my case I return a dictionary with text key for an example. It is important to keep sort=False and sort_with_batch=True to only sort the examples in each batch and not the examples in the whole dataset! Find more details in the PyTorchText BucketIterator documentation here - look at the BPTTIterator because it has same parameters except the bptt_len argument. Note: If you want just a single DataLoader use torchtext.data.BucketIterator instead of torchtext.data.BucketIterator.splits and make sure to provide just one PyTorch Dataset instead of tuple of PyTorch Datasets and change the parameter batch_sizes and its tuple values to batch_size with single value: dataloader = torchtext.data.BucketIterator(dataset, batch_size=batch_size, ) # Group similar length text sequences together in batches. torchtext_train_dataloader , torchtext_valid_dataloader = torchtext . data . BucketIterator . splits ( # Datasets for iterator to draw data from ( train_dataset , valid_dataset ), # Tuple of train and validation batch sizes. batch_sizes = ( train_batch_size , valid_batch_size ), # Device to load batches on. device = device , # Function to use for sorting examples. sort_key = lambda x : len ( x [ 'text' ]), # Repeat the iterator for multiple epochs. repeat = True , # Sort all examples in data using `sort_key`. sort = False , # Shuffle data on each epoch run. shuffle = True , # Use `sort_key` to sort examples in each batch. sort_within_batch = True , ) # Print number of batches in each split. print ( 'Created `torchtext_train_dataloader` with %d batches!' % len ( torchtext_train_dataloader )) print ( 'Created `torchtext_valid_dataloader` with %d batches!' % len ( torchtext_valid_dataloader )) Created `torchtext_train_dataloader` with 2500 batches! Created `torchtext_valid_dataloader` with 1250 batches! Compare DataLoaders Let's compare the PyTorch DataLoader batches with the PyTorchText BucketIterator batches. We can see how nicely examples of similar length are grouped in same batch with PyTorchText. Note: When using the PyTorchText BucketIterator, make sure to call create_batches() before looping through each batch! Else you won't get any output form the iterator. # Loop through regular dataloader. print ( 'PyTorch DataLoader \\n ' ) for batch in torch_train_dataloader : # Let's check batch size. print ( 'Batch size: %d \\n ' % len ( batch [ 'text' ])) print ( 'LABEL \\t LENGTH \\t TEXT' . ljust ( 10 )) # Print each example. for text , label in zip ( batch [ 'text' ], batch [ 'label' ]): print ( ' %s \\t %d \\t %s ' . ljust ( 10 ) % ( label , len ( text ), text )) print ( ' \\n ' ) # Only look at first batch. Reuse this code in training models. break # Create batches - needs to be called before each loop. torchtext_train_dataloader . create_batches () # Loop through BucketIterator. print ( 'PyTorchText BuketIterator \\n ' ) for batch in torchtext_train_dataloader . batches : # Let's check batch size. print ( 'Batch size: %d \\n ' % len ( batch )) print ( 'LABEL \\t LENGTH \\t TEXT' . ljust ( 10 )) # Print each example. for example in batch : print ( ' %s \\t %d \\t %s ' . ljust ( 10 ) % ( example [ 'label' ], len ( example [ 'text' ]), example [ 'text' ])) print ( ' \\n ' ) # Only look at first batch. Reuse this code in training models. break PyTorch DataLoader Batch size: 10 LABEL LENGTH TEXT neg 811 Much as we all love Al Pacino, it was painful to see him in this movie. A publicity hack at the grubby ending of what seems to have once been a distinguished and idealistic career Pacino plays his part looking like an unmade bed and assaulting everyone with a totally bogus and inconsistent southern accent.<br /><br />The plot spools out this way and that with so many loose ends and improbabilities that the mind reels (and then retreats).<br /><br />Kim Basinger is there, not doing much. Her scenes with Pacino are flat and unconvincing. Hard to believe they meant a lot to each other. There's no energy there.<br /><br />Tea Leone, on the other hand, lit up the screen. She was electric and her scenes with Pacino were by far the most interesting in the movie, but not enough to save Al from embarrassment. neg 572 God, I am so sick of the crap that comes out of America called \"Cartoons\"!<br /><br />Since anime became popular, the USA animators either produce a cartoon with a 5-year-old-lazy-ass style of drawing (Kappa Mikey) or some cheep Japanese animation rip-off. (Usually messing up classic characters) No wonder anime is beating American cartoons! <br /><br />They are not even trying anymore! <br /><br />Oh, I just heard of this last night; I live in the UK and when I found out this show first came out in 2005,well, I never knew the UK was so up-to-date with current shows. pos 3122 First an explanation on what makes a great movie for me. Excitement about not knowing what is coming next will make me enjoy a movie the first time I watch it (case en point: Twister). There are also other things that go into a great first viewing such as good humor (John Candy in Uncle Buck and The Great Outdoors), good plot with good resolution (Madeline and Matilda), imaginative storytelling (all Star Wars episodes-George Lucas is THE MAN), and good music (again all Star Wars episodes, Wizard of Oz, Sound of Music). What makes me watch a movie at least six times in the theatre and buy a DVD or VHS tape? Characters. With that said, I present Cindy Lou Who and The Grinch. Excellent performance Taylor Momsen and Jim Carrey. The rest of the cast was very good, particularly Jeffery Tambor, Bill Irwin, Molly Shannon, Christine Baranski, and Josh Ryan Evans. But, every single scene with Cindy and The Grinch-together is excellent and very funny and/or heartwarming. Cindy Lou is my favorite character in this movie and the most compelling reason why the movie is better than the cartoon. The Grinch has a strong plot, good conflicts, and a very good theme (I can't get started because I don't want to spoil it). Jim Carrey was very funny as The Grinch-particularly when he interacted with Cindy. And the music! Wow! Excellent music by James Horner. I loved his selection of instruments and the compositions. Very good job Jim Carrey-I didn't know you could sing. Taylor Momsen! Whoa! Your voice is reason enough to see the movie at least once. On your solo - Where Are You Christmas - is your voice really as high as it sounds? Sounds like an F#? That is an obscene range for a 7-year old (obscene meant in the best possible way). Great job. This is the best performance by a child I have ever heard in a movie(Taylor beat out the Von Trapp Children-no small feat!). And now to the actors. Jim Carrey was great, funny, and, surprisingly very sensitive (this really showed through in his scenes with Taylor Momsen). Taylor Momsen's unspoken expressions(one of the secrets to a good acting performance) are very strong-she really becomes Cindy Lou Who. And when she does dialogue she is even stronger.<br /><br />******************************danger:spoiler alert********************* ***********************************************************************<br /><br />Examples: expression when she first sees The Grinch. This is a classic quote (\"You're the the the\" and then filled in with the Grinch line \"da da da THE GRINCH-after which she topples into the sorter and then is rescued by The Grinch). The \"Thanks for saving me\" quote and subsequent response by The Grinch was also very good.<br /><br />My favorite part of the movie is when Cindy invites The Grinch to be Holiday Cheermeister. This scene is two excellent actors at their best interacting and expressing with each other. Little Taylor Momsen completely holds her own with Jim Carrey in this spot. I sincerely hope we see Taylor Momsen in many more films to come. All in all everything was great about this movie (except maybe the feet and noses). pos 483 Red Rock West is one of those rare films that keeps you guessing the entire time as to what will happen next. Nicolas Cage is mistaken for a contract killer as he enters a small town trying to find work. Dennis Hopper is the bad guy and no one plays them better. Look for a brief appearance by country singing star Dwight Yoakam. This is a serious drama most of the time but there are some lighter moments. What matters is that you will enjoy this low budget but high quality effort! pos 759 This movie is a remake of two movies that were a lot better. The last one, Heaven Can Wait, was great, I suggest you see that one. This one is not so great. The last third of the movie is not so bad and Chris Rock starts to show some of the comic fun that got him to where he is today. However, I don't know what happened to the first two parts of this movie. It plays like some really bad \"B\" movie where people sound like they are in some bad TV sit-com. The situations are forced and it is like they are just trying to get the story over so they can start the real movie. It all seems real fake and the editing is just bad. I don't know how they could release this movie like that. Anyway, the last part isn't to bad, so wait for the video and see it then. pos 2471 VIVAH in my opinion is the best movie of 2006, coming from a director that has proved successful throughout his career. I am not too keen in romantic movies these days, because i see them as \"old wine in a new bottle\" and so predictable. However, i have watched this movie three times now...and believe me it's an awesome movie.<br /><br />VIVAH goes back to the traditional route, displaying simple characters into a sensible and realistic story of the journey between engagement and marriage. The movie entertains in all manners as it can be reflected to what we do (or would do) when it comes to marriage. In that sense Sooraj R. Barjatya has done his homework well and has depicted a very realistic story into a well-made highly entertaining movie.<br /><br />Several sequences in this movie catch your interest immediately: <br /><br />* When Shahid Kapoor comes to see the bride (Amrita Rao) - the way he tries to look at her without making it too obvious in front of his and her family. The song 'Do Anjaane Ajnabi' goes well with the mood of this scene.<br /><br />* The first conversation between Shahid and Amrita, when he comes to see her - i.e. a shy Shahid not knowing exactly what to talk about but pulling of a decent conversation. Also Amrita's naive nature, limited eye-contact, shy characteristics and answering softly to Shahid's questions.<br /><br />* The emotional breakdown of Amrita and her uncle (Alok Nath) when she feeds him at Shahid's party in the form of another's daughter-in-law rather than her uncle's beloved niece.<br /><br />Clearly the movie belongs to Amrita Rao all the way. The actress portrays the role of Poonam with such conviction that you cannot imagine anybody else replacing her. She looks beautiful throughout the whole movie, and portrays an innocent and shy traditional girl perfectly.<br /><br />Shahid Kapoor performs brilliantly too. He delivers a promising performance and shows that he is no less than Salman Khan when it comes to acting in a Sooraj R. Barjatya film. In fact Shahid and Amrita make a cute on-screen couple, without a shadow of doubt. Other characters - Alok Nath (Excellent), Anupam Kher (Brilliant), Mohan Joshi (Very good).<br /><br />On the whole, VIVAH delivers what it promised, a well made and realistic story of two families. The movie has top-notch performances, excellent story and great music to suit the film, as well as being directed by the fabulous Sooraj R. Barjatya. It's a must see! neg 626 Watching this Movie? l thought to myself, what a lot of garbage. These girls must have rocks for brains for even agreeing to be part of it. Waste of time watching it, faint heavens l only hired it. The acting was below standard and story was unbearable. Anyone contemplating watching this film, please save your money. The film has no credit at all. l am a real film buff and this is worse than \"Attack of the Green Tomatoes\".<br /><br />l only hope that this piece of trash didn't cost too much to make. Money would have been better spent on the homeless people of the world. l only hope there isn't a sequel in the pipeline. pos 2599 A SPECIAL DAY (Ettore Scola - Italy/Canada 1977).<br /><br />Every once in a while, you come across a film that really touches a nerve. This one offers a very simple premise, almost flawlessly executed in every way and incredibly moving at the same time. It's surprising Ettore Scola's \"Una giornate particulare\" is relatively unheralded, even hated by some critics. Time Out calls it 'rubbish' and Leonard Maltin, somewhat milder, 'pleasant but trifling.' I disagree, not only because this film is deeply moving, but within its simple story it shows us more insights about daily life in fascist Italy than most films I've seen. The cinematography is distinctly unflashy, even a bit bland, and the storyline straightforward, which might explain the film's relative unpopularity. Considering late '70s audiences weren't exactly spoiled with great Italian films, it's even stranger this one didn't really catch on with the critics.<br /><br />The film begins with a ten-minute collage of archive footage from Hitler's visit to Italy on may 8th 1938. Set against this background, we first meet Antonietta (Loren), a lonely, love-ridden housewife with six children in a roman apartment building. One day, when her Beo escapes, she meets her neighbour Gabriele (Mastroianni), who seems to be only one in the building not attending the ceremonies. He is well-mannered, cultured and soon she is attracted to him. During the whole film, we hear the fascist rally from the radio of the concierge hollering through the courtyard. Scola playfully uses the camera to make us part of the proceedings. After the opening scene, the camera swanks across the courtyard of the modernist (hypermodern at the time) apartment block, seemingly searching for our main characters, whom we haven't met yet. <br /><br />Marcello Mastrionani and Sophia Loren are unforgettable in the two leading roles, all the more astonishing since they are cast completely against type. Canadian born John Vernon plays Loren's husband, but he is only on screen in the first and last scene. I figure his voice must have been dubbed, since he's not of Italian descent and never lived there, to my knowledge, so I cannot imagine he speaks Italian. If his voice has been dubbed, I didn't notice at all. On the contrary, he's completely believable as an Italian, even more than the rest of the cast. The story is simple but extremely effective, the performances are outstanding, the ending is just perfect and the framing doesn't come off as overly pretentious but works completely. Don't miss out on this one.<br /><br />Camera Obscura --- 9/10 neg 1482 There are some extremely talented black directors Spike Lee,Carl Franklin,Billy Dukes,Denzel and a host of others who bring well deserved credit to the film industry . Then there are the Wayans Brothers who at one time(15,years ago) had an extremely funny television show'In Living Colour' that launched the career of Jim Carrey amongst others . Now we have stupidity substituting for humour and gross out gags(toilet humour) as the standard operating procedure . People are not as stupid as those portrayed in 'Little Man' they couldn't possibly be . A baby with a full set of teeth and a tattoo is accepted as being only months old ? Baby comes with a five o'clock shadow that he shaves off . It is intimated that the baby has sex with his foster mother behind her husbands,Darryl's, back .Oh, yea that is just hilarious . As a master criminal 'Little Man' is the stupidest on planet earth . He stashes a stolen rock that is just huge in a woman's purse and then has to pursue her . Co-star Chazz Palminteri,why Chazz, offers the best line: \"I'm surrounded by morons.\" Based, without credit, on a Chuck Jones cartoon, Baby Buggy Bunny . This is far too stupid to be even remotely funny . A clue as to how bad this film is Damon Wayans appeared on Jay Leno the other night,prior to the BAT awards and he did not,even mention this dreadful movie . When will Hollywood stop green lighting trash from the Wayans Brothers . When they get over their white mans guilt in all likelihood . neg 4380 There is a bit of a spoiler below, which could ruin the surprise of the ONE unexpected and truly funny scene in this film. There is also information about the first film in this series.<br /><br />I caught this film on DVD, which someone gave as a gift to my roommate. It came as a set together with the first film in the \"Blind Dead\" series.<br /><br />This movie was certainly much worse than the first, \"La Noche del Terror Ciego\". In addition, many of the features of the first movie were changed significantly. To boot, the movie was dubbed in English (the first was subtitled), which I tend to find distracting.<br /><br />The concept behind the series is that in the distant past a local branch of the Knights Templar was involved in heinous and secret rituals. Upon discovery of these crimes, the local peasantry put the Templars to death in such a manner that their eyes can no longer be used, thus preventing them from returning from Hell to exact their revenge. We then jump to modern times where because of some event, the Templars arise from the dead to exact their revenge upon the villagers whose ancestors messed them up in the first place. Of course, since the undead knights have no eyes, they can only find their victims when they make some sort of noise.<br /><br />The Templars were a secretive order, from about the 12th century, coming out of the Crusades. They were only around for about 150 years, before they were suppressed in the early 1300s by the Pope and others. Because they were secretive, there were always rumors about their ceremonies, particularly for initiation. Also, because of the way the society was organized, you didn't necessarily have church officials overseeing things, which meant they didn't have an inside man when things heated up. And, because of the nature of their trials, they were tortured into confessions. The order was strongest in France, but did exist in Portugal and Spain, where the movies take place.<br /><br />Where the first movie had a virgin sacrifice and knights drinking the blood directly from the body of the virgin (breast shots here, of course, this is a horror film after all), and then, once the knights come back to life, they attack their victims by eating them alive and sucking their blood; in this sequel, this all disappears. You still have the same scene (redone, not the same footage) of them sacrificing the virgin, but they drain the blood into a bowl and drink it from that. Thus, when they come back, they just hack people up with their swords or claw people to death, which I have to say is a much less effective means of disturbing your audience. There's also a time problem: in the first film the dating is much closer to the Templars, where here they are now saying it is the 500 anniversary of the peasants burning these guys at the stake, which would date it around 1473. And the way that the Templars lose their eyes is much less interesting as well. In the first, they have them pecked out by crows. Now they are simply burned out, and in quite a ridiculous manner.<br /><br />Oh yeah, and maybe it was just me, but there seemed to be a lot of people from the first movie reappearing in this film (despite having died). Not really a problem, since the movie is completely different and not a sequel in the sense of a continuation, but odd none-the-less.<br /><br />The highlight of this movie is the rich fellow who uses a child to distract the undead while he makes a break for the jeep. The child's father had already been suckered by this rich man into making an attempt to get the jeep, so he walks out and tells her to find her father. It comes somewhat out of the blue, and is easily the funniest scene in the film. Of course, why the child doesn't die at this point is beyond me, and disappointed for horror fans.<br /><br />I couldn't possibly recommend this film to anyone. It isn't so bad that it becomes funny, so it just ends up being a mediocre horror film. The bulk of the film has several people holed up in a church, each making various attempts to go it alone in order to escape the blind dead who have them surrounded. When the film ends, you are not surprised at the outcome at all; in fact, quite disappointed. If you are into the novelty of seeing a Spanish horror film, see the first movie, which at least has some innovative ideas and not so expected outcomes. PyTorchText BuketIterator Batch size: 10 LABEL LENGTH TEXT neg 1118 Most college students find themselves lost in the bubble of academia, cut off from the communities in which they study and live. Their conversations are held with their fellow students and the college faculty. Steven Greenstreet's documentary is a prime example of a disillusioned college student who judges the entire community based on limited contact with a small number of its members.<br /><br />The documentary focused on a small group of individuals who were portrayed as representing large groups of the population. As is usual, the people who scream the most get the most media attention. Other than its misrepresentation of the community in which the film was set, the documentary was well made. My only dispute is that the feelings and uproar depicted in the film were attributed to the entire community rather than the few individuals who expressed them.<br /><br />Naturally it is important to examine a controversy like this and make people aware of the differences that exist between political viewpoints, but it is ridiculous to implicate an entire community of people in the actions of a few radicals. neg 1120 Looked forward to viewing this film and seeing these great actors perform. However, I was sadly disappointed in the script and the entire plot of the story. David Duchovny,(Dr. Eugene Sands),\"Connie & Carla\",'04, was the doctor in the story who uses drugs and losses his license to practice medicine. Dr. Sands was visiting a night club and was able to use his medical experience to help a wounded customer and was assisted by Angelina Jolie,(Claire),\"Taking Lives\",'04, who immediately becomes attracted to Dr. David Sands. Timothy Hutton,(Raymond Blossom),\"Kinsey\",'04, plays the Big Shot Gangster and a man with all kinds of money and connections. Timothy Hutton seems to over act in most of the scenes and goes completely out of his mind trying to keep his gang members from being killed. Gary Dourdan,(Yates),\"CSI-Vegas TV Series\", plays a great supporting role and portrays a real COOL DUDE who is a so-called body guard for Raymond Blossom. Angelina Jolie looks beautiful and sexy with her ruby red lips which draws a great deal of attention from all the men. This film is not the greatest, but it does entertain. pos 1120 I must say that, looking at Hamlet from the perspective of a student, Brannagh's version of Hamlet is by far the best. His dedication to stay true to the original text should be applauded. It helps the play come to life on screen, and makes it easier for people holding the text while watching, as we did while studying it, to follow and analyze the text.<br /><br />One of the things I have heard criticized many times is the casting of major Hollywood names in the play. I find that this helps viewers recognize the characters easier, as opposed to having actors that all look and sound the same that aid in the confusion normally associated with Shakespeare.<br /><br />Also, his flashbacks help to clear up many ambiguities in the text. Such as how far the relationship between Hamlet and Ophelia really went and why Fortinbras just happened to be at the castle at the end. All in all, not only does this version contain some brilliant performances by actors both familiar and not familiar with Shakespeare. It is presented in a way that one does not have to be an English Literature Ph.D to understand and enjoy it. pos 1120 As a baseball die-hard, this movie goes contrary to what I expect in a sports movie: authentic-looking sports action, believable characters, and an original story line. While \"Angels in the Outfield\" fails miserably in the first category, it succeeds beautifully in the latter two. \"Angels\" weaves the story of Roger and J.P., two Anaheim foster kids in love with baseball but searching for a family, with that of the woebegone Angels franchise, struggling to draw fans and win games. Pushed by his deadbeat father's promise that they would be a family only when the Angels win the pennant, Roger asks for some heavenly help, and gets it in the form of diamond-dwelling spirits bent on reversing the franchise's downward spiral. And, when short-fused manager George Knox (portrayed by Danny Glover) begins believing in what Roger sees, the team suddenly has hope for turning their season around--and Roger and J.P. find something to believe in. Glover in particular gives a nice performance, and Tony Danza, playing a washed-up pitcher, also does well, despite clearly having ZERO idea of how to pitch out of the windup! neg 1121 I have a piece of advice for the people who made this movie too, if you're gonna make a movie like this be sure you got the f/x to back it up. Also don't get a bunch of z list actors to play in it. Another thing, just about all of us have seen Jurassic Park, so don't blatantly copy it. All in all this movie sucked, f/x sucked, acting sucked, story unoriginal. Let's talk about the acting for just a second, the Carradine guy who's career peaked in 1984 when he did \"Revenge of the Nerds\" (which was actually a great comedy). He's not exactly z list, he can act. He just should have said no to this s--t bag. He should have did what Mark Hamill did after \"Return of the Jedi\" and go quietly into the night. He made his mark as a \"Nerd\" and that should have been that. I understand he has bills to pay, but that hardly excuses this s--t bag. Have I called this movie that yet? O.K. I just wanted to be sure. If I sound a little hostile, I apologize. I just wasted 2hrs of my life I could have spent doing something productive like watching paint peel, and I feel cheated. I'll close on that note. Thank you for your time. neg 1121 By 1941 Columbia was a full-fledged major studio and could produce a movie with the same technical polish as MGM, Paramount or Warners. That's the best thing that could be said about \"Adam Had Four Sons,\" a leaden soap opera with almost terminally bland performances by Ingrid Bergman (top-billed for the first time in an American film) and Warner Baxter. Bergman plays a Frenchwoman (this was the era in which Hollywood thought one foreign accent was as good as another) hired as governess to Baxter's four sons and staying on (with one interruption caused by the stock-market crash of 1907) until the boys are grown men serving in World War I. Just about everyone in the movie is so goody-good it's a relief when Susan Hayward as the villainess enters midway through \u2014 she's about the only watchable person in the movie even though she's clearly channeling Bette Davis and Vivien Leigh; it's also the first in her long succession of alcoholic roles \u2014 but the script remains saccharine and the ending is utterly preposterous. No wonder Bergman turned down the similarly plotted \"The Valley of Decision\" four years later. neg 1123 I have never read the book\"A wrinkle in time\". To be perfectly honesty, after seeing the movie, do I really want to? Well, I shouldn't be reviewing this movie i'll start off with that. Next i'll say that the TV movie is pretty forgettable. Do you know why I say that? Because I forgot what happens in it. I told you it was forgettable. To be perfectly honest, no TV movie will ever be better than \"Merlin\".<br /><br />How do I describe a TV movie? I have never written a review for one before. Well, i'll just say that they usually have some celebrities. A wrinkle in time includes only one. Alfre Woodard(Or Woodward, I am not sure), the Oscar winner. <br /><br />The film has cheesy special effects, a mildly interesting plot, scenes that make you go \"WTF\". The movie is incredibly bad and it makes you go\"WTF\". What did I expect? It's a TV movie. They usually aren't good. As is this one. A wrinkle in time is a waste of time and a big time waster. To top it off, you'll most likely forget about it the second it's over. Well, maybe not the second it's over. But within a few minutes.<br /><br />A wrinkle in time:*/**** neg 1123 After watching \"The Bodyguard\" last night, I felt compelled to write a review of it.<br /><br />This could have been a pretty decent movie had it not been for the awful camera-work. It was beyond annoying. The angles were all wrong, it was impossible to see anything, especially during the fight sequences. The closeups were even horrible.<br /><br />The story has Sonny Chiba hiring himself out as a bodyguard to anyone willing to lead him to the top of a drug ring. He is approached by Judy Lee, who is never quite straight with Chiba. Lee's involvement in the drug ring is deeper than Chiba thought, as the Mob and another gang of thugs are after her.<br /><br />The story was decent, and despite horrible dubbing, this could have been a good movie. Given better direction and editing, I'm sure this would have been a classic Kung Foo movie. As it is, it's more like another cheesy 70's action movie.<br /><br />Note: The opening sequence has a quote familiar to \"Pulp Fiction\" fans, and then continues to a karate school in Times Square that is in no way related to the rest of the movie.<br /><br />Rating: 4 out of 10 neg 1123 There are some really terrific ideas in this violent movie that, if executed clearly, could have elevated it from Spaghetti-western blandness into something special. Unfortunately, A TOWN CALLED HELL is one of the worst edited movies imaginable! Scenes start and end abruptly, characters leave for long stretches, the performances (and accents) of the actors are pretty inconsistent, etc.<br /><br />Robert Shaw is a Mexican(!) revolutionary who, after taking part in wiping out a village, stays on to become a priest(!)...ten years later the village is being run by \"mayor\" Telly Salavas. Stella Stevens arrives looking for revenge on the man who killed her husband. Colonel Martin Landau arrives looking for Shaw. They all yell at each other A LOT and they all shoot each other A LOT. Fernando Rey is in it too (as a blind man). The performances aren't bad, but they are mightily uneven. Savalas has an accent sometimes as does Landau (who is really grating here). Shaw and Rey prove that they are incapable of really embarrassing themselves and Stevens looks pretty foxy (if a bit out of place amongst the sweaty filth). neg 1124 The movie is plain bad. Simply awful. The string of bad movies from Bollywood has no end! They must be running out of excuses for making such awful movies (or not).<br /><br />The problem seems to be with mainly the directors. This movie has 2 good actors who have proved in the past that the have the ability to deliver great performance...but they were directed so poorly. The poor script did not help either.<br /><br />This movie has plenty of ridiculous moments and very bad editing in the first half. For instance :<br /><br />After his 1st big concert, Ajay Devgan, meets up with Om Puri (from whom he ran away some 30 years ago and talked to again) and all Om Puri finds to say is to beware of his friendship with Salman!!! What a load of crap. Seriously. Not to mention the baaad soundtrack. Whatever happened to Shankar Ehsaan Loy?<br /><br />Ajay Devgun is total miscast for portraying a rockstar.<br /><br />Only saving grace are the good performances in the second half. Ajay shines as his character shows his dark side. So does Salman as the drug addict. <br /><br />Watch it maybe only for the last half hour. Train Loop Examples Now let's look at a model training loop would look like. I printed the first 10 batches list of examples lengths to show how nicely they are grouped throughout the dataset! # Example of number of epochs epochs = 1 # Example of loop through each epoch for epoch in range ( epochs ): # Create batches - needs to be called before each loop. torchtext_train_dataloader . create_batches () # Loop through BucketIterator. for sample_id , batch in enumerate ( torchtext_train_dataloader . batches ): print ( 'Batch examples lengths: %s ' . ljust ( 20 ) % str ([ len ( example [ 'text' ]) for example in batch ])) # Let's break early, you get the idea. if sample_id == 10 : break Batch examples lengths: [791, 792, 792, 793, 797, 797, 799, 799, 801, 801] Batch examples lengths: [4823, 4832, 4859, 4895, 4944, 5025, 5150, 5309, 5313, 5450] Batch examples lengths: [695, 696, 696, 696, 697, 699, 699, 700, 700, 701] Batch examples lengths: [960, 961, 963, 963, 963, 966, 966, 967, 968, 969] Batch examples lengths: [1204, 1205, 1208, 1209, 1212, 1214, 1218, 1221, 1226, 1229] Batch examples lengths: [2639, 2651, 2651, 2672, 2692, 2704, 2707, 2712, 2720, 2724] Batch examples lengths: [1815, 1830, 1835, 1838, 1841, 1849, 1852, 1878, 1889, 1895] Batch examples lengths: [3111, 3115, 3133, 3174, 3201, 3206, 3217, 3278, 3294, 3334] Batch examples lengths: [3001, 3031, 3039, 3047, 3056, 3077, 3084, 3103, 3104, 3107] Batch examples lengths: [1053, 1053, 1056, 1057, 1060, 1067, 1073, 1077, 1078, 1080] Batch examples lengths: [751, 751, 756, 758, 759, 760, 761, 762, 763, 764] Using PyTorchText TabularDataset Now I will use the TabularDataset functionality which creates the PyTorchDataset object right from our local files. We don't need to create a custom PyTorch Dataset class to load our dataset as long as we have tabular files of our data. Data to Files Since our dataset is scattered into multiple files, I created a function files_to_tsv which puts our dataset into a .tsv file (Tab-Separated Values). Since I'll use the TabularDataset from pytorch.data I need to pass tabular format files. For text data I find the Tab Separated Values format easier to deal with. I will call the files_to_tsv function for each of the two partitions train and test . The function will return the name of the .tsv file saved so we can use it later in PyTorchText. def files_to_tsv ( partition_path , save_path = './' ): \"\"\"Parse each file in partition and keep track of sentiments. Create a list of pairs [tag, text] Arguments: partition_path (:obj:`str`): Partition used: train or test. save_path (:obj:`str`): Path where to save the final .tsv file. Returns: :obj:`str`: Filename of created .tsv file. \"\"\" # List of all examples in format [tag, text]. examples = [] # Print partition. print ( partition_path ) # Loop through each sentiment. for sentiment in [ 'pos' , 'neg' ]: # Find path for sentiment. sentiment_path = os . path . join ( partition_path , sentiment ) # Get all files from path sentiment. files_names = os . listdir ( sentiment_path ) # For each file in path sentiment. for file_name in tqdm ( files_names , desc = f ' { sentiment } Files' ): # Get file content. file_content = io . open ( os . path . join ( sentiment_path , file_name ), mode = 'r' , encoding = 'utf-8' ) . read () # Fix any format errors. file_content = fix_text ( file_content ) # Append sentiment and file content. examples . append ([ sentiment , file_content ]) # Create a TSV file with same format `sentiment text`. examples = [ \" %s \\t %s \" % ( example [ 0 ], example [ 1 ]) for example in examples ] # Create file name. tsv_filename = os . path . basename ( partition_path ) + '_pos_neg_ %d .tsv' % len ( examples ) # Write to TSV file. io . open ( os . path . join ( save_path , tsv_filename ), mode = 'w' , encoding = 'utf-8' ) . write ( ' \\n ' . join ( examples )) # Return TSV file name. return tsv_filename # Path where to save tsv file. data_path = '/content' # Convert train files to tsv file. train_filename = files_to_tsv ( partition_path = '/content/aclImdb/train' , save_path = data_path ) # Convert test files to tsv file. test_filename = files_to_tsv ( partition_path = '/content/aclImdb/test' , save_path = data_path ) /content/aclImdb/train pos Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [00:34<00:00, 367.26it/s] neg Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [00:21<00:00, 573.00it/s] /content/aclImdb/test pos Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [00:11<00:00, 1075.80it/s] neg Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [00:12<00:00, 1037.94it/s] TabularDataset Here I setup the data fields for PyTorchText. We have to tell the library how to handle each column of the .tsv file. For this we need to create data.Field objects for each column. text_tokenizer : For this example I don't use an actual tokenizer for the text column but I need to create one because it requires as input. I created a dummy tokenizer that returns same value. Depending on the project, here is where you will have your own tokenizer. It needs to take as input text and output a list. label_tokenizer The label tokenizer is also a dummy tokenizer. This is where you will have a encoder to transform labels to ids. Since we have two .tsv files it's great that we can use the .split function from TabularDataset to handle two files at the same time one for train and the other one for test. Find more details about torchtext.data functionality here . # Text tokenizer function - dummy tokenizer to return same text. # Here you will use your own tokenizer. text_tokenizer = lambda x : x # Label tokenizer - dummy label encoder that returns same label. # Here you will add your own label encoder. label_tokenizer = lambda x : x # Data field for text column - invoke tokenizer. TEXT = torchtext . data . Field ( sequential = True , tokenize = text_tokenizer , lower = False ) # Data field for labels - invoke tokenize label encoder. LABEL = torchtext . data . Field ( sequential = True , tokenize = label_tokenizer , use_vocab = False ) # Create data fields as tuples of description variable and data field. datafields = [( \"label\" , LABEL ), ( \"text\" , TEXT )] # Since we have have tab separated data we use TabularDataset train_dataset , valid_dataset = torchtext . data . TabularDataset . splits ( # Path to train and validation. path = data_path , # Train data filename. train = train_filename , # Validation file name. validation = test_filename , # Format of local files. format = 'tsv' , # Check if we have header. skip_header = False , # How to handle fields. fields = datafields ) PyTorchText Bucket Iterator Dataloader I'm using same setup as in the PyTorchText Bucket Iterator Dataloader code cell section. The only difference is in the sort_key since there is different way to access example attributes (we had dictionary format before). # Group similar length text sequences together in batches. torchtext_train_dataloader , torchtext_valid_dataloader = torchtext . data . BucketIterator . splits ( # Datasets for iterator to draw data from ( train_dataset , valid_dataset ), # Tuple of train and validation batch sizes. batch_sizes = ( train_batch_size , valid_batch_size ), # Device to load batches on. device = device , # Function to use for sorting examples. sort_key = lambda x : len ( x . text ), # Repeat the iterator for multiple epochs. repeat = True , # Sort all examples in data using `sort_key`. sort = False , # Shuffle data on each epoch run. shuffle = True , # Use `sort_key` to sort examples in each batch. sort_within_batch = True , ) # Print number of batches in each split. print ( 'Created `torchtext_train_dataloader` with %d batches!' % len ( torchtext_train_dataloader )) print ( 'Created `torchtext_valid_dataloader` with %d batches!' % len ( torchtext_valid_dataloader )) Created `torchtext_train_dataloader` with 2500 batches! Created `torchtext_valid_dataloader` with 1250 batches! Compare DataLoaders Let's compare the PyTorch DataLoader batches with the PyTorchText BucketIterator batches created with TabularDataset. We can see how nicely examples of similar length are grouped in same batch with PyTorchText. Note: When using the PyTorchText BucketIterator, make sure to call create_batches() before looping through each batch! Else you won't get any output form the iterator. # Loop through regular dataloader. print ( 'PyTorch DataLoader \\n ' ) for batch in torch_train_dataloader : # Let's check batch size. print ( 'Batch size: %d \\n ' % len ( batch [ 'text' ])) print ( 'LABEL \\t LENGTH \\t TEXT' . ljust ( 10 )) # Print each example. for text , label in zip ( batch [ 'text' ], batch [ 'label' ]): print ( ' %s \\t %d \\t %s ' . ljust ( 10 ) % ( label , len ( text ), text )) print ( ' \\n ' ) # Only look at first batch. Reuse this code in training models. break # Create batches - needs to be called before each loop. torchtext_train_dataloader . create_batches () # Loop through BucketIterator. print ( 'PyTorchText BuketIterator \\n ' ) for batch in torchtext_train_dataloader . batches : # Let's check batch size. print ( 'Batch size: %d \\n ' % len ( batch )) print ( 'LABEL \\t LENGTH \\t TEXT' . ljust ( 10 )) # Print each example. for example in batch : print ( ' %s \\t %d \\t %s ' . ljust ( 10 ) % ( example . label , len ( example . text ), example . text )) print ( ' \\n ' ) # Only look at first batch. Reuse this code in training models. break PyTorch DataLoader Batch size: 10 LABEL LENGTH TEXT pos 1742 As a child I preferred the first Care Bear movie since this one seemed so dark. I always sat down and watched the first one. As I got older I learned to prefer this one. What I do think is that this film is too dark for infants, but as you get older you learn to treasure it since you understand it more, it doesn't seem as dark as it was back when you were a child.<br /><br />This movie, in my opinion, is better than the first one, everything is so much deeper. It may contradict the first movie but you must ignore the first movie to watch this one. The cubs are just too adorable, I rewind that 'Flying My Colors' scene. I tend to annoy everyone by singing it.<br /><br />The sound track is great! A big hand to Carol and Dean Parks. I love every song in this movie, I have downloaded them all and is all I am listening to, I'm listening to 'Our beginning' also known as 'Recalling' at the moment. I have always preferred this sound track to the first one, although I just totally love Carol Kings song in the first movie 'Care-A-Lot'.<br /><br />I think the animation is great, the animation in both movies are fantastic. I was surprised when I sat down and watched it about 10 years later and saw that the animation for the time was excellent. It was really surprising.<br /><br />There is not a lot of back up from other people to say that this movie is great, but it is. I do not think it is weird/strange. I think it is a wonderful movie.<br /><br />Basically, this movie is about how the Care Bears came about and to defeat the Demon, Dark Heart. The end is surprising and again, beats any 'Pokemon Movie' with the Care Bears Moral issues. It leaves an effect on you. Again this movie can teach everyone at all ages about morality. pos 1475 Worry not, Disney fans--this special edition DVD of the beloved Cinderella won't turn into a pumpkin at the strike of midnight. One of the most enduring animated films of all time, the Disney-fide adaptation of the gory Brothers Grimm fairy tale became a classic in its own right, thanks to some memorable tunes (including \"A Dream Is a Wish Your Heart Makes,\" \"Bibbidi-Bobbidi-Boo,\" and the title song) and some endearingly cute comic relief. The famous slipper (click for larger image) We all know the story--the wicked stepmother and stepsisters simply won't have it, this uppity Cinderella thinking she's going to a ball designed to find the handsome prince an appropriate sweetheart, but perseverance, animal buddies, and a well-timed entrance by a fairy godmother make sure things turn out all right. There are a few striking sequences of pure animation--for example, Cinderella is reflected in bubbles drifting through the air--and the design is rich and evocative throughout. It's a simple story padded here agreeably with comic business, particularly Cinderella's rodent pals (dressed up conspicuously like the dwarf sidekicks of another famous Disney heroine) and their misadventures with a wretched cat named Lucifer. There's also much harrumphing and exposition spouting by the King and the Grand Duke. It's a much simpler and more graceful work than the more frenetically paced animated films of today, which makes it simultaneously quaint and highly gratifying. pos 1279 Seldom do I ever encounter a film so completely fulfilling that I must speak about it immediately. This movie is definitely some of the finest entertainment available and it is highly authentic. I happened to see the dubbed version but I'm on my way right now to grab the DVD remaster with original Chinese dialogue. Still, the dubbing didn't get in the way and sometimes provided some seriously funny humour: \"Poison Clan rocks the world!!!\"<br /><br />The story-telling stays true to Chinese methods of intrigue, suspense, and inter-personal relationships. You can expect twists and turns as the identities of the 5 venoms are revealed and an expert pace.<br /><br />The martial arts fight choreography is in a class of its own and must be seen to be believed. It's like watching real animals fight each other, but construed from their own arcane martial arts forms. Such level of skill amongst the cast is unsurpassed in modern day cinema.<br /><br />The combination provides for a serious dose of old Chinese culture and I recommend it solely on the basis of the film's genuine intent to tell a martial arts story and the mastery of its execution. ...Of course, if you just want to see people pummel each other, along with crude forms of ancient Chinese torture, be my guest! pos 1071 I'm sure that most people already know the story-the miserly Ebenezer Scrooge gets a visit from three spirits (the Ghosts of Christmas Past, Present and Yet to Come) who highlight parts of his life in the hopes of saving his soul and changing his ways. Dickens' classic story in one form or another has stood the test of time to become a beloved holiday favorite.<br /><br />While I grew up watching the 1951 version starring Alastair Sims, and I believe that he is the definitive Scrooge, I have been impressed with this version, which was released when I was in high school. George C. Scott plays a convincing and mean Ebenezer Scrooge, and the actors playing the ghosts are rather frightening and menacing. David Warner is a good Bob Cratchit as well.<br /><br />This version is beautifully filmed, and uses more modern filming styles (for the 1980's) which make it more palatable for my children than the 1951 black and white version.<br /><br />This is a worthy adaptation of the story and is one that I watch almost every year at some point in the Christmas season. neg 876 What was an exciting and fairly original series by Fox has degraded down to meandering tripe. During the first season, Dark Angel was on my weekly \"must see\" list, and not just because of Jessica Alba.<br /><br />Unfortunately, the powers-that-be over at Fox decided that they needed to \"fine-tune\" the plotline. Within 3 episodes of the season opener, they had totally lost me as a viewer (not even to see Jessica Alba!). I found the new characters that were added in the second season to be too ridiculous and amateurish. The new plotlines were stretching the continuity and credibility of the show too thin. On one of the second season episodes, they even had Max sleeping and dreaming - where the first season stated she biologically couldn't sleep.<br /><br />The moral of the story (the one that Hollywood never gets): If it works, don't screw with it!<br /><br />azjazz pos 1981 Greta Garbo's American film debut is an analogy of how our lives can be swept off course by fate and our actions, as in a torrent, causing us to lose a part of ourselves along the way.<br /><br />Greta plays Leonora, a poor peasant girl in love with Ricardo Cortez's character Don Rafael, a landowner. Ricardo is in love with her too, but is too easily influenced by his domineering mother. Leonora ends up homeless and travels to Paris, where she becomes a famous opera singer and develops the reputation for being a loose woman. In reality, part of her attitude is bitterness over Rafael's abandonment.<br /><br />She returns to her home to visit her family and eventually confronts Rafael. Surprisingly, no one knows that she's the famous La Brunna, and Garbo acts up her role as the diva she truly was and re prised with such cool haughtiness in her later portrayals.<br /><br />Ricardo Cortez reminds one a lot of Valentino in looks in this part, and he was groomed to be a Valentino clone by MGM, though he never thought he could be in reality and he was right. He is believable in an unsympathetic part as a weak willed Mama's boy, and allows himself to age realistically but comically at the end of the movie. He fails to win Leonora when she returns home, and later when he follows her, his courage is undermined.<br /><br />This movie is beautifully shot, with brilliant storm sequences and the sets depicting Spain at the time are authentic looking. There are also some fine secondary performances by old timers Lucien Littlefield, Tully Marshall, and Mack Swain.<br /><br />Although this is a story of lost love and missed chances, I don't think Leonora and Rafael would have been happy together, as he needed a more traditional wife and she was very much a career woman, and I don't think would have been happy in a small village. The ending is true to life and pulls no punches.<br /><br />See this one as Garbo's American film debut and a precursor of things to come pos 1007 *What I Like About SPOILERS* Teenager Holly Tyler (Amanda Bynes) goes to live with older sister Valerie (Jennie Garth) to avoid moving to Japan with her father; but she doesn't know the half of the wacky things that will happen to her from now on, and not only to her, but to her sister, her friends Gary (Wesley Jonathan) and Tina (Alison Munn), boyfriend Henry (Michael McMillian), crush Vince (Nick Zano), Valerie's boyfriend Jeff (Simon Rex), first boss (then firefighter then husband) Vic (Dan Cortese), annoying colleague Lauren (Leslie Grossman) and second boss Peter (?) If you don't have a funny bone in your body, please skip this; if you like only veeeery sophisticated comedy this isn't for you; if you like a funny, sometimes touching show with two hot chicks who can act in the lead (and none other than the fabulous 'Mary Cherry' from Popular - Leslie Grossman - in the main cast), then what the hell are you waiting for? You're welcome to Casa De Tyler! What I Like About You (2002-2006): 8. pos 318 This movie is wonderful. The writing, directing, acting all are fantastic. Very witty and clever script. Quality performances by actors, Ally Sheedy is strong and dynamic and delightfully quirky. Really original and heart-warmingly unpredicatable. The scenes are alive with fresh energy and really talented production. pos 1846 In Le Million, Rene Clair, one of the cinema's great directors and great pioneers, created a gem of light comedy which for all its lightness is a groundbreaking and technically brilliant film which clearly influenced subsequent film-makers such as the Marx Brothers, Lubitsch, and Mamoulian. The plot, a witty story of a poor artist who wins a huge lottery jackpot but has to search frantically all over town for the missing ticket, is basically just a device to support a series of wonderfully witty comic scenes enacted in a dream world of the director's imagination.<br /><br />One of the most impressive things about this film is that, though it is set in the middle of Paris and includes nothing actually impossible, it achieves a sustained and involving fairy-tale/fantasy atmosphere, in which it seems quite natural that people sing as much as they talk, or that a tussle over a stolen jacket should take on the form of a football game. Another memorable element is that Le Million includes what may be the funniest opera ever put on film (O that blonde-braided soprano! \"I laugh, ha! ha!\") Also a delight is the casting: Clair has assembled a group of amazing, sharply different character actors, each of them illustrating with deadly satiric accuracy a bourgeois French \"type,\" so that the film seems like a set of Daumier prints come to life.<br /><br />The hilarity takes a little while to get rolling, and I found the characters not as emotionally engaging as they can be even in a light comedy (as they are, for instance, in many Lubitsch films.) For these reasons I refrained from giving it the highest rating. But these minor cavils shouldn't distract from an enthusiastic recommendation.<br /><br />Should you see it? By all means. Highly recommended whether you want a classic and influential work of cinema or just a fun comedy. pos 1260 Before I comment about this movie, you should realize that when I saw this movie, I expected the typical crap, horror, B-movie and just wanted to have fun. Jack Frost is one that not only delivers but is actually one of the best that I've seen in a long time. Scott McDonald is great as Jack Frost, in fact I think he has a future in being psychopaths in big time movies if ever given the chance. McDonald is a serial killer who becomes a snowman through some stupid accidental mix of ridiculous elements. As soon as that snowman starts moving around and killing people, though, you will find it hard not to laugh. The lines that are said are completely retarded but really funny. The fact that the rest of the cast completely over-acts just adds to stupidity of the film, but it's stupidity is it's genius. The scene where the snowman is with the teenage girl is truly classic in B-movie, horror film fashion. I truly hope there is a sequel and I'll be right there to watch it on whatever cable channel does it. Of course it's only fun to watch the first few times and it's not exactly a good work of motion picture technology, but I just like to see snowmen kill people. I gave it a 7 out of 10, this is a great movie for dates and couples in the late hours. PyTorchText BuketIterator Batch size: 10 LABEL LENGTH TEXT neg 1118 Most college students find themselves lost in the bubble of academia, cut off from the communities in which they study and live. Their conversations are held with their fellow students and the college faculty. Steven Greenstreet's documentary is a prime example of a disillusioned college student who judges the entire community based on limited contact with a small number of its members.<br /><br />The documentary focused on a small group of individuals who were portrayed as representing large groups of the population. As is usual, the people who scream the most get the most media attention. Other than its misrepresentation of the community in which the film was set, the documentary was well made. My only dispute is that the feelings and uproar depicted in the film were attributed to the entire community rather than the few individuals who expressed them.<br /><br />Naturally it is important to examine a controversy like this and make people aware of the differences that exist between political viewpoints, but it is ridiculous to implicate an entire community of people in the actions of a few radicals. neg 1120 Looked forward to viewing this film and seeing these great actors perform. However, I was sadly disappointed in the script and the entire plot of the story. David Duchovny,(Dr. Eugene Sands),\"Connie & Carla\",'04, was the doctor in the story who uses drugs and losses his license to practice medicine. Dr. Sands was visiting a night club and was able to use his medical experience to help a wounded customer and was assisted by Angelina Jolie,(Claire),\"Taking Lives\",'04, who immediately becomes attracted to Dr. David Sands. Timothy Hutton,(Raymond Blossom),\"Kinsey\",'04, plays the Big Shot Gangster and a man with all kinds of money and connections. Timothy Hutton seems to over act in most of the scenes and goes completely out of his mind trying to keep his gang members from being killed. Gary Dourdan,(Yates),\"CSI-Vegas TV Series\", plays a great supporting role and portrays a real COOL DUDE who is a so-called body guard for Raymond Blossom. Angelina Jolie looks beautiful and sexy with her ruby red lips which draws a great deal of attention from all the men. This film is not the greatest, but it does entertain. pos 1120 I must say that, looking at Hamlet from the perspective of a student, Brannagh's version of Hamlet is by far the best. His dedication to stay true to the original text should be applauded. It helps the play come to life on screen, and makes it easier for people holding the text while watching, as we did while studying it, to follow and analyze the text.<br /><br />One of the things I have heard criticized many times is the casting of major Hollywood names in the play. I find that this helps viewers recognize the characters easier, as opposed to having actors that all look and sound the same that aid in the confusion normally associated with Shakespeare.<br /><br />Also, his flashbacks help to clear up many ambiguities in the text. Such as how far the relationship between Hamlet and Ophelia really went and why Fortinbras just happened to be at the castle at the end. All in all, not only does this version contain some brilliant performances by actors both familiar and not familiar with Shakespeare. It is presented in a way that one does not have to be an English Literature Ph.D to understand and enjoy it. pos 1120 As a baseball die-hard, this movie goes contrary to what I expect in a sports movie: authentic-looking sports action, believable characters, and an original story line. While \"Angels in the Outfield\" fails miserably in the first category, it succeeds beautifully in the latter two. \"Angels\" weaves the story of Roger and J.P., two Anaheim foster kids in love with baseball but searching for a family, with that of the woebegone Angels franchise, struggling to draw fans and win games. Pushed by his deadbeat father's promise that they would be a family only when the Angels win the pennant, Roger asks for some heavenly help, and gets it in the form of diamond-dwelling spirits bent on reversing the franchise's downward spiral. And, when short-fused manager George Knox (portrayed by Danny Glover) begins believing in what Roger sees, the team suddenly has hope for turning their season around--and Roger and J.P. find something to believe in. Glover in particular gives a nice performance, and Tony Danza, playing a washed-up pitcher, also does well, despite clearly having ZERO idea of how to pitch out of the windup! neg 1121 I have a piece of advice for the people who made this movie too, if you're gonna make a movie like this be sure you got the f/x to back it up. Also don't get a bunch of z list actors to play in it. Another thing, just about all of us have seen Jurassic Park, so don't blatantly copy it. All in all this movie sucked, f/x sucked, acting sucked, story unoriginal. Let's talk about the acting for just a second, the Carradine guy who's career peaked in 1984 when he did \"Revenge of the Nerds\" (which was actually a great comedy). He's not exactly z list, he can act. He just should have said no to this s--t bag. He should have did what Mark Hamill did after \"Return of the Jedi\" and go quietly into the night. He made his mark as a \"Nerd\" and that should have been that. I understand he has bills to pay, but that hardly excuses this s--t bag. Have I called this movie that yet? O.K. I just wanted to be sure. If I sound a little hostile, I apologize. I just wasted 2hrs of my life I could have spent doing something productive like watching paint peel, and I feel cheated. I'll close on that note. Thank you for your time. neg 1121 By 1941 Columbia was a full-fledged major studio and could produce a movie with the same technical polish as MGM, Paramount or Warners. That's the best thing that could be said about \"Adam Had Four Sons,\" a leaden soap opera with almost terminally bland performances by Ingrid Bergman (top-billed for the first time in an American film) and Warner Baxter. Bergman plays a Frenchwoman (this was the era in which Hollywood thought one foreign accent was as good as another) hired as governess to Baxter's four sons and staying on (with one interruption caused by the stock-market crash of 1907) until the boys are grown men serving in World War I. Just about everyone in the movie is so goody-good it's a relief when Susan Hayward as the villainess enters midway through \u2014 she's about the only watchable person in the movie even though she's clearly channeling Bette Davis and Vivien Leigh; it's also the first in her long succession of alcoholic roles \u2014 but the script remains saccharine and the ending is utterly preposterous. No wonder Bergman turned down the similarly plotted \"The Valley of Decision\" four years later. neg 1123 I have never read the book\"A wrinkle in time\". To be perfectly honesty, after seeing the movie, do I really want to? Well, I shouldn't be reviewing this movie i'll start off with that. Next i'll say that the TV movie is pretty forgettable. Do you know why I say that? Because I forgot what happens in it. I told you it was forgettable. To be perfectly honest, no TV movie will ever be better than \"Merlin\".<br /><br />How do I describe a TV movie? I have never written a review for one before. Well, i'll just say that they usually have some celebrities. A wrinkle in time includes only one. Alfre Woodard(Or Woodward, I am not sure), the Oscar winner. <br /><br />The film has cheesy special effects, a mildly interesting plot, scenes that make you go \"WTF\". The movie is incredibly bad and it makes you go\"WTF\". What did I expect? It's a TV movie. They usually aren't good. As is this one. A wrinkle in time is a waste of time and a big time waster. To top it off, you'll most likely forget about it the second it's over. Well, maybe not the second it's over. But within a few minutes.<br /><br />A wrinkle in time:*/**** neg 1123 After watching \"The Bodyguard\" last night, I felt compelled to write a review of it.<br /><br />This could have been a pretty decent movie had it not been for the awful camera-work. It was beyond annoying. The angles were all wrong, it was impossible to see anything, especially during the fight sequences. The closeups were even horrible.<br /><br />The story has Sonny Chiba hiring himself out as a bodyguard to anyone willing to lead him to the top of a drug ring. He is approached by Judy Lee, who is never quite straight with Chiba. Lee's involvement in the drug ring is deeper than Chiba thought, as the Mob and another gang of thugs are after her.<br /><br />The story was decent, and despite horrible dubbing, this could have been a good movie. Given better direction and editing, I'm sure this would have been a classic Kung Foo movie. As it is, it's more like another cheesy 70's action movie.<br /><br />Note: The opening sequence has a quote familiar to \"Pulp Fiction\" fans, and then continues to a karate school in Times Square that is in no way related to the rest of the movie.<br /><br />Rating: 4 out of 10 neg 1123 There are some really terrific ideas in this violent movie that, if executed clearly, could have elevated it from Spaghetti-western blandness into something special. Unfortunately, A TOWN CALLED HELL is one of the worst edited movies imaginable! Scenes start and end abruptly, characters leave for long stretches, the performances (and accents) of the actors are pretty inconsistent, etc.<br /><br />Robert Shaw is a Mexican(!) revolutionary who, after taking part in wiping out a village, stays on to become a priest(!)...ten years later the village is being run by \"mayor\" Telly Salavas. Stella Stevens arrives looking for revenge on the man who killed her husband. Colonel Martin Landau arrives looking for Shaw. They all yell at each other A LOT and they all shoot each other A LOT. Fernando Rey is in it too (as a blind man). The performances aren't bad, but they are mightily uneven. Savalas has an accent sometimes as does Landau (who is really grating here). Shaw and Rey prove that they are incapable of really embarrassing themselves and Stevens looks pretty foxy (if a bit out of place amongst the sweaty filth). neg 1124 The movie is plain bad. Simply awful. The string of bad movies from Bollywood has no end! They must be running out of excuses for making such awful movies (or not).<br /><br />The problem seems to be with mainly the directors. This movie has 2 good actors who have proved in the past that the have the ability to deliver great performance...but they were directed so poorly. The poor script did not help either.<br /><br />This movie has plenty of ridiculous moments and very bad editing in the first half. For instance :<br /><br />After his 1st big concert, Ajay Devgan, meets up with Om Puri (from whom he ran away some 30 years ago and talked to again) and all Om Puri finds to say is to beware of his friendship with Salman!!! What a load of crap. Seriously. Not to mention the baaad soundtrack. Whatever happened to Shankar Ehsaan Loy?<br /><br />Ajay Devgun is total miscast for portraying a rockstar.<br /><br />Only saving grace are the good performances in the second half. Ajay shines as his character shows his dark side. So does Salman as the drug addict. <br /><br />Watch it maybe only for the last half hour. Train Loop Examples Now let's look at a model training loop would look like. I printed the first 10 batches list of examples lengths to show how nicely they are grouped throughout the dataset! We see that we get same exact behavior as we did when using PyTorch Dataset. Now it depends on which way is easier for you to use PyTorchText BucketIterator: with PyTorch Dataset or with PyTorchText TabularDataset # Example of number of epochs. epochs = 1 # Example of loop through each epoch. for epoch in range ( epochs ): # Create batches - needs to be called before each loop. torchtext_train_dataloader . create_batches () # Loop through BucketIterator. for sample_id , batch in enumerate ( torchtext_train_dataloader . batches ): # Put all example.text of batch in single array. batch_text = [ example . text for example in batch ] print ( 'Batch examples lengths: %s ' . ljust ( 20 ) % str ([ len ( text ) for text in batch_text ])) # Let's break early, you get the idea. if sample_id == 10 : break Batch examples lengths: [791, 791, 792, 792, 793, 797, 797, 799, 799, 801] Batch examples lengths: [4766, 4823, 4832, 4859, 4895, 4944, 5025, 5150, 5309, 5313] Batch examples lengths: [695, 695, 696, 696, 696, 697, 699, 699, 699, 700] Batch examples lengths: [958, 959, 960, 961, 963, 963, 963, 966, 966, 967] Batch examples lengths: [1200, 1203, 1204, 1205, 1208, 1209, 1212, 1214, 1218, 1221] Batch examples lengths: [2621, 2628, 2639, 2651, 2651, 2672, 2690, 2704, 2705, 2712] Batch examples lengths: [1811, 1812, 1815, 1830, 1835, 1838, 1841, 1849, 1852, 1878] Batch examples lengths: [3104, 3107, 3111, 3115, 3133, 3174, 3201, 3206, 3217, 3278] Batch examples lengths: [3000, 3001, 3001, 3031, 3039, 3047, 3056, 3075, 3084, 3103] Batch examples lengths: [1046, 1050, 1053, 1053, 1054, 1057, 1060, 1067, 1073, 1077] Batch examples lengths: [749, 751, 751, 756, 758, 759, 760, 761, 762, 763] Final Note If you made it this far Congrats! \ud83c\udf8a and Thank you! \ud83d\ude4f for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you see something wrong please let me know by opening an issue on my ml_things GitHub repository ! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can. Contact \ud83c\udfa3 \ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"PyTorchText BucketIterator"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#better-batches-with-pytorchtext-bucketiterator","text":"","title":"Better Batches with PyTorchText BucketIterator"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#how-to-use-pytorchtext-bucketiterator-to-sort-text-data-for-better-batching","text":"Disclaimer: The format of this tutorial notebook is very similar with my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format. This notebook is a simple tutorial on how to use the powerful PytorchText BucketIterator functionality to group examples ( I use examples and sequences interchangeably ) of similar lengths into batches. This allows us to provide the most optimal batches when training models with text data. Having batches with similar length examples provides a lot of gain for recurrent models (RNN, GRU, LSTM) and transformers models (bert, roBerta, gpt2, xlnet, etc.) where padding will be minimal. Basically any model that takes as input variable text data sequences will benefit from this tutorial. I will not train any models in this notebook! I will release a tutorial where I use this implementation to train a transformer model. The purpose is to use an example text datasets and batch it using PyTorchText with BucketIterator and show how it groups text sequences of similar length in batches. This tutorial has two main parts: Using PyTorch Dataset with PyTorchText Bucket Iterator : Here I implemented a standard PyTorch Dataset class that reads in the example text datasets and use PyTorch Bucket Iterator to group similar length examples in same batches. I want to show how easy it is to use this powerful functionality form PyTorchText on a regular PyTorch Dataset workflow which you already have setup. Using PyTorch Text TabularDataset with PyTorchText Bucket Iterator : Here I use the built-in PyTorchText TabularDataset that reads data straight from local files without the need to create a PyTorch Dataset class. Then I follow same steps as in the previous part to show how nicely text examples are grouped together. This notebooks is a code adaptation and implementation inspired from a few sources: torchtext_translation_tutorial , pytorch/text - GitHub , torchtext documentation and A Comprehensive Introduction to Torchtext .","title":"How to use PyTorchText BucketIterator to sort text data for better batching."},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#what-should-i-know-for-this-notebook","text":"Some basic PyTorch regarding Dataset class and using DataLoaders. Some knowledge of PyTorchText is helpful but not critical in understanding this tutorial. The BucketIterator is similar in applying Dataloader to a PyTorch Dataset.","title":"What should I know for this notebook?"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#how-to-use-this-notebook","text":"The code is made with reusability in mind. It can be easily adapted for other text datasets and other NLP tasks in order to achieve optimal batching. Comments should provide enough guidance to easily adapt this notebook to your needs. This code is designed mostly for classification tasks in mind, but it can be adapted for any other Natural Language Processing tasks where batching text data is needed.","title":"How to use this notebook?"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#dataset","text":"I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with.","title":"Dataset"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#coding","text":"Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant\u200a-\u200ashow the output. I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.","title":"Coding"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#downloads","text":"Download the IMDB Movie Reviews sentiment dataset and unzip it locally. # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz","title":"Downloads"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#installs","text":"ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Building wheel for ml-things (setup.py) ... done Building wheel for ftfy (setup.py) ... done","title":"Installs"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#imports","text":"Import all needed libraries for this notebook. Declare basic parameters used for this notebook: device - Device to use by torch: GPU/CPU. I use CPU as default since I will not perform any costly operations. train_batch_size - Batch size used on train data. valid_batch_size - Batch size used for validation data. It usually is greater than train_batch_size since the model would only need to make prediction and no gradient calculations is needed. import io import os import torchtext from tqdm.notebook import tqdm from ml_things import fix_text from torch.utils.data import Dataset , DataLoader # Will use `cpu` for simplicity. device = 'cpu' # Number of batches for training train_batch_size = 10 # Number of batches for validation. Use a larger value than training. # It helps speed up the validation process. valid_batch_size = 20","title":"Imports"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#using-pytorch-dataset","text":"This is where I create the PyTorch Dataset objects for training and validation that can be used to feed data into a model. This is standard procedure when using PyTorch.","title":"Using PyTorch Dataset"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#dataset-class","text":"Implementation of the PyTorch Dataset class. Most important components in a PyTorch Dataset class are: __len__(self, ) where it returns the number of examples in our dataset that we read in __init__(self, ) . This will ensure that len() will return the number of examples. __getitem__(self, item) where given an index item will return the example corresponding to the item position. class MovieReviewsTextDataset ( Dataset ): r \"\"\"PyTorch Dataset class for loading data. This is where the data parsing happens. This class is built with reusability in mind. Arguments: path (:obj:`str`): Path to the data partition. \"\"\" def __init__ ( self , path ): # Check if path exists. if not os . path . isdir ( path ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) self . texts = [] self . labels = [] # Since the labels are defined by folders with data we loop # through each label. for label in [ 'pos' , 'neg' ]: sentiment_path = os . path . join ( path , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:10] # Sample for debugging. # Go through each file and read its content. for file_name in tqdm ( files_names , desc = f ' { label } Files' ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. self . texts . append ( content ) # Save labels. self . labels . append ( label ) # Number of examples. self . n_examples = len ( self . labels ) return def __len__ ( self ): r \"\"\"When used `len` return the number of examples. \"\"\" return self . n_examples def __getitem__ ( self , item ): r \"\"\"Given an index return an example from the position. Arguments: item (:obj:`int`): Index position to pick an example to return. Returns: :obj:`Dict[str, str]`: Dictionary of inputs that are used to feed to a model. \"\"\" return { 'text' : self . texts [ item ], 'label' : self . labels [ item ]}","title":"Dataset Class"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#train---validation-datasets","text":"Create PyTorch Dataset for train and validation partitions. print ( 'Dealing with Train...' ) # Create pytorch dataset. train_dataset = MovieReviewsTextDataset ( path = '/content/aclImdb/train' ) print ( f 'Created `train_dataset` with { len ( train_dataset ) } examples!' ) print () print ( 'Dealing with Validation...' ) # Create pytorch dataset. valid_dataset = MovieReviewsTextDataset ( path = '/content/aclImdb/test' ) print ( f 'Created `valid_dataset` with { len ( valid_dataset ) } examples!' ) Dealing with Train... pos Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [01:22<00:00, 151.34it/s] neg Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [01:10<00:00, 178.52it/s] Created `train_dataset` with 25000 examples! Dealing with Validation... pos Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [01:22<00:00, 151.34it/s] neg Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [01:10<00:00, 178.52it/s] Created `valid_dataset` with 25000 examples!","title":"Train - Validation Datasets"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#pytorch-dataloader","text":"In order to group examples from the PyTorch Dataset into batches we use PyTorch DataLoader. This is standard when using PyTorch. # Move pytorch dataset into dataloader. torch_train_dataloader = DataLoader ( train_dataset , batch_size = train_batch_size , shuffle = True ) print ( f 'Created `torch_train_dataloader` with { len ( torch_train_dataloader ) } batches!' ) # Move pytorch dataset into dataloader. torch_valid_dataloader = DataLoader ( valid_dataset , batch_size = valid_batch_size , shuffle = False ) print ( f 'Created `torch_valid_dataloader` with { len ( torch_valid_dataloader ) } batches!' ) Created `torch_train_dataloader` with 2500 batches! Created `torch_valid_dataloader` with 1250 batches!","title":"PyTorch DataLoader"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#pytorchtext-bucket-iterator-dataloader","text":"Here is where the magic happens! We pass in the train_dataset and valid_dataset PyTorch Dataset splits into BucketIterator to create the actual batches. It's very nice that PyTorchText can handle splits! No need to write same line of code again for train and validation split. The sort_key parameter is very important! It is used to order text sequences in batches. Since we want to batch sequences of text with similar length, we will use a simple function that returns the length of an data example ( len(x['text') ). This function needs to follow the format of the PyTorch Dataset we created in order to return the length of an example, in my case I return a dictionary with text key for an example. It is important to keep sort=False and sort_with_batch=True to only sort the examples in each batch and not the examples in the whole dataset! Find more details in the PyTorchText BucketIterator documentation here - look at the BPTTIterator because it has same parameters except the bptt_len argument. Note: If you want just a single DataLoader use torchtext.data.BucketIterator instead of torchtext.data.BucketIterator.splits and make sure to provide just one PyTorch Dataset instead of tuple of PyTorch Datasets and change the parameter batch_sizes and its tuple values to batch_size with single value: dataloader = torchtext.data.BucketIterator(dataset, batch_size=batch_size, ) # Group similar length text sequences together in batches. torchtext_train_dataloader , torchtext_valid_dataloader = torchtext . data . BucketIterator . splits ( # Datasets for iterator to draw data from ( train_dataset , valid_dataset ), # Tuple of train and validation batch sizes. batch_sizes = ( train_batch_size , valid_batch_size ), # Device to load batches on. device = device , # Function to use for sorting examples. sort_key = lambda x : len ( x [ 'text' ]), # Repeat the iterator for multiple epochs. repeat = True , # Sort all examples in data using `sort_key`. sort = False , # Shuffle data on each epoch run. shuffle = True , # Use `sort_key` to sort examples in each batch. sort_within_batch = True , ) # Print number of batches in each split. print ( 'Created `torchtext_train_dataloader` with %d batches!' % len ( torchtext_train_dataloader )) print ( 'Created `torchtext_valid_dataloader` with %d batches!' % len ( torchtext_valid_dataloader )) Created `torchtext_train_dataloader` with 2500 batches! Created `torchtext_valid_dataloader` with 1250 batches!","title":"PyTorchText Bucket Iterator Dataloader"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#compare-dataloaders","text":"Let's compare the PyTorch DataLoader batches with the PyTorchText BucketIterator batches. We can see how nicely examples of similar length are grouped in same batch with PyTorchText. Note: When using the PyTorchText BucketIterator, make sure to call create_batches() before looping through each batch! Else you won't get any output form the iterator. # Loop through regular dataloader. print ( 'PyTorch DataLoader \\n ' ) for batch in torch_train_dataloader : # Let's check batch size. print ( 'Batch size: %d \\n ' % len ( batch [ 'text' ])) print ( 'LABEL \\t LENGTH \\t TEXT' . ljust ( 10 )) # Print each example. for text , label in zip ( batch [ 'text' ], batch [ 'label' ]): print ( ' %s \\t %d \\t %s ' . ljust ( 10 ) % ( label , len ( text ), text )) print ( ' \\n ' ) # Only look at first batch. Reuse this code in training models. break # Create batches - needs to be called before each loop. torchtext_train_dataloader . create_batches () # Loop through BucketIterator. print ( 'PyTorchText BuketIterator \\n ' ) for batch in torchtext_train_dataloader . batches : # Let's check batch size. print ( 'Batch size: %d \\n ' % len ( batch )) print ( 'LABEL \\t LENGTH \\t TEXT' . ljust ( 10 )) # Print each example. for example in batch : print ( ' %s \\t %d \\t %s ' . ljust ( 10 ) % ( example [ 'label' ], len ( example [ 'text' ]), example [ 'text' ])) print ( ' \\n ' ) # Only look at first batch. Reuse this code in training models. break PyTorch DataLoader Batch size: 10 LABEL LENGTH TEXT neg 811 Much as we all love Al Pacino, it was painful to see him in this movie. A publicity hack at the grubby ending of what seems to have once been a distinguished and idealistic career Pacino plays his part looking like an unmade bed and assaulting everyone with a totally bogus and inconsistent southern accent.<br /><br />The plot spools out this way and that with so many loose ends and improbabilities that the mind reels (and then retreats).<br /><br />Kim Basinger is there, not doing much. Her scenes with Pacino are flat and unconvincing. Hard to believe they meant a lot to each other. There's no energy there.<br /><br />Tea Leone, on the other hand, lit up the screen. She was electric and her scenes with Pacino were by far the most interesting in the movie, but not enough to save Al from embarrassment. neg 572 God, I am so sick of the crap that comes out of America called \"Cartoons\"!<br /><br />Since anime became popular, the USA animators either produce a cartoon with a 5-year-old-lazy-ass style of drawing (Kappa Mikey) or some cheep Japanese animation rip-off. (Usually messing up classic characters) No wonder anime is beating American cartoons! <br /><br />They are not even trying anymore! <br /><br />Oh, I just heard of this last night; I live in the UK and when I found out this show first came out in 2005,well, I never knew the UK was so up-to-date with current shows. pos 3122 First an explanation on what makes a great movie for me. Excitement about not knowing what is coming next will make me enjoy a movie the first time I watch it (case en point: Twister). There are also other things that go into a great first viewing such as good humor (John Candy in Uncle Buck and The Great Outdoors), good plot with good resolution (Madeline and Matilda), imaginative storytelling (all Star Wars episodes-George Lucas is THE MAN), and good music (again all Star Wars episodes, Wizard of Oz, Sound of Music). What makes me watch a movie at least six times in the theatre and buy a DVD or VHS tape? Characters. With that said, I present Cindy Lou Who and The Grinch. Excellent performance Taylor Momsen and Jim Carrey. The rest of the cast was very good, particularly Jeffery Tambor, Bill Irwin, Molly Shannon, Christine Baranski, and Josh Ryan Evans. But, every single scene with Cindy and The Grinch-together is excellent and very funny and/or heartwarming. Cindy Lou is my favorite character in this movie and the most compelling reason why the movie is better than the cartoon. The Grinch has a strong plot, good conflicts, and a very good theme (I can't get started because I don't want to spoil it). Jim Carrey was very funny as The Grinch-particularly when he interacted with Cindy. And the music! Wow! Excellent music by James Horner. I loved his selection of instruments and the compositions. Very good job Jim Carrey-I didn't know you could sing. Taylor Momsen! Whoa! Your voice is reason enough to see the movie at least once. On your solo - Where Are You Christmas - is your voice really as high as it sounds? Sounds like an F#? That is an obscene range for a 7-year old (obscene meant in the best possible way). Great job. This is the best performance by a child I have ever heard in a movie(Taylor beat out the Von Trapp Children-no small feat!). And now to the actors. Jim Carrey was great, funny, and, surprisingly very sensitive (this really showed through in his scenes with Taylor Momsen). Taylor Momsen's unspoken expressions(one of the secrets to a good acting performance) are very strong-she really becomes Cindy Lou Who. And when she does dialogue she is even stronger.<br /><br />******************************danger:spoiler alert********************* ***********************************************************************<br /><br />Examples: expression when she first sees The Grinch. This is a classic quote (\"You're the the the\" and then filled in with the Grinch line \"da da da THE GRINCH-after which she topples into the sorter and then is rescued by The Grinch). The \"Thanks for saving me\" quote and subsequent response by The Grinch was also very good.<br /><br />My favorite part of the movie is when Cindy invites The Grinch to be Holiday Cheermeister. This scene is two excellent actors at their best interacting and expressing with each other. Little Taylor Momsen completely holds her own with Jim Carrey in this spot. I sincerely hope we see Taylor Momsen in many more films to come. All in all everything was great about this movie (except maybe the feet and noses). pos 483 Red Rock West is one of those rare films that keeps you guessing the entire time as to what will happen next. Nicolas Cage is mistaken for a contract killer as he enters a small town trying to find work. Dennis Hopper is the bad guy and no one plays them better. Look for a brief appearance by country singing star Dwight Yoakam. This is a serious drama most of the time but there are some lighter moments. What matters is that you will enjoy this low budget but high quality effort! pos 759 This movie is a remake of two movies that were a lot better. The last one, Heaven Can Wait, was great, I suggest you see that one. This one is not so great. The last third of the movie is not so bad and Chris Rock starts to show some of the comic fun that got him to where he is today. However, I don't know what happened to the first two parts of this movie. It plays like some really bad \"B\" movie where people sound like they are in some bad TV sit-com. The situations are forced and it is like they are just trying to get the story over so they can start the real movie. It all seems real fake and the editing is just bad. I don't know how they could release this movie like that. Anyway, the last part isn't to bad, so wait for the video and see it then. pos 2471 VIVAH in my opinion is the best movie of 2006, coming from a director that has proved successful throughout his career. I am not too keen in romantic movies these days, because i see them as \"old wine in a new bottle\" and so predictable. However, i have watched this movie three times now...and believe me it's an awesome movie.<br /><br />VIVAH goes back to the traditional route, displaying simple characters into a sensible and realistic story of the journey between engagement and marriage. The movie entertains in all manners as it can be reflected to what we do (or would do) when it comes to marriage. In that sense Sooraj R. Barjatya has done his homework well and has depicted a very realistic story into a well-made highly entertaining movie.<br /><br />Several sequences in this movie catch your interest immediately: <br /><br />* When Shahid Kapoor comes to see the bride (Amrita Rao) - the way he tries to look at her without making it too obvious in front of his and her family. The song 'Do Anjaane Ajnabi' goes well with the mood of this scene.<br /><br />* The first conversation between Shahid and Amrita, when he comes to see her - i.e. a shy Shahid not knowing exactly what to talk about but pulling of a decent conversation. Also Amrita's naive nature, limited eye-contact, shy characteristics and answering softly to Shahid's questions.<br /><br />* The emotional breakdown of Amrita and her uncle (Alok Nath) when she feeds him at Shahid's party in the form of another's daughter-in-law rather than her uncle's beloved niece.<br /><br />Clearly the movie belongs to Amrita Rao all the way. The actress portrays the role of Poonam with such conviction that you cannot imagine anybody else replacing her. She looks beautiful throughout the whole movie, and portrays an innocent and shy traditional girl perfectly.<br /><br />Shahid Kapoor performs brilliantly too. He delivers a promising performance and shows that he is no less than Salman Khan when it comes to acting in a Sooraj R. Barjatya film. In fact Shahid and Amrita make a cute on-screen couple, without a shadow of doubt. Other characters - Alok Nath (Excellent), Anupam Kher (Brilliant), Mohan Joshi (Very good).<br /><br />On the whole, VIVAH delivers what it promised, a well made and realistic story of two families. The movie has top-notch performances, excellent story and great music to suit the film, as well as being directed by the fabulous Sooraj R. Barjatya. It's a must see! neg 626 Watching this Movie? l thought to myself, what a lot of garbage. These girls must have rocks for brains for even agreeing to be part of it. Waste of time watching it, faint heavens l only hired it. The acting was below standard and story was unbearable. Anyone contemplating watching this film, please save your money. The film has no credit at all. l am a real film buff and this is worse than \"Attack of the Green Tomatoes\".<br /><br />l only hope that this piece of trash didn't cost too much to make. Money would have been better spent on the homeless people of the world. l only hope there isn't a sequel in the pipeline. pos 2599 A SPECIAL DAY (Ettore Scola - Italy/Canada 1977).<br /><br />Every once in a while, you come across a film that really touches a nerve. This one offers a very simple premise, almost flawlessly executed in every way and incredibly moving at the same time. It's surprising Ettore Scola's \"Una giornate particulare\" is relatively unheralded, even hated by some critics. Time Out calls it 'rubbish' and Leonard Maltin, somewhat milder, 'pleasant but trifling.' I disagree, not only because this film is deeply moving, but within its simple story it shows us more insights about daily life in fascist Italy than most films I've seen. The cinematography is distinctly unflashy, even a bit bland, and the storyline straightforward, which might explain the film's relative unpopularity. Considering late '70s audiences weren't exactly spoiled with great Italian films, it's even stranger this one didn't really catch on with the critics.<br /><br />The film begins with a ten-minute collage of archive footage from Hitler's visit to Italy on may 8th 1938. Set against this background, we first meet Antonietta (Loren), a lonely, love-ridden housewife with six children in a roman apartment building. One day, when her Beo escapes, she meets her neighbour Gabriele (Mastroianni), who seems to be only one in the building not attending the ceremonies. He is well-mannered, cultured and soon she is attracted to him. During the whole film, we hear the fascist rally from the radio of the concierge hollering through the courtyard. Scola playfully uses the camera to make us part of the proceedings. After the opening scene, the camera swanks across the courtyard of the modernist (hypermodern at the time) apartment block, seemingly searching for our main characters, whom we haven't met yet. <br /><br />Marcello Mastrionani and Sophia Loren are unforgettable in the two leading roles, all the more astonishing since they are cast completely against type. Canadian born John Vernon plays Loren's husband, but he is only on screen in the first and last scene. I figure his voice must have been dubbed, since he's not of Italian descent and never lived there, to my knowledge, so I cannot imagine he speaks Italian. If his voice has been dubbed, I didn't notice at all. On the contrary, he's completely believable as an Italian, even more than the rest of the cast. The story is simple but extremely effective, the performances are outstanding, the ending is just perfect and the framing doesn't come off as overly pretentious but works completely. Don't miss out on this one.<br /><br />Camera Obscura --- 9/10 neg 1482 There are some extremely talented black directors Spike Lee,Carl Franklin,Billy Dukes,Denzel and a host of others who bring well deserved credit to the film industry . Then there are the Wayans Brothers who at one time(15,years ago) had an extremely funny television show'In Living Colour' that launched the career of Jim Carrey amongst others . Now we have stupidity substituting for humour and gross out gags(toilet humour) as the standard operating procedure . People are not as stupid as those portrayed in 'Little Man' they couldn't possibly be . A baby with a full set of teeth and a tattoo is accepted as being only months old ? Baby comes with a five o'clock shadow that he shaves off . It is intimated that the baby has sex with his foster mother behind her husbands,Darryl's, back .Oh, yea that is just hilarious . As a master criminal 'Little Man' is the stupidest on planet earth . He stashes a stolen rock that is just huge in a woman's purse and then has to pursue her . Co-star Chazz Palminteri,why Chazz, offers the best line: \"I'm surrounded by morons.\" Based, without credit, on a Chuck Jones cartoon, Baby Buggy Bunny . This is far too stupid to be even remotely funny . A clue as to how bad this film is Damon Wayans appeared on Jay Leno the other night,prior to the BAT awards and he did not,even mention this dreadful movie . When will Hollywood stop green lighting trash from the Wayans Brothers . When they get over their white mans guilt in all likelihood . neg 4380 There is a bit of a spoiler below, which could ruin the surprise of the ONE unexpected and truly funny scene in this film. There is also information about the first film in this series.<br /><br />I caught this film on DVD, which someone gave as a gift to my roommate. It came as a set together with the first film in the \"Blind Dead\" series.<br /><br />This movie was certainly much worse than the first, \"La Noche del Terror Ciego\". In addition, many of the features of the first movie were changed significantly. To boot, the movie was dubbed in English (the first was subtitled), which I tend to find distracting.<br /><br />The concept behind the series is that in the distant past a local branch of the Knights Templar was involved in heinous and secret rituals. Upon discovery of these crimes, the local peasantry put the Templars to death in such a manner that their eyes can no longer be used, thus preventing them from returning from Hell to exact their revenge. We then jump to modern times where because of some event, the Templars arise from the dead to exact their revenge upon the villagers whose ancestors messed them up in the first place. Of course, since the undead knights have no eyes, they can only find their victims when they make some sort of noise.<br /><br />The Templars were a secretive order, from about the 12th century, coming out of the Crusades. They were only around for about 150 years, before they were suppressed in the early 1300s by the Pope and others. Because they were secretive, there were always rumors about their ceremonies, particularly for initiation. Also, because of the way the society was organized, you didn't necessarily have church officials overseeing things, which meant they didn't have an inside man when things heated up. And, because of the nature of their trials, they were tortured into confessions. The order was strongest in France, but did exist in Portugal and Spain, where the movies take place.<br /><br />Where the first movie had a virgin sacrifice and knights drinking the blood directly from the body of the virgin (breast shots here, of course, this is a horror film after all), and then, once the knights come back to life, they attack their victims by eating them alive and sucking their blood; in this sequel, this all disappears. You still have the same scene (redone, not the same footage) of them sacrificing the virgin, but they drain the blood into a bowl and drink it from that. Thus, when they come back, they just hack people up with their swords or claw people to death, which I have to say is a much less effective means of disturbing your audience. There's also a time problem: in the first film the dating is much closer to the Templars, where here they are now saying it is the 500 anniversary of the peasants burning these guys at the stake, which would date it around 1473. And the way that the Templars lose their eyes is much less interesting as well. In the first, they have them pecked out by crows. Now they are simply burned out, and in quite a ridiculous manner.<br /><br />Oh yeah, and maybe it was just me, but there seemed to be a lot of people from the first movie reappearing in this film (despite having died). Not really a problem, since the movie is completely different and not a sequel in the sense of a continuation, but odd none-the-less.<br /><br />The highlight of this movie is the rich fellow who uses a child to distract the undead while he makes a break for the jeep. The child's father had already been suckered by this rich man into making an attempt to get the jeep, so he walks out and tells her to find her father. It comes somewhat out of the blue, and is easily the funniest scene in the film. Of course, why the child doesn't die at this point is beyond me, and disappointed for horror fans.<br /><br />I couldn't possibly recommend this film to anyone. It isn't so bad that it becomes funny, so it just ends up being a mediocre horror film. The bulk of the film has several people holed up in a church, each making various attempts to go it alone in order to escape the blind dead who have them surrounded. When the film ends, you are not surprised at the outcome at all; in fact, quite disappointed. If you are into the novelty of seeing a Spanish horror film, see the first movie, which at least has some innovative ideas and not so expected outcomes. PyTorchText BuketIterator Batch size: 10 LABEL LENGTH TEXT neg 1118 Most college students find themselves lost in the bubble of academia, cut off from the communities in which they study and live. Their conversations are held with their fellow students and the college faculty. Steven Greenstreet's documentary is a prime example of a disillusioned college student who judges the entire community based on limited contact with a small number of its members.<br /><br />The documentary focused on a small group of individuals who were portrayed as representing large groups of the population. As is usual, the people who scream the most get the most media attention. Other than its misrepresentation of the community in which the film was set, the documentary was well made. My only dispute is that the feelings and uproar depicted in the film were attributed to the entire community rather than the few individuals who expressed them.<br /><br />Naturally it is important to examine a controversy like this and make people aware of the differences that exist between political viewpoints, but it is ridiculous to implicate an entire community of people in the actions of a few radicals. neg 1120 Looked forward to viewing this film and seeing these great actors perform. However, I was sadly disappointed in the script and the entire plot of the story. David Duchovny,(Dr. Eugene Sands),\"Connie & Carla\",'04, was the doctor in the story who uses drugs and losses his license to practice medicine. Dr. Sands was visiting a night club and was able to use his medical experience to help a wounded customer and was assisted by Angelina Jolie,(Claire),\"Taking Lives\",'04, who immediately becomes attracted to Dr. David Sands. Timothy Hutton,(Raymond Blossom),\"Kinsey\",'04, plays the Big Shot Gangster and a man with all kinds of money and connections. Timothy Hutton seems to over act in most of the scenes and goes completely out of his mind trying to keep his gang members from being killed. Gary Dourdan,(Yates),\"CSI-Vegas TV Series\", plays a great supporting role and portrays a real COOL DUDE who is a so-called body guard for Raymond Blossom. Angelina Jolie looks beautiful and sexy with her ruby red lips which draws a great deal of attention from all the men. This film is not the greatest, but it does entertain. pos 1120 I must say that, looking at Hamlet from the perspective of a student, Brannagh's version of Hamlet is by far the best. His dedication to stay true to the original text should be applauded. It helps the play come to life on screen, and makes it easier for people holding the text while watching, as we did while studying it, to follow and analyze the text.<br /><br />One of the things I have heard criticized many times is the casting of major Hollywood names in the play. I find that this helps viewers recognize the characters easier, as opposed to having actors that all look and sound the same that aid in the confusion normally associated with Shakespeare.<br /><br />Also, his flashbacks help to clear up many ambiguities in the text. Such as how far the relationship between Hamlet and Ophelia really went and why Fortinbras just happened to be at the castle at the end. All in all, not only does this version contain some brilliant performances by actors both familiar and not familiar with Shakespeare. It is presented in a way that one does not have to be an English Literature Ph.D to understand and enjoy it. pos 1120 As a baseball die-hard, this movie goes contrary to what I expect in a sports movie: authentic-looking sports action, believable characters, and an original story line. While \"Angels in the Outfield\" fails miserably in the first category, it succeeds beautifully in the latter two. \"Angels\" weaves the story of Roger and J.P., two Anaheim foster kids in love with baseball but searching for a family, with that of the woebegone Angels franchise, struggling to draw fans and win games. Pushed by his deadbeat father's promise that they would be a family only when the Angels win the pennant, Roger asks for some heavenly help, and gets it in the form of diamond-dwelling spirits bent on reversing the franchise's downward spiral. And, when short-fused manager George Knox (portrayed by Danny Glover) begins believing in what Roger sees, the team suddenly has hope for turning their season around--and Roger and J.P. find something to believe in. Glover in particular gives a nice performance, and Tony Danza, playing a washed-up pitcher, also does well, despite clearly having ZERO idea of how to pitch out of the windup! neg 1121 I have a piece of advice for the people who made this movie too, if you're gonna make a movie like this be sure you got the f/x to back it up. Also don't get a bunch of z list actors to play in it. Another thing, just about all of us have seen Jurassic Park, so don't blatantly copy it. All in all this movie sucked, f/x sucked, acting sucked, story unoriginal. Let's talk about the acting for just a second, the Carradine guy who's career peaked in 1984 when he did \"Revenge of the Nerds\" (which was actually a great comedy). He's not exactly z list, he can act. He just should have said no to this s--t bag. He should have did what Mark Hamill did after \"Return of the Jedi\" and go quietly into the night. He made his mark as a \"Nerd\" and that should have been that. I understand he has bills to pay, but that hardly excuses this s--t bag. Have I called this movie that yet? O.K. I just wanted to be sure. If I sound a little hostile, I apologize. I just wasted 2hrs of my life I could have spent doing something productive like watching paint peel, and I feel cheated. I'll close on that note. Thank you for your time. neg 1121 By 1941 Columbia was a full-fledged major studio and could produce a movie with the same technical polish as MGM, Paramount or Warners. That's the best thing that could be said about \"Adam Had Four Sons,\" a leaden soap opera with almost terminally bland performances by Ingrid Bergman (top-billed for the first time in an American film) and Warner Baxter. Bergman plays a Frenchwoman (this was the era in which Hollywood thought one foreign accent was as good as another) hired as governess to Baxter's four sons and staying on (with one interruption caused by the stock-market crash of 1907) until the boys are grown men serving in World War I. Just about everyone in the movie is so goody-good it's a relief when Susan Hayward as the villainess enters midway through \u2014 she's about the only watchable person in the movie even though she's clearly channeling Bette Davis and Vivien Leigh; it's also the first in her long succession of alcoholic roles \u2014 but the script remains saccharine and the ending is utterly preposterous. No wonder Bergman turned down the similarly plotted \"The Valley of Decision\" four years later. neg 1123 I have never read the book\"A wrinkle in time\". To be perfectly honesty, after seeing the movie, do I really want to? Well, I shouldn't be reviewing this movie i'll start off with that. Next i'll say that the TV movie is pretty forgettable. Do you know why I say that? Because I forgot what happens in it. I told you it was forgettable. To be perfectly honest, no TV movie will ever be better than \"Merlin\".<br /><br />How do I describe a TV movie? I have never written a review for one before. Well, i'll just say that they usually have some celebrities. A wrinkle in time includes only one. Alfre Woodard(Or Woodward, I am not sure), the Oscar winner. <br /><br />The film has cheesy special effects, a mildly interesting plot, scenes that make you go \"WTF\". The movie is incredibly bad and it makes you go\"WTF\". What did I expect? It's a TV movie. They usually aren't good. As is this one. A wrinkle in time is a waste of time and a big time waster. To top it off, you'll most likely forget about it the second it's over. Well, maybe not the second it's over. But within a few minutes.<br /><br />A wrinkle in time:*/**** neg 1123 After watching \"The Bodyguard\" last night, I felt compelled to write a review of it.<br /><br />This could have been a pretty decent movie had it not been for the awful camera-work. It was beyond annoying. The angles were all wrong, it was impossible to see anything, especially during the fight sequences. The closeups were even horrible.<br /><br />The story has Sonny Chiba hiring himself out as a bodyguard to anyone willing to lead him to the top of a drug ring. He is approached by Judy Lee, who is never quite straight with Chiba. Lee's involvement in the drug ring is deeper than Chiba thought, as the Mob and another gang of thugs are after her.<br /><br />The story was decent, and despite horrible dubbing, this could have been a good movie. Given better direction and editing, I'm sure this would have been a classic Kung Foo movie. As it is, it's more like another cheesy 70's action movie.<br /><br />Note: The opening sequence has a quote familiar to \"Pulp Fiction\" fans, and then continues to a karate school in Times Square that is in no way related to the rest of the movie.<br /><br />Rating: 4 out of 10 neg 1123 There are some really terrific ideas in this violent movie that, if executed clearly, could have elevated it from Spaghetti-western blandness into something special. Unfortunately, A TOWN CALLED HELL is one of the worst edited movies imaginable! Scenes start and end abruptly, characters leave for long stretches, the performances (and accents) of the actors are pretty inconsistent, etc.<br /><br />Robert Shaw is a Mexican(!) revolutionary who, after taking part in wiping out a village, stays on to become a priest(!)...ten years later the village is being run by \"mayor\" Telly Salavas. Stella Stevens arrives looking for revenge on the man who killed her husband. Colonel Martin Landau arrives looking for Shaw. They all yell at each other A LOT and they all shoot each other A LOT. Fernando Rey is in it too (as a blind man). The performances aren't bad, but they are mightily uneven. Savalas has an accent sometimes as does Landau (who is really grating here). Shaw and Rey prove that they are incapable of really embarrassing themselves and Stevens looks pretty foxy (if a bit out of place amongst the sweaty filth). neg 1124 The movie is plain bad. Simply awful. The string of bad movies from Bollywood has no end! They must be running out of excuses for making such awful movies (or not).<br /><br />The problem seems to be with mainly the directors. This movie has 2 good actors who have proved in the past that the have the ability to deliver great performance...but they were directed so poorly. The poor script did not help either.<br /><br />This movie has plenty of ridiculous moments and very bad editing in the first half. For instance :<br /><br />After his 1st big concert, Ajay Devgan, meets up with Om Puri (from whom he ran away some 30 years ago and talked to again) and all Om Puri finds to say is to beware of his friendship with Salman!!! What a load of crap. Seriously. Not to mention the baaad soundtrack. Whatever happened to Shankar Ehsaan Loy?<br /><br />Ajay Devgun is total miscast for portraying a rockstar.<br /><br />Only saving grace are the good performances in the second half. Ajay shines as his character shows his dark side. So does Salman as the drug addict. <br /><br />Watch it maybe only for the last half hour.","title":"Compare DataLoaders"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#train-loop-examples","text":"Now let's look at a model training loop would look like. I printed the first 10 batches list of examples lengths to show how nicely they are grouped throughout the dataset! # Example of number of epochs epochs = 1 # Example of loop through each epoch for epoch in range ( epochs ): # Create batches - needs to be called before each loop. torchtext_train_dataloader . create_batches () # Loop through BucketIterator. for sample_id , batch in enumerate ( torchtext_train_dataloader . batches ): print ( 'Batch examples lengths: %s ' . ljust ( 20 ) % str ([ len ( example [ 'text' ]) for example in batch ])) # Let's break early, you get the idea. if sample_id == 10 : break Batch examples lengths: [791, 792, 792, 793, 797, 797, 799, 799, 801, 801] Batch examples lengths: [4823, 4832, 4859, 4895, 4944, 5025, 5150, 5309, 5313, 5450] Batch examples lengths: [695, 696, 696, 696, 697, 699, 699, 700, 700, 701] Batch examples lengths: [960, 961, 963, 963, 963, 966, 966, 967, 968, 969] Batch examples lengths: [1204, 1205, 1208, 1209, 1212, 1214, 1218, 1221, 1226, 1229] Batch examples lengths: [2639, 2651, 2651, 2672, 2692, 2704, 2707, 2712, 2720, 2724] Batch examples lengths: [1815, 1830, 1835, 1838, 1841, 1849, 1852, 1878, 1889, 1895] Batch examples lengths: [3111, 3115, 3133, 3174, 3201, 3206, 3217, 3278, 3294, 3334] Batch examples lengths: [3001, 3031, 3039, 3047, 3056, 3077, 3084, 3103, 3104, 3107] Batch examples lengths: [1053, 1053, 1056, 1057, 1060, 1067, 1073, 1077, 1078, 1080] Batch examples lengths: [751, 751, 756, 758, 759, 760, 761, 762, 763, 764]","title":"Train Loop Examples"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#using-pytorchtext-tabulardataset","text":"Now I will use the TabularDataset functionality which creates the PyTorchDataset object right from our local files. We don't need to create a custom PyTorch Dataset class to load our dataset as long as we have tabular files of our data.","title":"Using PyTorchText TabularDataset"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#data-to-files","text":"Since our dataset is scattered into multiple files, I created a function files_to_tsv which puts our dataset into a .tsv file (Tab-Separated Values). Since I'll use the TabularDataset from pytorch.data I need to pass tabular format files. For text data I find the Tab Separated Values format easier to deal with. I will call the files_to_tsv function for each of the two partitions train and test . The function will return the name of the .tsv file saved so we can use it later in PyTorchText. def files_to_tsv ( partition_path , save_path = './' ): \"\"\"Parse each file in partition and keep track of sentiments. Create a list of pairs [tag, text] Arguments: partition_path (:obj:`str`): Partition used: train or test. save_path (:obj:`str`): Path where to save the final .tsv file. Returns: :obj:`str`: Filename of created .tsv file. \"\"\" # List of all examples in format [tag, text]. examples = [] # Print partition. print ( partition_path ) # Loop through each sentiment. for sentiment in [ 'pos' , 'neg' ]: # Find path for sentiment. sentiment_path = os . path . join ( partition_path , sentiment ) # Get all files from path sentiment. files_names = os . listdir ( sentiment_path ) # For each file in path sentiment. for file_name in tqdm ( files_names , desc = f ' { sentiment } Files' ): # Get file content. file_content = io . open ( os . path . join ( sentiment_path , file_name ), mode = 'r' , encoding = 'utf-8' ) . read () # Fix any format errors. file_content = fix_text ( file_content ) # Append sentiment and file content. examples . append ([ sentiment , file_content ]) # Create a TSV file with same format `sentiment text`. examples = [ \" %s \\t %s \" % ( example [ 0 ], example [ 1 ]) for example in examples ] # Create file name. tsv_filename = os . path . basename ( partition_path ) + '_pos_neg_ %d .tsv' % len ( examples ) # Write to TSV file. io . open ( os . path . join ( save_path , tsv_filename ), mode = 'w' , encoding = 'utf-8' ) . write ( ' \\n ' . join ( examples )) # Return TSV file name. return tsv_filename # Path where to save tsv file. data_path = '/content' # Convert train files to tsv file. train_filename = files_to_tsv ( partition_path = '/content/aclImdb/train' , save_path = data_path ) # Convert test files to tsv file. test_filename = files_to_tsv ( partition_path = '/content/aclImdb/test' , save_path = data_path ) /content/aclImdb/train pos Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [00:34<00:00, 367.26it/s] neg Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [00:21<00:00, 573.00it/s] /content/aclImdb/test pos Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [00:11<00:00, 1075.80it/s] neg Files: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500/12500 [00:12<00:00, 1037.94it/s]","title":"Data to Files"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#tabulardataset","text":"Here I setup the data fields for PyTorchText. We have to tell the library how to handle each column of the .tsv file. For this we need to create data.Field objects for each column. text_tokenizer : For this example I don't use an actual tokenizer for the text column but I need to create one because it requires as input. I created a dummy tokenizer that returns same value. Depending on the project, here is where you will have your own tokenizer. It needs to take as input text and output a list. label_tokenizer The label tokenizer is also a dummy tokenizer. This is where you will have a encoder to transform labels to ids. Since we have two .tsv files it's great that we can use the .split function from TabularDataset to handle two files at the same time one for train and the other one for test. Find more details about torchtext.data functionality here . # Text tokenizer function - dummy tokenizer to return same text. # Here you will use your own tokenizer. text_tokenizer = lambda x : x # Label tokenizer - dummy label encoder that returns same label. # Here you will add your own label encoder. label_tokenizer = lambda x : x # Data field for text column - invoke tokenizer. TEXT = torchtext . data . Field ( sequential = True , tokenize = text_tokenizer , lower = False ) # Data field for labels - invoke tokenize label encoder. LABEL = torchtext . data . Field ( sequential = True , tokenize = label_tokenizer , use_vocab = False ) # Create data fields as tuples of description variable and data field. datafields = [( \"label\" , LABEL ), ( \"text\" , TEXT )] # Since we have have tab separated data we use TabularDataset train_dataset , valid_dataset = torchtext . data . TabularDataset . splits ( # Path to train and validation. path = data_path , # Train data filename. train = train_filename , # Validation file name. validation = test_filename , # Format of local files. format = 'tsv' , # Check if we have header. skip_header = False , # How to handle fields. fields = datafields )","title":"TabularDataset"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#pytorchtext-bucket-iterator-dataloader_1","text":"I'm using same setup as in the PyTorchText Bucket Iterator Dataloader code cell section. The only difference is in the sort_key since there is different way to access example attributes (we had dictionary format before). # Group similar length text sequences together in batches. torchtext_train_dataloader , torchtext_valid_dataloader = torchtext . data . BucketIterator . splits ( # Datasets for iterator to draw data from ( train_dataset , valid_dataset ), # Tuple of train and validation batch sizes. batch_sizes = ( train_batch_size , valid_batch_size ), # Device to load batches on. device = device , # Function to use for sorting examples. sort_key = lambda x : len ( x . text ), # Repeat the iterator for multiple epochs. repeat = True , # Sort all examples in data using `sort_key`. sort = False , # Shuffle data on each epoch run. shuffle = True , # Use `sort_key` to sort examples in each batch. sort_within_batch = True , ) # Print number of batches in each split. print ( 'Created `torchtext_train_dataloader` with %d batches!' % len ( torchtext_train_dataloader )) print ( 'Created `torchtext_valid_dataloader` with %d batches!' % len ( torchtext_valid_dataloader )) Created `torchtext_train_dataloader` with 2500 batches! Created `torchtext_valid_dataloader` with 1250 batches!","title":"PyTorchText Bucket Iterator Dataloader"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#compare-dataloaders_1","text":"Let's compare the PyTorch DataLoader batches with the PyTorchText BucketIterator batches created with TabularDataset. We can see how nicely examples of similar length are grouped in same batch with PyTorchText. Note: When using the PyTorchText BucketIterator, make sure to call create_batches() before looping through each batch! Else you won't get any output form the iterator. # Loop through regular dataloader. print ( 'PyTorch DataLoader \\n ' ) for batch in torch_train_dataloader : # Let's check batch size. print ( 'Batch size: %d \\n ' % len ( batch [ 'text' ])) print ( 'LABEL \\t LENGTH \\t TEXT' . ljust ( 10 )) # Print each example. for text , label in zip ( batch [ 'text' ], batch [ 'label' ]): print ( ' %s \\t %d \\t %s ' . ljust ( 10 ) % ( label , len ( text ), text )) print ( ' \\n ' ) # Only look at first batch. Reuse this code in training models. break # Create batches - needs to be called before each loop. torchtext_train_dataloader . create_batches () # Loop through BucketIterator. print ( 'PyTorchText BuketIterator \\n ' ) for batch in torchtext_train_dataloader . batches : # Let's check batch size. print ( 'Batch size: %d \\n ' % len ( batch )) print ( 'LABEL \\t LENGTH \\t TEXT' . ljust ( 10 )) # Print each example. for example in batch : print ( ' %s \\t %d \\t %s ' . ljust ( 10 ) % ( example . label , len ( example . text ), example . text )) print ( ' \\n ' ) # Only look at first batch. Reuse this code in training models. break PyTorch DataLoader Batch size: 10 LABEL LENGTH TEXT pos 1742 As a child I preferred the first Care Bear movie since this one seemed so dark. I always sat down and watched the first one. As I got older I learned to prefer this one. What I do think is that this film is too dark for infants, but as you get older you learn to treasure it since you understand it more, it doesn't seem as dark as it was back when you were a child.<br /><br />This movie, in my opinion, is better than the first one, everything is so much deeper. It may contradict the first movie but you must ignore the first movie to watch this one. The cubs are just too adorable, I rewind that 'Flying My Colors' scene. I tend to annoy everyone by singing it.<br /><br />The sound track is great! A big hand to Carol and Dean Parks. I love every song in this movie, I have downloaded them all and is all I am listening to, I'm listening to 'Our beginning' also known as 'Recalling' at the moment. I have always preferred this sound track to the first one, although I just totally love Carol Kings song in the first movie 'Care-A-Lot'.<br /><br />I think the animation is great, the animation in both movies are fantastic. I was surprised when I sat down and watched it about 10 years later and saw that the animation for the time was excellent. It was really surprising.<br /><br />There is not a lot of back up from other people to say that this movie is great, but it is. I do not think it is weird/strange. I think it is a wonderful movie.<br /><br />Basically, this movie is about how the Care Bears came about and to defeat the Demon, Dark Heart. The end is surprising and again, beats any 'Pokemon Movie' with the Care Bears Moral issues. It leaves an effect on you. Again this movie can teach everyone at all ages about morality. pos 1475 Worry not, Disney fans--this special edition DVD of the beloved Cinderella won't turn into a pumpkin at the strike of midnight. One of the most enduring animated films of all time, the Disney-fide adaptation of the gory Brothers Grimm fairy tale became a classic in its own right, thanks to some memorable tunes (including \"A Dream Is a Wish Your Heart Makes,\" \"Bibbidi-Bobbidi-Boo,\" and the title song) and some endearingly cute comic relief. The famous slipper (click for larger image) We all know the story--the wicked stepmother and stepsisters simply won't have it, this uppity Cinderella thinking she's going to a ball designed to find the handsome prince an appropriate sweetheart, but perseverance, animal buddies, and a well-timed entrance by a fairy godmother make sure things turn out all right. There are a few striking sequences of pure animation--for example, Cinderella is reflected in bubbles drifting through the air--and the design is rich and evocative throughout. It's a simple story padded here agreeably with comic business, particularly Cinderella's rodent pals (dressed up conspicuously like the dwarf sidekicks of another famous Disney heroine) and their misadventures with a wretched cat named Lucifer. There's also much harrumphing and exposition spouting by the King and the Grand Duke. It's a much simpler and more graceful work than the more frenetically paced animated films of today, which makes it simultaneously quaint and highly gratifying. pos 1279 Seldom do I ever encounter a film so completely fulfilling that I must speak about it immediately. This movie is definitely some of the finest entertainment available and it is highly authentic. I happened to see the dubbed version but I'm on my way right now to grab the DVD remaster with original Chinese dialogue. Still, the dubbing didn't get in the way and sometimes provided some seriously funny humour: \"Poison Clan rocks the world!!!\"<br /><br />The story-telling stays true to Chinese methods of intrigue, suspense, and inter-personal relationships. You can expect twists and turns as the identities of the 5 venoms are revealed and an expert pace.<br /><br />The martial arts fight choreography is in a class of its own and must be seen to be believed. It's like watching real animals fight each other, but construed from their own arcane martial arts forms. Such level of skill amongst the cast is unsurpassed in modern day cinema.<br /><br />The combination provides for a serious dose of old Chinese culture and I recommend it solely on the basis of the film's genuine intent to tell a martial arts story and the mastery of its execution. ...Of course, if you just want to see people pummel each other, along with crude forms of ancient Chinese torture, be my guest! pos 1071 I'm sure that most people already know the story-the miserly Ebenezer Scrooge gets a visit from three spirits (the Ghosts of Christmas Past, Present and Yet to Come) who highlight parts of his life in the hopes of saving his soul and changing his ways. Dickens' classic story in one form or another has stood the test of time to become a beloved holiday favorite.<br /><br />While I grew up watching the 1951 version starring Alastair Sims, and I believe that he is the definitive Scrooge, I have been impressed with this version, which was released when I was in high school. George C. Scott plays a convincing and mean Ebenezer Scrooge, and the actors playing the ghosts are rather frightening and menacing. David Warner is a good Bob Cratchit as well.<br /><br />This version is beautifully filmed, and uses more modern filming styles (for the 1980's) which make it more palatable for my children than the 1951 black and white version.<br /><br />This is a worthy adaptation of the story and is one that I watch almost every year at some point in the Christmas season. neg 876 What was an exciting and fairly original series by Fox has degraded down to meandering tripe. During the first season, Dark Angel was on my weekly \"must see\" list, and not just because of Jessica Alba.<br /><br />Unfortunately, the powers-that-be over at Fox decided that they needed to \"fine-tune\" the plotline. Within 3 episodes of the season opener, they had totally lost me as a viewer (not even to see Jessica Alba!). I found the new characters that were added in the second season to be too ridiculous and amateurish. The new plotlines were stretching the continuity and credibility of the show too thin. On one of the second season episodes, they even had Max sleeping and dreaming - where the first season stated she biologically couldn't sleep.<br /><br />The moral of the story (the one that Hollywood never gets): If it works, don't screw with it!<br /><br />azjazz pos 1981 Greta Garbo's American film debut is an analogy of how our lives can be swept off course by fate and our actions, as in a torrent, causing us to lose a part of ourselves along the way.<br /><br />Greta plays Leonora, a poor peasant girl in love with Ricardo Cortez's character Don Rafael, a landowner. Ricardo is in love with her too, but is too easily influenced by his domineering mother. Leonora ends up homeless and travels to Paris, where she becomes a famous opera singer and develops the reputation for being a loose woman. In reality, part of her attitude is bitterness over Rafael's abandonment.<br /><br />She returns to her home to visit her family and eventually confronts Rafael. Surprisingly, no one knows that she's the famous La Brunna, and Garbo acts up her role as the diva she truly was and re prised with such cool haughtiness in her later portrayals.<br /><br />Ricardo Cortez reminds one a lot of Valentino in looks in this part, and he was groomed to be a Valentino clone by MGM, though he never thought he could be in reality and he was right. He is believable in an unsympathetic part as a weak willed Mama's boy, and allows himself to age realistically but comically at the end of the movie. He fails to win Leonora when she returns home, and later when he follows her, his courage is undermined.<br /><br />This movie is beautifully shot, with brilliant storm sequences and the sets depicting Spain at the time are authentic looking. There are also some fine secondary performances by old timers Lucien Littlefield, Tully Marshall, and Mack Swain.<br /><br />Although this is a story of lost love and missed chances, I don't think Leonora and Rafael would have been happy together, as he needed a more traditional wife and she was very much a career woman, and I don't think would have been happy in a small village. The ending is true to life and pulls no punches.<br /><br />See this one as Garbo's American film debut and a precursor of things to come pos 1007 *What I Like About SPOILERS* Teenager Holly Tyler (Amanda Bynes) goes to live with older sister Valerie (Jennie Garth) to avoid moving to Japan with her father; but she doesn't know the half of the wacky things that will happen to her from now on, and not only to her, but to her sister, her friends Gary (Wesley Jonathan) and Tina (Alison Munn), boyfriend Henry (Michael McMillian), crush Vince (Nick Zano), Valerie's boyfriend Jeff (Simon Rex), first boss (then firefighter then husband) Vic (Dan Cortese), annoying colleague Lauren (Leslie Grossman) and second boss Peter (?) If you don't have a funny bone in your body, please skip this; if you like only veeeery sophisticated comedy this isn't for you; if you like a funny, sometimes touching show with two hot chicks who can act in the lead (and none other than the fabulous 'Mary Cherry' from Popular - Leslie Grossman - in the main cast), then what the hell are you waiting for? You're welcome to Casa De Tyler! What I Like About You (2002-2006): 8. pos 318 This movie is wonderful. The writing, directing, acting all are fantastic. Very witty and clever script. Quality performances by actors, Ally Sheedy is strong and dynamic and delightfully quirky. Really original and heart-warmingly unpredicatable. The scenes are alive with fresh energy and really talented production. pos 1846 In Le Million, Rene Clair, one of the cinema's great directors and great pioneers, created a gem of light comedy which for all its lightness is a groundbreaking and technically brilliant film which clearly influenced subsequent film-makers such as the Marx Brothers, Lubitsch, and Mamoulian. The plot, a witty story of a poor artist who wins a huge lottery jackpot but has to search frantically all over town for the missing ticket, is basically just a device to support a series of wonderfully witty comic scenes enacted in a dream world of the director's imagination.<br /><br />One of the most impressive things about this film is that, though it is set in the middle of Paris and includes nothing actually impossible, it achieves a sustained and involving fairy-tale/fantasy atmosphere, in which it seems quite natural that people sing as much as they talk, or that a tussle over a stolen jacket should take on the form of a football game. Another memorable element is that Le Million includes what may be the funniest opera ever put on film (O that blonde-braided soprano! \"I laugh, ha! ha!\") Also a delight is the casting: Clair has assembled a group of amazing, sharply different character actors, each of them illustrating with deadly satiric accuracy a bourgeois French \"type,\" so that the film seems like a set of Daumier prints come to life.<br /><br />The hilarity takes a little while to get rolling, and I found the characters not as emotionally engaging as they can be even in a light comedy (as they are, for instance, in many Lubitsch films.) For these reasons I refrained from giving it the highest rating. But these minor cavils shouldn't distract from an enthusiastic recommendation.<br /><br />Should you see it? By all means. Highly recommended whether you want a classic and influential work of cinema or just a fun comedy. pos 1260 Before I comment about this movie, you should realize that when I saw this movie, I expected the typical crap, horror, B-movie and just wanted to have fun. Jack Frost is one that not only delivers but is actually one of the best that I've seen in a long time. Scott McDonald is great as Jack Frost, in fact I think he has a future in being psychopaths in big time movies if ever given the chance. McDonald is a serial killer who becomes a snowman through some stupid accidental mix of ridiculous elements. As soon as that snowman starts moving around and killing people, though, you will find it hard not to laugh. The lines that are said are completely retarded but really funny. The fact that the rest of the cast completely over-acts just adds to stupidity of the film, but it's stupidity is it's genius. The scene where the snowman is with the teenage girl is truly classic in B-movie, horror film fashion. I truly hope there is a sequel and I'll be right there to watch it on whatever cable channel does it. Of course it's only fun to watch the first few times and it's not exactly a good work of motion picture technology, but I just like to see snowmen kill people. I gave it a 7 out of 10, this is a great movie for dates and couples in the late hours. PyTorchText BuketIterator Batch size: 10 LABEL LENGTH TEXT neg 1118 Most college students find themselves lost in the bubble of academia, cut off from the communities in which they study and live. Their conversations are held with their fellow students and the college faculty. Steven Greenstreet's documentary is a prime example of a disillusioned college student who judges the entire community based on limited contact with a small number of its members.<br /><br />The documentary focused on a small group of individuals who were portrayed as representing large groups of the population. As is usual, the people who scream the most get the most media attention. Other than its misrepresentation of the community in which the film was set, the documentary was well made. My only dispute is that the feelings and uproar depicted in the film were attributed to the entire community rather than the few individuals who expressed them.<br /><br />Naturally it is important to examine a controversy like this and make people aware of the differences that exist between political viewpoints, but it is ridiculous to implicate an entire community of people in the actions of a few radicals. neg 1120 Looked forward to viewing this film and seeing these great actors perform. However, I was sadly disappointed in the script and the entire plot of the story. David Duchovny,(Dr. Eugene Sands),\"Connie & Carla\",'04, was the doctor in the story who uses drugs and losses his license to practice medicine. Dr. Sands was visiting a night club and was able to use his medical experience to help a wounded customer and was assisted by Angelina Jolie,(Claire),\"Taking Lives\",'04, who immediately becomes attracted to Dr. David Sands. Timothy Hutton,(Raymond Blossom),\"Kinsey\",'04, plays the Big Shot Gangster and a man with all kinds of money and connections. Timothy Hutton seems to over act in most of the scenes and goes completely out of his mind trying to keep his gang members from being killed. Gary Dourdan,(Yates),\"CSI-Vegas TV Series\", plays a great supporting role and portrays a real COOL DUDE who is a so-called body guard for Raymond Blossom. Angelina Jolie looks beautiful and sexy with her ruby red lips which draws a great deal of attention from all the men. This film is not the greatest, but it does entertain. pos 1120 I must say that, looking at Hamlet from the perspective of a student, Brannagh's version of Hamlet is by far the best. His dedication to stay true to the original text should be applauded. It helps the play come to life on screen, and makes it easier for people holding the text while watching, as we did while studying it, to follow and analyze the text.<br /><br />One of the things I have heard criticized many times is the casting of major Hollywood names in the play. I find that this helps viewers recognize the characters easier, as opposed to having actors that all look and sound the same that aid in the confusion normally associated with Shakespeare.<br /><br />Also, his flashbacks help to clear up many ambiguities in the text. Such as how far the relationship between Hamlet and Ophelia really went and why Fortinbras just happened to be at the castle at the end. All in all, not only does this version contain some brilliant performances by actors both familiar and not familiar with Shakespeare. It is presented in a way that one does not have to be an English Literature Ph.D to understand and enjoy it. pos 1120 As a baseball die-hard, this movie goes contrary to what I expect in a sports movie: authentic-looking sports action, believable characters, and an original story line. While \"Angels in the Outfield\" fails miserably in the first category, it succeeds beautifully in the latter two. \"Angels\" weaves the story of Roger and J.P., two Anaheim foster kids in love with baseball but searching for a family, with that of the woebegone Angels franchise, struggling to draw fans and win games. Pushed by his deadbeat father's promise that they would be a family only when the Angels win the pennant, Roger asks for some heavenly help, and gets it in the form of diamond-dwelling spirits bent on reversing the franchise's downward spiral. And, when short-fused manager George Knox (portrayed by Danny Glover) begins believing in what Roger sees, the team suddenly has hope for turning their season around--and Roger and J.P. find something to believe in. Glover in particular gives a nice performance, and Tony Danza, playing a washed-up pitcher, also does well, despite clearly having ZERO idea of how to pitch out of the windup! neg 1121 I have a piece of advice for the people who made this movie too, if you're gonna make a movie like this be sure you got the f/x to back it up. Also don't get a bunch of z list actors to play in it. Another thing, just about all of us have seen Jurassic Park, so don't blatantly copy it. All in all this movie sucked, f/x sucked, acting sucked, story unoriginal. Let's talk about the acting for just a second, the Carradine guy who's career peaked in 1984 when he did \"Revenge of the Nerds\" (which was actually a great comedy). He's not exactly z list, he can act. He just should have said no to this s--t bag. He should have did what Mark Hamill did after \"Return of the Jedi\" and go quietly into the night. He made his mark as a \"Nerd\" and that should have been that. I understand he has bills to pay, but that hardly excuses this s--t bag. Have I called this movie that yet? O.K. I just wanted to be sure. If I sound a little hostile, I apologize. I just wasted 2hrs of my life I could have spent doing something productive like watching paint peel, and I feel cheated. I'll close on that note. Thank you for your time. neg 1121 By 1941 Columbia was a full-fledged major studio and could produce a movie with the same technical polish as MGM, Paramount or Warners. That's the best thing that could be said about \"Adam Had Four Sons,\" a leaden soap opera with almost terminally bland performances by Ingrid Bergman (top-billed for the first time in an American film) and Warner Baxter. Bergman plays a Frenchwoman (this was the era in which Hollywood thought one foreign accent was as good as another) hired as governess to Baxter's four sons and staying on (with one interruption caused by the stock-market crash of 1907) until the boys are grown men serving in World War I. Just about everyone in the movie is so goody-good it's a relief when Susan Hayward as the villainess enters midway through \u2014 she's about the only watchable person in the movie even though she's clearly channeling Bette Davis and Vivien Leigh; it's also the first in her long succession of alcoholic roles \u2014 but the script remains saccharine and the ending is utterly preposterous. No wonder Bergman turned down the similarly plotted \"The Valley of Decision\" four years later. neg 1123 I have never read the book\"A wrinkle in time\". To be perfectly honesty, after seeing the movie, do I really want to? Well, I shouldn't be reviewing this movie i'll start off with that. Next i'll say that the TV movie is pretty forgettable. Do you know why I say that? Because I forgot what happens in it. I told you it was forgettable. To be perfectly honest, no TV movie will ever be better than \"Merlin\".<br /><br />How do I describe a TV movie? I have never written a review for one before. Well, i'll just say that they usually have some celebrities. A wrinkle in time includes only one. Alfre Woodard(Or Woodward, I am not sure), the Oscar winner. <br /><br />The film has cheesy special effects, a mildly interesting plot, scenes that make you go \"WTF\". The movie is incredibly bad and it makes you go\"WTF\". What did I expect? It's a TV movie. They usually aren't good. As is this one. A wrinkle in time is a waste of time and a big time waster. To top it off, you'll most likely forget about it the second it's over. Well, maybe not the second it's over. But within a few minutes.<br /><br />A wrinkle in time:*/**** neg 1123 After watching \"The Bodyguard\" last night, I felt compelled to write a review of it.<br /><br />This could have been a pretty decent movie had it not been for the awful camera-work. It was beyond annoying. The angles were all wrong, it was impossible to see anything, especially during the fight sequences. The closeups were even horrible.<br /><br />The story has Sonny Chiba hiring himself out as a bodyguard to anyone willing to lead him to the top of a drug ring. He is approached by Judy Lee, who is never quite straight with Chiba. Lee's involvement in the drug ring is deeper than Chiba thought, as the Mob and another gang of thugs are after her.<br /><br />The story was decent, and despite horrible dubbing, this could have been a good movie. Given better direction and editing, I'm sure this would have been a classic Kung Foo movie. As it is, it's more like another cheesy 70's action movie.<br /><br />Note: The opening sequence has a quote familiar to \"Pulp Fiction\" fans, and then continues to a karate school in Times Square that is in no way related to the rest of the movie.<br /><br />Rating: 4 out of 10 neg 1123 There are some really terrific ideas in this violent movie that, if executed clearly, could have elevated it from Spaghetti-western blandness into something special. Unfortunately, A TOWN CALLED HELL is one of the worst edited movies imaginable! Scenes start and end abruptly, characters leave for long stretches, the performances (and accents) of the actors are pretty inconsistent, etc.<br /><br />Robert Shaw is a Mexican(!) revolutionary who, after taking part in wiping out a village, stays on to become a priest(!)...ten years later the village is being run by \"mayor\" Telly Salavas. Stella Stevens arrives looking for revenge on the man who killed her husband. Colonel Martin Landau arrives looking for Shaw. They all yell at each other A LOT and they all shoot each other A LOT. Fernando Rey is in it too (as a blind man). The performances aren't bad, but they are mightily uneven. Savalas has an accent sometimes as does Landau (who is really grating here). Shaw and Rey prove that they are incapable of really embarrassing themselves and Stevens looks pretty foxy (if a bit out of place amongst the sweaty filth). neg 1124 The movie is plain bad. Simply awful. The string of bad movies from Bollywood has no end! They must be running out of excuses for making such awful movies (or not).<br /><br />The problem seems to be with mainly the directors. This movie has 2 good actors who have proved in the past that the have the ability to deliver great performance...but they were directed so poorly. The poor script did not help either.<br /><br />This movie has plenty of ridiculous moments and very bad editing in the first half. For instance :<br /><br />After his 1st big concert, Ajay Devgan, meets up with Om Puri (from whom he ran away some 30 years ago and talked to again) and all Om Puri finds to say is to beware of his friendship with Salman!!! What a load of crap. Seriously. Not to mention the baaad soundtrack. Whatever happened to Shankar Ehsaan Loy?<br /><br />Ajay Devgun is total miscast for portraying a rockstar.<br /><br />Only saving grace are the good performances in the second half. Ajay shines as his character shows his dark side. So does Salman as the drug addict. <br /><br />Watch it maybe only for the last half hour.","title":"Compare DataLoaders"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#train-loop-examples_1","text":"Now let's look at a model training loop would look like. I printed the first 10 batches list of examples lengths to show how nicely they are grouped throughout the dataset! We see that we get same exact behavior as we did when using PyTorch Dataset. Now it depends on which way is easier for you to use PyTorchText BucketIterator: with PyTorch Dataset or with PyTorchText TabularDataset # Example of number of epochs. epochs = 1 # Example of loop through each epoch. for epoch in range ( epochs ): # Create batches - needs to be called before each loop. torchtext_train_dataloader . create_batches () # Loop through BucketIterator. for sample_id , batch in enumerate ( torchtext_train_dataloader . batches ): # Put all example.text of batch in single array. batch_text = [ example . text for example in batch ] print ( 'Batch examples lengths: %s ' . ljust ( 20 ) % str ([ len ( text ) for text in batch_text ])) # Let's break early, you get the idea. if sample_id == 10 : break Batch examples lengths: [791, 791, 792, 792, 793, 797, 797, 799, 799, 801] Batch examples lengths: [4766, 4823, 4832, 4859, 4895, 4944, 5025, 5150, 5309, 5313] Batch examples lengths: [695, 695, 696, 696, 696, 697, 699, 699, 699, 700] Batch examples lengths: [958, 959, 960, 961, 963, 963, 963, 966, 966, 967] Batch examples lengths: [1200, 1203, 1204, 1205, 1208, 1209, 1212, 1214, 1218, 1221] Batch examples lengths: [2621, 2628, 2639, 2651, 2651, 2672, 2690, 2704, 2705, 2712] Batch examples lengths: [1811, 1812, 1815, 1830, 1835, 1838, 1841, 1849, 1852, 1878] Batch examples lengths: [3104, 3107, 3111, 3115, 3133, 3174, 3201, 3206, 3217, 3278] Batch examples lengths: [3000, 3001, 3001, 3031, 3039, 3047, 3056, 3075, 3084, 3103] Batch examples lengths: [1046, 1050, 1053, 1053, 1054, 1057, 1060, 1067, 1073, 1077] Batch examples lengths: [749, 751, 751, 756, 758, 759, 760, 761, 762, 763]","title":"Train Loop Examples"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#final-note","text":"If you made it this far Congrats! \ud83c\udf8a and Thank you! \ud83d\ude4f for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you see something wrong please let me know by opening an issue on my ml_things GitHub repository ! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.","title":"Final Note"},{"location":"tutorial_notebooks/pytorchtext_bucketiterator/#contact-","text":"\ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"Contact \ud83c\udfa3"},{"location":"tutorial_notebooks/tutorial_template_page/","text":"Title Work in progress Info Intro to this tutorial What should I know for this notebook? Any requirements. How to use this notebook? Instructions. What ? Tutorial specific answer. Dataset I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with. Coding Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations. Downloads Download the Large Movie Review Dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Installs transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. Give it a try! Code Cell: # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Output: Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .9MB 6 .7MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 890kB 48 .9MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 .1MB 49 .0MB/s Building wheel for transformers ( PEP 517 ) ... done Building wheel for sacremoses ( setup.py ) ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 5 .2MB/s Building wheel for ml-things ( setup.py ) ... done Building wheel for ftfy ( setup.py ) ... done Imports Import all needed libraries for this notebook. Declare parameters used for this notebook: * * Code Cell: Helper Functions Class() / function() Class / function description. Code Cell: Load Model and Tokenizer Loading the three essential parts of the pretrained transformers: configuration , tokenizer and model . I also need to load the model on the device I'm planning to use (GPU / CPU). Code Cell: Output: Dataset and DataLoader Details. Code Cell: Output: Train Code Cell: Output: Use ColabImage plots straight in here Evaluate Evaluation! Code Cell: Output: Use ColabImage plots straight in here Final Note If you made it this far Congrats and Thank you for your interest in my tutorial! Other details. If you see something wrong please let me know by opening an issue on my ml_things GitHub repository! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can. Contact GitHub: gmihaila Website: gmihaila.github.io LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila Thank you! Find out more About Me .","title":"**:gear: Title**"},{"location":"tutorial_notebooks/tutorial_template_page/#title","text":"","title":"Title"},{"location":"tutorial_notebooks/tutorial_template_page/#work-in-progress","text":"","title":"Work in progress"},{"location":"tutorial_notebooks/tutorial_template_page/#info","text":"Intro to this tutorial","title":"Info"},{"location":"tutorial_notebooks/tutorial_template_page/#what-should-i-know-for-this-notebook","text":"Any requirements.","title":"What should I know for this notebook?"},{"location":"tutorial_notebooks/tutorial_template_page/#how-to-use-this-notebook","text":"Instructions.","title":"How to use this notebook?"},{"location":"tutorial_notebooks/tutorial_template_page/#what-","text":"Tutorial specific answer.","title":"What ?"},{"location":"tutorial_notebooks/tutorial_template_page/#dataset","text":"I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with.","title":"Dataset"},{"location":"tutorial_notebooks/tutorial_template_page/#coding","text":"Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.","title":"Coding"},{"location":"tutorial_notebooks/tutorial_template_page/#downloads","text":"Download the Large Movie Review Dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz","title":"Downloads"},{"location":"tutorial_notebooks/tutorial_template_page/#installs","text":"transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. Give it a try! Code Cell: # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Output: Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .9MB 6 .7MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 890kB 48 .9MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 .1MB 49 .0MB/s Building wheel for transformers ( PEP 517 ) ... done Building wheel for sacremoses ( setup.py ) ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 5 .2MB/s Building wheel for ml-things ( setup.py ) ... done Building wheel for ftfy ( setup.py ) ... done","title":"Installs"},{"location":"tutorial_notebooks/tutorial_template_page/#imports","text":"Import all needed libraries for this notebook. Declare parameters used for this notebook: * * Code Cell:","title":"Imports"},{"location":"tutorial_notebooks/tutorial_template_page/#helper-functions","text":"Class() / function() Class / function description. Code Cell:","title":"Helper Functions"},{"location":"tutorial_notebooks/tutorial_template_page/#load-model-and-tokenizer","text":"Loading the three essential parts of the pretrained transformers: configuration , tokenizer and model . I also need to load the model on the device I'm planning to use (GPU / CPU). Code Cell: Output:","title":"Load Model and Tokenizer"},{"location":"tutorial_notebooks/tutorial_template_page/#dataset-and-dataloader","text":"Details. Code Cell: Output:","title":"Dataset and DataLoader"},{"location":"tutorial_notebooks/tutorial_template_page/#train","text":"Code Cell: Output: Use ColabImage plots straight in here","title":"Train"},{"location":"tutorial_notebooks/tutorial_template_page/#evaluate","text":"Evaluation! Code Cell: Output: Use ColabImage plots straight in here","title":"Evaluate"},{"location":"tutorial_notebooks/tutorial_template_page/#final-note","text":"If you made it this far Congrats and Thank you for your interest in my tutorial! Other details. If you see something wrong please let me know by opening an issue on my ml_things GitHub repository! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.","title":"Final Note"},{"location":"tutorial_notebooks/tutorial_template_page/#contact","text":"GitHub: gmihaila Website: gmihaila.github.io LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila Thank you! Find out more About Me .","title":"Contact"},{"location":"useful/useful/","text":"Useful Code Here is where I put Python code snippets that I use in my Machine Learning research work. I'm using this page to have code easily accessible and to be able to share it with others. Tip : use Table of contents on the top-right side of the page to avoid endless scrolling, and is a good idea to use Copy to clipboard button on the upper right corner of each code cell to get things done quickly. Read FIle One liner to read any file: io . open ( \"my_file.txt\" , mode = 'r' , encoding = 'utf-8' ) . read () Details: import io Write File One liner to write a string to a file: io . open ( \"my_file.txt\" , mode = 'w' , encoding = 'utf-8' ) . write ( \"Your text!\" ) Details: import io Debug Start debugging after this line: import pdb ; pdb . set_trace () Details: use dir() to see all current variables, locals() to see variables and their values and globals() to see all global variables with values. Pip Install GitHub Install library directly from GitHub using pip: pip install git+github_url Details: add @version_number at the end to use a certain version to install. Parse Argument Parse arguments given when running a .py file. parser = argparse . ArgumentParser ( description = 'Description' ) parser . add_argument ( '--argument' , help = 'Help me.' , type = str ) # parse arguments args = parser . parse_args () Details: import argparse and use python script.py --argument something when running script. Doctest How to run a simple unittesc using function documentaiton. Useful when need to do unittest inside notebook: # function to test def add ( a , b ): ''' >>> add(2, 2) 5 ''' return a + b # run doctest import doctest doctest . testmod ( verbose = True ) Details: ml_things Fix Text I use this package everytime I read text data from a source I don't trust. Since text data is always messy, I always use it. It is great in fixing any bad Unicode. fix_text ( text = \"Text to be fixed\" ) Details: Install it pip install ftfy and import it from ftfy import fix_text . Current Date How to get current date in Python. I use this when need to name log files: from datetime import date today = date . today () # dd/mm/YY in string format today . strftime ( \" %d /%m/%Y\" ) Details: More details here Current Time Get current time in Python: from datetime import datetime # datetime object containing current date and time now = datetime . now () # dd/mm/YY H:M:S now . strftime ( \" %d /%m/%Y %H:%M:%S\" ) Details: More details here Remove Punctuation The fastest way to remove punctuation in Python3: table = str . maketrans ( dict . fromkeys ( string . punctuation )) \"string. With. Punctuation?\" . translate ( table ) Details: Import string . Code adapted from StackOverflow Remove punctuation from Unicode formatted strings . Class Instances from Dictionary Create class instances from dictionary. Very handy when working with notebooks and need to pass arguments as class instances. # Create dictionary of arguments. my_args = dict ( argument_one = 23 , argument_two = False ) # Convert dicitonary to class instances. my_args = type ( 'allMyArguments' , ( object ,), my_args ) Details: Code adapted from StackOverflow Creating class instance properties from a dictionary? List of Lists into Flat List Given a list of lists convert it to a single flat size list. It is the fasest way to conserve each elemnt type. l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ], [ 7 ], [ 8 , 9 ], [ 'this' , 'is' ]] functools . reduce ( operator . concat , l ) Details: Import operator, functools . Code adapted from StackOverflow How to make a flat list out of list of lists? Pickle and Unpickle Save python objects into binary using pickle. Load python objects from binary files using pickle. a = { 'hello' : 'world' } with open ( 'filename.pickle' , 'wb' ) as handle : pickle . dump ( a , handle , protocol = pickle . HIGHEST_PROTOCOL ) with open ( 'filename.pickle' , 'rb' ) as handle : b = pickle . load ( handle ) Details: Import pickle . Code adapted from StackOverflow How can I use pickle to save a dict? Notebook Input Variables How to ask user for input value to a variable. In the case of a password variable how to ask for a password variable. from getpass import getpass # Populate variables from user inputs. user = input ( 'User name: ' ) password = getpass ( 'Password: ' ) Details: Code adapted from StackOverflow Methods for using Git with Google Colab Notebook Clone private Repository GitHub How to clone a private repo. Will need to login and ask for password. This snippet can be ran multiple times because it first check if the repo was cloned already. import os from getpass import getpass # Repository name. repo = 'gmihaila/ml_things' # Remove .git extension if present. repo = repo [: - 4 ] if '.git' in repo else repo # Check if repo wasn't already cloned if not os . path . isdir ( os . path . join ( '/content' , os . path . basename ( repo ))): # Use GitHub username. u = input ( 'GitHub username: ' ) # Ask user for GitHub password. p = getpass ( 'GitHub password: ' ) # Clone repo. ! git clone https : // $ u : $ p @github . com / $ repo # Remove password variable. p = '' else : # Make sure repo is up to date - pull. ! git - C / content / dialogue_dataset pull Details: Code adapted from StackOverflow Methods for using Git with Google Colab Import Module Given Path How to import a module from a local path. Make it act as a installed library. import sys # Append module path. sys . path . append ( '/path/to/module' ) Details: After that we can use import module.stuff . Code adapted from StackOverflow Adding a path to sys.path (over using imp) . PyTorch Code snippets related to PyTorch : Dataset Code sample on how to create a PyTorch Dataset. The __len__(self) function needs to return the number of examples in your dataset and _getitem__(self,item) will use the index item to select an example from your dataset: from torch.utils.data import Dataset , DataLoader class PyTorchDataset ( Dataset ): \"\"\"PyTorch Dataset. \"\"\" def __init__ ( self ,): return def __len__ ( self ): return def __getitem__ ( self , item ): return # create pytorch dataset pytorch_dataset = PyTorchDataset () # move pytorch dataset into dataloader pytorch_dataloader = DataLoader ( pytorch_dataset , batch_size = 32 , shuffle = True ) Details: Find more details here PyTorch Device How to setup device in PyTorch to detect if GPU is available. If there is no GPU available it will default to CPU. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) Details: Adapted from Stack Overflow How to check if pytorch is using the GPU? .","title":"Useful Code"},{"location":"useful/useful/#useful-code","text":"Here is where I put Python code snippets that I use in my Machine Learning research work. I'm using this page to have code easily accessible and to be able to share it with others. Tip : use Table of contents on the top-right side of the page to avoid endless scrolling, and is a good idea to use Copy to clipboard button on the upper right corner of each code cell to get things done quickly.","title":"Useful Code"},{"location":"useful/useful/#read-file","text":"One liner to read any file: io . open ( \"my_file.txt\" , mode = 'r' , encoding = 'utf-8' ) . read () Details: import io","title":"Read FIle"},{"location":"useful/useful/#write-file","text":"One liner to write a string to a file: io . open ( \"my_file.txt\" , mode = 'w' , encoding = 'utf-8' ) . write ( \"Your text!\" ) Details: import io","title":"Write File"},{"location":"useful/useful/#debug","text":"Start debugging after this line: import pdb ; pdb . set_trace () Details: use dir() to see all current variables, locals() to see variables and their values and globals() to see all global variables with values.","title":"Debug"},{"location":"useful/useful/#pip-install-github","text":"Install library directly from GitHub using pip: pip install git+github_url Details: add @version_number at the end to use a certain version to install.","title":"Pip Install GitHub"},{"location":"useful/useful/#parse-argument","text":"Parse arguments given when running a .py file. parser = argparse . ArgumentParser ( description = 'Description' ) parser . add_argument ( '--argument' , help = 'Help me.' , type = str ) # parse arguments args = parser . parse_args () Details: import argparse and use python script.py --argument something when running script.","title":"Parse Argument"},{"location":"useful/useful/#doctest","text":"How to run a simple unittesc using function documentaiton. Useful when need to do unittest inside notebook: # function to test def add ( a , b ): ''' >>> add(2, 2) 5 ''' return a + b # run doctest import doctest doctest . testmod ( verbose = True ) Details: ml_things","title":"Doctest"},{"location":"useful/useful/#fix-text","text":"I use this package everytime I read text data from a source I don't trust. Since text data is always messy, I always use it. It is great in fixing any bad Unicode. fix_text ( text = \"Text to be fixed\" ) Details: Install it pip install ftfy and import it from ftfy import fix_text .","title":"Fix Text"},{"location":"useful/useful/#current-date","text":"How to get current date in Python. I use this when need to name log files: from datetime import date today = date . today () # dd/mm/YY in string format today . strftime ( \" %d /%m/%Y\" ) Details: More details here","title":"Current Date"},{"location":"useful/useful/#current-time","text":"Get current time in Python: from datetime import datetime # datetime object containing current date and time now = datetime . now () # dd/mm/YY H:M:S now . strftime ( \" %d /%m/%Y %H:%M:%S\" ) Details: More details here","title":"Current Time"},{"location":"useful/useful/#remove-punctuation","text":"The fastest way to remove punctuation in Python3: table = str . maketrans ( dict . fromkeys ( string . punctuation )) \"string. With. Punctuation?\" . translate ( table ) Details: Import string . Code adapted from StackOverflow Remove punctuation from Unicode formatted strings .","title":"Remove Punctuation"},{"location":"useful/useful/#class-instances-from-dictionary","text":"Create class instances from dictionary. Very handy when working with notebooks and need to pass arguments as class instances. # Create dictionary of arguments. my_args = dict ( argument_one = 23 , argument_two = False ) # Convert dicitonary to class instances. my_args = type ( 'allMyArguments' , ( object ,), my_args ) Details: Code adapted from StackOverflow Creating class instance properties from a dictionary?","title":"Class Instances from Dictionary"},{"location":"useful/useful/#list-of-lists-into-flat-list","text":"Given a list of lists convert it to a single flat size list. It is the fasest way to conserve each elemnt type. l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ], [ 7 ], [ 8 , 9 ], [ 'this' , 'is' ]] functools . reduce ( operator . concat , l ) Details: Import operator, functools . Code adapted from StackOverflow How to make a flat list out of list of lists?","title":"List of Lists into Flat List"},{"location":"useful/useful/#pickle-and-unpickle","text":"Save python objects into binary using pickle. Load python objects from binary files using pickle. a = { 'hello' : 'world' } with open ( 'filename.pickle' , 'wb' ) as handle : pickle . dump ( a , handle , protocol = pickle . HIGHEST_PROTOCOL ) with open ( 'filename.pickle' , 'rb' ) as handle : b = pickle . load ( handle ) Details: Import pickle . Code adapted from StackOverflow How can I use pickle to save a dict?","title":"Pickle and Unpickle"},{"location":"useful/useful/#notebook-input-variables","text":"How to ask user for input value to a variable. In the case of a password variable how to ask for a password variable. from getpass import getpass # Populate variables from user inputs. user = input ( 'User name: ' ) password = getpass ( 'Password: ' ) Details: Code adapted from StackOverflow Methods for using Git with Google Colab","title":"Notebook Input Variables"},{"location":"useful/useful/#notebook-clone-private-repository-github","text":"How to clone a private repo. Will need to login and ask for password. This snippet can be ran multiple times because it first check if the repo was cloned already. import os from getpass import getpass # Repository name. repo = 'gmihaila/ml_things' # Remove .git extension if present. repo = repo [: - 4 ] if '.git' in repo else repo # Check if repo wasn't already cloned if not os . path . isdir ( os . path . join ( '/content' , os . path . basename ( repo ))): # Use GitHub username. u = input ( 'GitHub username: ' ) # Ask user for GitHub password. p = getpass ( 'GitHub password: ' ) # Clone repo. ! git clone https : // $ u : $ p @github . com / $ repo # Remove password variable. p = '' else : # Make sure repo is up to date - pull. ! git - C / content / dialogue_dataset pull Details: Code adapted from StackOverflow Methods for using Git with Google Colab","title":"Notebook Clone private Repository GitHub"},{"location":"useful/useful/#import-module-given-path","text":"How to import a module from a local path. Make it act as a installed library. import sys # Append module path. sys . path . append ( '/path/to/module' ) Details: After that we can use import module.stuff . Code adapted from StackOverflow Adding a path to sys.path (over using imp) .","title":"Import Module Given Path"},{"location":"useful/useful/#pytorch","text":"Code snippets related to PyTorch :","title":"PyTorch"},{"location":"useful/useful/#dataset","text":"Code sample on how to create a PyTorch Dataset. The __len__(self) function needs to return the number of examples in your dataset and _getitem__(self,item) will use the index item to select an example from your dataset: from torch.utils.data import Dataset , DataLoader class PyTorchDataset ( Dataset ): \"\"\"PyTorch Dataset. \"\"\" def __init__ ( self ,): return def __len__ ( self ): return def __getitem__ ( self , item ): return # create pytorch dataset pytorch_dataset = PyTorchDataset () # move pytorch dataset into dataloader pytorch_dataloader = DataLoader ( pytorch_dataset , batch_size = 32 , shuffle = True ) Details: Find more details here","title":"Dataset"},{"location":"useful/useful/#pytorch-device","text":"How to setup device in PyTorch to detect if GPU is available. If there is no GPU available it will default to CPU. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) Details: Adapted from Stack Overflow How to check if pytorch is using the GPU? .","title":"PyTorch Device"}]}