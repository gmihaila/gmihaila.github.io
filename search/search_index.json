{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About me George is a PhD candidate at University of North Texas, in the Department of Computer Science. He received his Bachelors Degree in Electrical Engineering in his home-country, Romania, and he got his Masters Degree in Computer Science in the United States. He worked three summers for State Farm as a Data Scientist (DS) and Machine Learning (ML) Engineer, and he has more than 4 years of experience in Natural Language Processing (NLP), Computer Vision (CV), Deep Learning (DL) and Reinforcement Learning (RL). He is currently working towards his doctoral thesis in casual dialog generation with persona. Competencies: data science, machine learning, deep learning, high performance computing, Tensorflow2.0, PyTorch, Python, R. Current Position Teaching Assistant Computer Science | University of North Texas August 2020 \u2013 Present Artificial Intelligence \u2013 Machine Learning Researcher University of North Texas (UNT) | Department of Computer Science Jan 2017 - Present Reading How Not to Be Wrong: The Power of Mathematical Thinking by Jordan Ellenberg Contact GitHub: gmihaila LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila","title":"About"},{"location":"#about-me","text":"George is a PhD candidate at University of North Texas, in the Department of Computer Science. He received his Bachelors Degree in Electrical Engineering in his home-country, Romania, and he got his Masters Degree in Computer Science in the United States. He worked three summers for State Farm as a Data Scientist (DS) and Machine Learning (ML) Engineer, and he has more than 4 years of experience in Natural Language Processing (NLP), Computer Vision (CV), Deep Learning (DL) and Reinforcement Learning (RL). He is currently working towards his doctoral thesis in casual dialog generation with persona. Competencies: data science, machine learning, deep learning, high performance computing, Tensorflow2.0, PyTorch, Python, R.","title":"About me"},{"location":"#current-position","text":"","title":"Current Position"},{"location":"#teaching-assistant","text":"Computer Science | University of North Texas August 2020 \u2013 Present","title":"Teaching Assistant"},{"location":"#artificial-intelligence--machine-learning-researcher","text":"University of North Texas (UNT) | Department of Computer Science Jan 2017 - Present","title":"Artificial Intelligence \u2013 Machine Learning Researcher"},{"location":"#reading","text":"How Not to Be Wrong: The Power of Mathematical Thinking by Jordan Ellenberg","title":"Reading"},{"location":"#contact","text":"GitHub: gmihaila LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila","title":"Contact"},{"location":"stuff/","text":"Welcome to Talon Hight-Performance Computing Documentation BTW this is mobile friendly and easy to deploy on GitHub Pages http://UNT-RITS.github.io/talon_sop/_build/html/index.html Task List Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Bundle code together Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } } Python Starter Content import tensorflow as tf import os print ( \"Hello world\" ) import tensorflow as tf import os print ( \"Hello world\" ) Formulas \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons \u2013 that's not all, we can also use GitHub's Octicons Foot Note Lorem ipsum 1 dolor sit amet R Starter Content Job Submit Content Tables Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Welcome to Talon Hight-Performance Computing Documentation"},{"location":"stuff/#welcome-to-talon-hight-performance-computing-documentation","text":"BTW this is mobile friendly and easy to deploy on GitHub Pages http://UNT-RITS.github.io/talon_sop/_build/html/index.html","title":"Welcome to Talon Hight-Performance Computing Documentation"},{"location":"stuff/#task-list","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Task List"},{"location":"stuff/#bundle-code-together","text":"Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } }","title":"Bundle code together"},{"location":"stuff/#python-starter","text":"","title":"Python Starter"},{"location":"stuff/#content","text":"import tensorflow as tf import os print ( \"Hello world\" ) import tensorflow as tf import os print ( \"Hello world\" )","title":"Content"},{"location":"stuff/#formulas","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons \u2013 that's not all, we can also use GitHub's Octicons","title":"Formulas"},{"location":"stuff/#foot-note","text":"Lorem ipsum 1 dolor sit amet","title":"Foot Note"},{"location":"stuff/#r-starter","text":"","title":"R Starter"},{"location":"stuff/#content_1","text":"","title":"Content"},{"location":"stuff/#job-submit","text":"","title":"Job Submit"},{"location":"stuff/#content_2","text":"","title":"Content"},{"location":"stuff/#tables","text":"Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Tables"},{"location":"activities/activities/","text":"Activities GPU Technology Conference (GTC) Code | Slides march 20-22, 2020 | San Jose California GPUs in Natural Language Processing Invited to participate in conference Instructor-Led Training at the GPU Technology Conference (GTC). Title: T22128: GPUs in Natural Language Processing Session Type: Instructor-Led Training Duration: 1 Hour 45 Minutes Find New Sentiments in Text Data Code | Slides march 3, 2020 | University of North Texas DP F223 Using text embedding and clustering methods to find new sentiments in text data. Use imdb movie reviews dataset and see how positive-negative sentiments can be used to find other sentiments. Use Tensorflow 2, Word Embeddings, Contextual Embeddings. We'll do clustering and find best number of cluster in data we don't know anything about. We'll do pretty plots to interpret results! We'll do all this on our UNT Talon Supercomputer! Duration: 1 Hour 15 Minutes Intro to using Git Lab Info | Slides may 3, 2019 | University of North Texas DP F285 How to use UNT HPC GitLab server Ever wondered what is Git and why every software company uses it? We'll teach some basic commands in Git Lab (platform used in almost every software company). And Yes, UNT has a Git Lab! Duration: 1 Hour 15 Minutes Intro to Word Embeddings - NLP Tools on Talon Info | Slides may 2, 2019 | University of North Texas DP F223 Natural Language Processing on Talon HPC Ever wondered or curious about natural language processing? How do you load a text file, how do you 'clean' it, how do you make use of it? We will learn all that and at the end you will build a Neural Network model that can predict a sentiment of a Movie Review. All this on Talon! Duration: 1 Hour 15 Minutes Using Python and Jupyter Notebooks on Talon Info | Slides may 1, 2019 | University of North Texas DP F223 Hands on work on Talon UNT HPC This workshop will go over some basic instruction about Python and Jupyter Notebooks. We will learn how to see what libraries we have available and how to install your own.Learn how to create a virtual environment, and most exciting build a simple Neural Network model to predict diabetes! All this on Talon! Duration: 1 Hour 15 Minutes Machine Learning - Neural Networks on Talon Info | Slides november 30, 2018 | University of North Texas GAB 535 What are Neural Networks, and why everyone is talking about them? This workshop is meant to give a general overview of why Neural Networks and Deep Learning are getting so much attention and help shed some light on how they work and what applications are possible because of them. Duration: 1 Hour 15 Minutes","title":"Activities"},{"location":"activities/activities/#activities","text":"","title":"Activities"},{"location":"activities/activities/#gpu-technology-conference-gtc","text":"Code | Slides march 20-22, 2020 | San Jose California GPUs in Natural Language Processing Invited to participate in conference Instructor-Led Training at the GPU Technology Conference (GTC). Title: T22128: GPUs in Natural Language Processing Session Type: Instructor-Led Training Duration: 1 Hour 45 Minutes","title":"GPU Technology Conference (GTC)"},{"location":"activities/activities/#find-new-sentiments-in-text-data","text":"Code | Slides march 3, 2020 | University of North Texas DP F223 Using text embedding and clustering methods to find new sentiments in text data. Use imdb movie reviews dataset and see how positive-negative sentiments can be used to find other sentiments. Use Tensorflow 2, Word Embeddings, Contextual Embeddings. We'll do clustering and find best number of cluster in data we don't know anything about. We'll do pretty plots to interpret results! We'll do all this on our UNT Talon Supercomputer! Duration: 1 Hour 15 Minutes","title":"Find New Sentiments in Text Data"},{"location":"activities/activities/#intro-to-using-git-lab","text":"Info | Slides may 3, 2019 | University of North Texas DP F285 How to use UNT HPC GitLab server Ever wondered what is Git and why every software company uses it? We'll teach some basic commands in Git Lab (platform used in almost every software company). And Yes, UNT has a Git Lab! Duration: 1 Hour 15 Minutes","title":"Intro to using Git Lab"},{"location":"activities/activities/#intro-to-word-embeddings---nlp-tools-on-talon","text":"Info | Slides may 2, 2019 | University of North Texas DP F223 Natural Language Processing on Talon HPC Ever wondered or curious about natural language processing? How do you load a text file, how do you 'clean' it, how do you make use of it? We will learn all that and at the end you will build a Neural Network model that can predict a sentiment of a Movie Review. All this on Talon! Duration: 1 Hour 15 Minutes","title":"Intro to Word Embeddings - NLP Tools on Talon"},{"location":"activities/activities/#using-python-and-jupyter-notebooks-on-talon","text":"Info | Slides may 1, 2019 | University of North Texas DP F223 Hands on work on Talon UNT HPC This workshop will go over some basic instruction about Python and Jupyter Notebooks. We will learn how to see what libraries we have available and how to install your own.Learn how to create a virtual environment, and most exciting build a simple Neural Network model to predict diabetes! All this on Talon! Duration: 1 Hour 15 Minutes","title":"Using Python and Jupyter Notebooks on Talon"},{"location":"activities/activities/#machine-learning---neural-networks-on-talon","text":"Info | Slides november 30, 2018 | University of North Texas GAB 535 What are Neural Networks, and why everyone is talking about them? This workshop is meant to give a general overview of why Neural Networks and Deep Learning are getting so much attention and help shed some light on how they work and what applications are possible because of them. Duration: 1 Hour 15 Minutes","title":"Machine Learning - Neural Networks on Talon"},{"location":"resume/resume/","text":"Resume pdf Summary Interest areas: Reinforcement Learning, Computer Vision, Big Data, Scaling Machine Learning, Parallel Processing. Main Research areas: Deep Learning, Natural Language Processing Thesis project: Casual Dialog Generation with different personas . For example, imagine a chatbot from the TV Series Friends that talks like your favorite character (Joey, Chandler etc.) Key Assets: Being a PhD student set me up to work with little guidance and being self-taught . How I like to takle problems: always start small and with good understating of the problem, build something simple that works and keep adding and testing on that until reaching the goal. Experience Data Scientist Intern State Farm | Enterprise Data & Analytics Align machine learning and artificial inteligence research to bussiness needs. Created a Reinforcement Learning (RL) proof of concept application. Built a framework around RL that makes it expandable into other business areas. Created custom data environments to easily train and deploy RL models. May 2020 \u2013 July 2020 Data Scientist \u2013 Machine Learning Engineer University of North Texas (UNT) | Research Information Technology Supported research project data science and Machine Learning (ML) needs. Maintain latest ML frameworks for UNT\u2019s High Performance Computing (HPC). Created workshops and tutorials about cloud computing HPC services. Sep 2018 - May 2020 Machine Learning Engineer Intern State Farm | Enterprise Data & Analytics Automated model risk validation reducing deployment timeline by 90%. Deployed Python package for automated model building and data analysis. Built a user interface for automated model comparison with CI/CD pipeline. May 2019 \u2013 July 2019 Data Scientist Intern State Farm | Enterprise &Data Analytics Introduced GPUs in ML tutorials that speed up training by over 50%. Increased model performance used in claims process by 5% using text data. Built Deep Learning Optical Character Recognition with over 75% accuracy. May 2018 \u2013 August 2018 Teaching Assistant \u2013 Computer Science University of North Texas (UNT) | Department of Computer Science Built and debugged C/C++ coding assignments and exams. Assisted Computer Science students as a peer mentor and grader. Sep 2017 \u2013 May 2018 Artificial Intelligence \u2013 Machine Learning Researcher University of North Texas (UNT) | Department of Computer Science Built state of the art Deep Learning models in Natural Language Processing. Implement concepts from research papers related to state of the art chatbots. Jan 2017 - Present Education PhD in Computer Science University of North Texas (UNT) | Department of Computer Science Focus on Natural Language Processing, Deep Generative Modeling, Companionable Robots, Persona Detection. May 2019 - Present | GPA 4.0 Masters in Computer Science University of North Texas (UNT) | Department of Computer Science Specialized in Machine Learning, Natural Language Processing, Big Data, Data Mining, Computer Vision and Deep Reinforcement Learning. May 2019 | GPA 3.9 Skills Reference Dr. Rodney D. Nielsen Associate Professor - Director HiLT Lab University of North Texas \u2013 Computer Science Engineer Email: Rodney.Nielsen@UNT.edu","title":"Resume"},{"location":"resume/resume/#resume--pdf","text":"","title":"Resume  pdf"},{"location":"resume/resume/#summary","text":"Interest areas: Reinforcement Learning, Computer Vision, Big Data, Scaling Machine Learning, Parallel Processing. Main Research areas: Deep Learning, Natural Language Processing Thesis project: Casual Dialog Generation with different personas . For example, imagine a chatbot from the TV Series Friends that talks like your favorite character (Joey, Chandler etc.) Key Assets: Being a PhD student set me up to work with little guidance and being self-taught . How I like to takle problems: always start small and with good understating of the problem, build something simple that works and keep adding and testing on that until reaching the goal.","title":"Summary"},{"location":"resume/resume/#experience","text":"","title":"Experience"},{"location":"resume/resume/#data-scientist-intern","text":"State Farm | Enterprise Data & Analytics Align machine learning and artificial inteligence research to bussiness needs. Created a Reinforcement Learning (RL) proof of concept application. Built a framework around RL that makes it expandable into other business areas. Created custom data environments to easily train and deploy RL models. May 2020 \u2013 July 2020","title":"Data Scientist Intern"},{"location":"resume/resume/#data-scientist--machine-learning-engineer","text":"University of North Texas (UNT) | Research Information Technology Supported research project data science and Machine Learning (ML) needs. Maintain latest ML frameworks for UNT\u2019s High Performance Computing (HPC). Created workshops and tutorials about cloud computing HPC services. Sep 2018 - May 2020","title":"Data Scientist \u2013 Machine Learning Engineer"},{"location":"resume/resume/#machine-learning-engineer-intern","text":"State Farm | Enterprise Data & Analytics Automated model risk validation reducing deployment timeline by 90%. Deployed Python package for automated model building and data analysis. Built a user interface for automated model comparison with CI/CD pipeline. May 2019 \u2013 July 2019","title":"Machine Learning Engineer Intern"},{"location":"resume/resume/#data-scientist-intern_1","text":"State Farm | Enterprise &Data Analytics Introduced GPUs in ML tutorials that speed up training by over 50%. Increased model performance used in claims process by 5% using text data. Built Deep Learning Optical Character Recognition with over 75% accuracy. May 2018 \u2013 August 2018","title":"Data Scientist Intern"},{"location":"resume/resume/#teaching-assistant--computer-science","text":"University of North Texas (UNT) | Department of Computer Science Built and debugged C/C++ coding assignments and exams. Assisted Computer Science students as a peer mentor and grader. Sep 2017 \u2013 May 2018","title":"Teaching Assistant \u2013 Computer Science"},{"location":"resume/resume/#artificial-intelligence--machine-learning-researcher","text":"University of North Texas (UNT) | Department of Computer Science Built state of the art Deep Learning models in Natural Language Processing. Implement concepts from research papers related to state of the art chatbots. Jan 2017 - Present","title":"Artificial Intelligence \u2013 Machine Learning Researcher"},{"location":"resume/resume/#education","text":"","title":"Education"},{"location":"resume/resume/#phd-in-computer-science","text":"University of North Texas (UNT) | Department of Computer Science Focus on Natural Language Processing, Deep Generative Modeling, Companionable Robots, Persona Detection. May 2019 - Present | GPA 4.0","title":"PhD in Computer Science"},{"location":"resume/resume/#masters-in-computer-science","text":"University of North Texas (UNT) | Department of Computer Science Specialized in Machine Learning, Natural Language Processing, Big Data, Data Mining, Computer Vision and Deep Reinforcement Learning. May 2019 | GPA 3.9","title":"Masters in Computer Science"},{"location":"resume/resume/#skills","text":"","title":"Skills"},{"location":"resume/resume/#reference","text":"","title":"Reference"},{"location":"resume/resume/#dr-rodney-d-nielsen","text":"Associate Professor - Director HiLT Lab University of North Texas \u2013 Computer Science Engineer Email: Rodney.Nielsen@UNT.edu","title":"Dr. Rodney D. Nielsen"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/","text":"Fine-tune Transformers in PyTorch using Hugging Face Transformers Complete tutorial on how to fine-tune 73 transformer models for text classification \u2014 no code changes necessary! Info This notebook is designed to use a pretrained transformers model and fine-tune it on a classification task. The focus of this tutorial will be on the code itself and how to adjust it to your needs. This notebook is using the AutoClasses from transformer by Hugging Face functionality. This functionality can guess a model's configuration, tokenizer and architecture just by passing in the model's name. This allows for code reusability on a large number of transformers models! What should I know for this notebook? I provided enough instructions and comments to be able to follow along with minimum Python coding knowledge. Since I am using PyTorch to fine-tune our transformers models any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too. How to use this notebook? I built this notebook with reusability in mind. The way I load the dataset into the PyTorch Dataset class is pretty standard and can be easily reused for any other dataset. The only modifications needed to use your own dataset will be in reading in the dataset inside the MovieReviewsDataset class which uses PyTorch Dataset . The DataLoader will return a dictionary of batch inputs format so that it can be fed straight to the model using the statement: outputs = model(**batch) . As long as this statement holds, the rest of the code will work! What transformers models work with this notebook? There are rare cases where I use a different model than Bert when dealing with classification from text data. When there is a need to run a different transformer model architecture, which one would work with this code? Since the name of the notebooks is finetune_transformers it should work with more than one type of transformers. I ran this notebook across all the pretrained models found on Hugging Face Transformer. This way you know ahead of time if the model you plan to use works with this code without any modifications. The list of pretrained transformers models that work with this notebook can be found here . There are 73 models that worked and 33 models that failed to work with this notebook. Dataset This notebook will cover fine-tune transformers for binary classification task. I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with. Coding Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations. Downloads Download the Large Movie Review Dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Installs transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. Give it a try! # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 6.7MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 48.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 49.0MB/s Building wheel for transformers (PEP 517) ... done Building wheel for sacremoses (setup.py) ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Building wheel for ml-things (setup.py) ... done Building wheel for ftfy (setup.py) ... done Imports Import all needed libraries for this notebook. Declare parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. epochs - Number of training epochs (authors recommend between 2 and 4). batch_size - Number of batches - depending on the max sequence length and GPU memory. For 512 sequence length a batch of 10 USUALY works without cuda memory issues. For small sequence length can try batch of 32 or higher. max_length - Pad or truncate text sequences to a specific length. I will set it to 60 to speed up training. device - Look for gpu to use. Will use cpu by default if no gpu found. model_name_or_path - Name of transformers model - will use already pretrained model. Path of transformer model - will load your own model from local disk. I always like to start off with bert-base-cased : 12-layer, 768-hidden, 12-heads, 109M parameters. Trained on cased English text. labels_ids - Dictionary of labels and their id - this will be used to convert string labels to numbers. n_labels - How many labels are we using in this dataset. This is used to decide size of classification head. import io import os import torch from tqdm.notebook import tqdm from torch.utils.data import Dataset , DataLoader from ml_things import plot_dict , plot_confusion_matrix , fix_text from sklearn.metrics import classification_report , accuracy_score from transformers import ( AutoConfig , AutoModelForSequenceClassification , AutoTokenizer , AdamW , get_linear_schedule_with_warmup , set_seed , ) # Set seed for reproducibility, set_seed ( 123 ) # Number of training epochs (authors recommend between 2 and 4) epochs = 4 # Number of batches - depending on the max sequence length and GPU memory. # For 512 sequence length batch of 10 works without cuda memory issues. # For small sequence length can try batch of 32 or higher. batch_size = 32 # Pad or truncate text sequences to a specific length # if `None` it will use maximum sequence of word piece tokens allowed by model. max_length = 60 # Look for gpu to use. Will use `cpu` by default if no gpu found. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) # Name of transformers model - will use already pretrained model. # Path of transformer model - will load your own model from local disk. model_name_or_path = 'bert-base-cased' # Dicitonary of labels and their id - this will be used to convert. # String labels to number ids. labels_ids = { 'neg' : 0 , 'pos' : 1 } # How many labels are we using in training. # This is used to decide size of classification head. n_labels = len ( labels_ids ) Helper Functions I like to keep all Classes and functions that will be used in this notebook under this section to help maintain a clean look of the notebook: MovieReviewsDataset(Dataset) If you worked with PyTorch before this is pretty standard. We need this class to read in our dataset, parse it, use tokenizer that transforms text into numbers and get it into a nice format to be fed to the model. Lucky for use, Hugging Face thought of everything and made the tokenizer do all the heavy lifting (split text into tokens, padding, truncating, encode text into numbers) and is very easy to use! In this class I only need to read in the content of each file, use fix_text to fix any Unicode problems and keep track of positive and negative sentiments. I will append all texts and labels in lists that later I will feed to the tokenizer and to the label ids to transform everything into numbers. There are three main parts of this PyTorch Dataset class: init () where we read in the dataset and transform text and labels into numbers. len () where we need to return the number of examples we read in. This is used when calling len(MovieReviewsDataset()) . getitem() always takes as an input an int value that represents which example from our examples to return from our dataset. If a value of 3 is passed, we will return the example form our dataset at position 3. It needs to return an object with the format that can be fed to our model. Luckily our tokenizer does that for us and returns a dictionary of variables ready to be fed to the model in this way: model(**inputs) . class MovieReviewsDataset ( Dataset ): r \"\"\"PyTorch Dataset class for loading data. This is where the data parsing happens and where the text gets encoded using loaded tokenizer. This class is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: path (:obj:`str`): Path to the data partition. use_tokenizer (:obj:`transformers.tokenization_?`): Transformer type tokenizer used to process raw text into numbers. labels_ids (:obj:`dict`): Dictionary to encode any labels names into numbers. Keys map to labels names and Values map to number associated to those labels. max_sequence_len (:obj:`int`, `optional`) Value to indicate the maximum desired sequence to truncate or pad text sequences. If no value is passed it will used maximum sequence size supported by the tokenizer and model. \"\"\" def __init__ ( self , path , use_tokenizer , labels_ids , max_sequence_len = None ): # Check if path exists. if not os . path . isdir ( path ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) # Check max sequence length. max_sequence_len = use_tokenizer . max_len if max_sequence_len is None else max_sequence_len texts = [] labels = [] print ( 'Reading partitions...' ) # Since the labels are defined by folders with data we loop # through each label. for label , label_id , in tqdm ( labels_ids . items ()): sentiment_path = os . path . join ( path , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:10] # Sample for debugging. print ( 'Reading %s files...' % label ) # Go through each file and read its content. for file_name in tqdm ( files_names ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. texts . append ( content ) # Save encode labels. labels . append ( label_id ) # Number of exmaples. self . n_examples = len ( labels ) # Use tokenizer on texts. This can take a while. print ( 'Using tokenizer on all texts. This can take a while...' ) self . inputs = use_tokenizer ( texts , add_special_tokens = True , truncation = True , padding = True , return_tensors = 'pt' , max_length = max_sequence_len ) # Get maximum sequence length. self . sequence_len = self . inputs [ 'input_ids' ] . shape [ - 1 ] print ( 'Texts padded or truncated to %d length!' % self . sequence_len ) # Add labels. self . inputs . update ({ 'labels' : torch . tensor ( labels )}) print ( 'Finished! \\n ' ) return def __len__ ( self ): r \"\"\"When used `len` return the number of examples. \"\"\" return self . n_examples def __getitem__ ( self , item ): r \"\"\"Given an index return an example from the position. Arguments: item (:obj:`int`): Index position to pick an example to return. Returns: :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model. It holddes the statement `model(**Returned Dictionary)`. \"\"\" return { key : self . inputs [ key ][ item ] for key in self . inputs . keys ()} train(dataloader, optimizer_, scheduler_, device_) I created this function to perform a full pass through the DataLoader object (the DataLoader object is created from our Dataset* type object using the **MovieReviewsDataset class). This is basically one epoch train through the entire dataset. The dataloader is created from PyTorch DataLoader which takes the object created from MovieReviewsDataset class and puts each example in batches. This way we can feed our model batches of data! The optimizer_ and scheduler_ are very common in PyTorch. They are required to update the parameters of our model and update our learning rate during training. There is a lot more than that but I won't go into details. This can actually be a huge rabbit hole since A LOT happens behind these functions that we don't need to worry. Thank you PyTorch! In the process we keep track of the actual labels and the predicted labels along with the loss. def train ( dataloader , optimizer_ , scheduler_ , device_ ): r \"\"\" Train pytorch model on a single pass through the data loader. It will use the global variable `model` which is the transformer model loaded on `_device` that we want to train on. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. optimizer_ (:obj:`transformers.optimization.AdamW`): Optimizer used for training. scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`): PyTorch scheduler. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss]. \"\"\" # Use global variable for model. global model # Tracking variables. predictions_labels = [] true_labels = [] # Total loss for this epoch. total_loss = 0 # Put the model into training mode. model . train () # For each batch of training data... for batch in tqdm ( dataloader , total = len ( dataloader )): # Add original labels - use later for evaluation. true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Always clear any previously calculated gradients before performing a # backward pass. model . zero_grad () # Perform a forward pass (evaluate the model on this training batch). # This will return the loss (rather than the model output) because we # have provided the `labels`. # The documentation for this a bert model function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to calculate training accuracy. loss , logits = outputs [: 2 ] # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # Perform a backward pass to calculate the gradients. loss . backward () # Clip the norm of the gradients to 1.0. # This is to help prevent the \"exploding gradients\" problem. torch . nn . utils . clip_grad_norm_ ( model . parameters (), 1.0 ) # Update parameters and take a step using the computed gradient. # The optimizer dictates the \"update rule\"--how the parameters are # modified based on their gradients, the learning rate, etc. optimizer . step () # Update the learning rate. scheduler . step () # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Convert these logits to list of predicted labels values. predictions_labels += logits . argmax ( axis =- 1 ) . flatten () . tolist () # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediction for future evaluations. return true_labels , predictions_labels , avg_epoch_loss validation(dataloader, device_) I implemented this function in a very similar way as train but without the parameters update, backward pass and gradient decent part. We don't need to do all of those VERY computationally intensive tasks because we only care about our model's predictions. I use the DataLoader in a similar way as in train to get out batches to feed to our model. In the process I keep track of the actual labels and the predicted labels along with the loss. def validation ( dataloader , device_ ): r \"\"\"Validation function to evaluate model performance on a separate set of data. This function will return the true and predicted labels so we can use later to evaluate the model's performance. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss] \"\"\" # Use global variable for model. global model # Tracking variables predictions_labels = [] true_labels = [] #total loss for this epoch. total_loss = 0 # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model . eval () # Evaluate data for one epoch for batch in tqdm ( dataloader , total = len ( dataloader )): # add original labels true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Telling the model not to compute or store gradients, saving memory and # speeding up validation with torch . no_grad (): # Forward pass, calculate logit predictions. # This will return the logits rather than the loss because we have # not provided labels. # token_type_ids is the same as the \"segment ids\", which # differentiates sentence 1 and 2 in 2-sentence tasks. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to to calculate training accuracy. loss , logits = outputs [: 2 ] # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # get predicitons to list predict_content = logits . argmax ( axis =- 1 ) . flatten () . tolist () # update list predictions_labels += predict_content # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediciton for future evaluations. return true_labels , predictions_labels , avg_epoch_loss Load Model and Tokenizer Loading the three essential parts of the pretrained transformers: configuration , tokenizer and model . I also need to load the model on the device I'm planning to use (GPU / CPU). Since I use the AutoClass functionality from Hugging Face I only need to worry about the model's name as input and the rest is handled by the transformers library. # Get model configuration. print ( 'Loading configuraiton...' ) model_config = AutoConfig . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , num_labels = n_labels ) # Get model's tokenizer. print ( 'Loading tokenizer...' ) tokenizer = AutoTokenizer . from_pretrained ( pretrained_model_name_or_path = model_name_or_path ) # Get the actual model. print ( 'Loading model...' ) model = AutoModelForSequenceClassification . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , config = model_config ) # Load model to defined device. model . to ( device ) print ( 'Model loaded to ` %s `' % device ) Loading configuraiton... Loading tokenizer... Loading model... Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Model loaded to `cuda` Dataset and DataLoader This is wehere I create the PyTorch Dataset and DataLoader objects that will be used to feed data into our model. This is where I use the MovieReviewsDataset class and create the dataset variables. Since data is partitioned for both train and test I will create a PyTorch Dataset and PyTorch DataLoader object for train and test. ONLY for simplicity I will use the test as validation. In practice NEVER USE THE TEST DATA FOR VALIDATION! print ( 'Dealing with Train...' ) # Create pytorch dataset. train_dataset = MovieReviewsDataset ( path = '/content/aclImdb/train' , use_tokenizer = tokenizer , labels_ids = labels_ids , max_sequence_len = max_length ) print ( 'Created `train_dataset` with %d examples!' % len ( train_dataset )) # Move pytorch dataset into dataloader. train_dataloader = DataLoader ( train_dataset , batch_size = batch_size , shuffle = True ) print ( 'Created `train_dataloader` with %d batches!' % len ( train_dataloader )) print () print ( 'Dealing with ...' ) # Create pytorch dataset. valid_dataset = MovieReviewsDataset ( path = '/content/aclImdb/test' , use_tokenizer = tokenizer , labels_ids = labels_ids , max_sequence_len = max_length ) print ( 'Created `valid_dataset` with %d examples!' % len ( valid_dataset )) # Move pytorch dataset into dataloader. valid_dataloader = DataLoader ( valid_dataset , batch_size = batch_size , shuffle = False ) print ( 'Created `eval_dataloader` with %d batches!' % len ( valid_dataloader )) Dealing with Train... Reading partitions... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|2/2 [00:34<00:00, 17.28s/it] Reading neg files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:34<00:00, 362.01it/s] Reading pos files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:23<00:00, 534.34it/s] Using tokenizer on all texts. This can take a while... Texts padded or truncated to 40 length! Finished! Created `train_dataset` with 25000 examples! Created `train_dataloader` with 25000 batches! Dealing with ... Reading partitions... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|2/2 [01:28<00:00, 44.13s/it] Reading neg files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [01:28<00:00, 141.71it/s] Reading pos files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [01:17<00:00, 161.60it/s] Using tokenizer on all texts. This can take a while... Texts padded or truncated to 40 length! Finished! Created `valid_dataset` with 25000 examples! Created `eval_dataloader` with 25000 batches! Train I create an optimizer and scheduler that will be used by PyTorch in training. I loop through the number of defined epochs and call the train and validation functions. I will output similar info after each epoch as in Keras: train_loss: - val_loss: - train_acc: - valid_acc . After training, I plot the train and validation loss and accuracy curves to check how the training went. # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the 'W' stands for 'Weight Decay fix\" optimizer = AdamW ( model . parameters (), lr = 2e-5 , # args.learning_rate - default is 5e-5, our notebook had 2e-5 eps = 1e-8 # args.adam_epsilon - default is 1e-8. ) # Total number of training steps is number of batches * number of epochs. # `train_dataloader` contains batched data so `len(train_dataloader)` gives # us the number of batches. total_steps = len ( train_dataloader ) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup ( optimizer , num_warmup_steps = 0 , # Default value in run_glue.py num_training_steps = total_steps ) # Store the average loss after each epoch so we can plot them. all_loss = { 'train_loss' :[], 'val_loss' :[]} all_acc = { 'train_acc' :[], 'val_acc' :[]} # Loop through each epoch. print ( 'Epoch' ) for epoch in tqdm ( range ( epochs )): print () print ( 'Training on batches...' ) # Perform one full pass over the training set. train_labels , train_predict , train_loss = train ( train_dataloader , optimizer , scheduler , device ) train_acc = accuracy_score ( train_labels , train_predict ) # Get prediction form model on validation data. print ( 'Validation on batches...' ) valid_labels , valid_predict , val_loss = validation ( valid_dataloader , device ) val_acc = accuracy_score ( valid_labels , valid_predict ) # Print loss and accuracy values to see how training evolves. print ( \" train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f \" % ( train_loss , val_loss , train_acc , val_acc )) print () # Store the loss value for plotting the learning curve. all_loss [ 'train_loss' ] . append ( train_loss ) all_loss [ 'val_loss' ] . append ( val_loss ) all_acc [ 'train_acc' ] . append ( train_acc ) all_acc [ 'val_acc' ] . append ( val_acc ) # Plot loss curves. plot_dict ( all_loss , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) # Plot accuracy curves. plot_dict ( all_acc , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) Epoch 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|4/4 [13:49<00:00, 207.37s/it] Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.86it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [00:46<00:00, 16.80it/s] train_loss: 0.44816 - val_loss: 0.38655 - train_acc: 0.78372 - valid_acc: 0.81892 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.86it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:13<00:00, 5.88it/s] train_loss: 0.29504 - val_loss: 0.43493 - train_acc: 0.87352 - valid_acc: 0.82360 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.87it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [01:43<00:00, 7.58it/s] train_loss: 0.16901 - val_loss: 0.48433 - train_acc: 0.93544 - valid_acc: 0.82624 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.87it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [00:46<00:00, 16.79it/s] train_loss: 0.09816 - val_loss: 0.73001 - train_acc: 0.96936 - valid_acc: 0.82144 It looks like a little over one epoch is enough training for this model and dataset. Evaluate When dealing with classification it's useful to look at precision recall and f1 score. Another good thing to look at when evaluating the model is the confusion matrix. # Get prediction form model on validation data. This is where you should use # your test data. true_labels , predictions_labels , avg_epoch_loss = validation ( valid_dataloader , device ) # Create the evaluation report. evaluation_report = classification_report ( true_labels , predictions_labels , labels = list ( labels_ids . values ()), target_names = list ( labels_ids . keys ())) # Show the evaluation report. print ( evaluation_report ) # Plot confusion matrix. plot_confusion_matrix ( y_true = true_labels , y_pred = predictions_labels , classes = list ( labels_ids . keys ()), normalize = True , magnify = 3 , ); 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [00:46<00:00, 16.77it/s] precision recall f1-score support neg 0.83 0.81 0.82 12500 pos 0.81 0.83 0.82 12500 accuracy 0.82 25000 macro avg 0.82 0.82 0.82 25000 weighted avg 0.82 0.82 0.82 25000 Normalized confusion matrix Results are not great, but for this tutorial we are not interested in performance. Final Note If you made it this far Congrats and Thank you for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you have 1 minute please give me a feedback in the comments. If you see something wrong please let me know by opening an issue on my ml_things GitHub repository! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can. Contact \ud83c\udfa3 \ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"Finetune Transformers"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#fine-tune-transformers-in-pytorch-using-hugging-face-transformers","text":"","title":"Fine-tune Transformers in PyTorch using Hugging Face Transformers"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#complete-tutorial-on-how-to-fine-tune-73-transformer-models-for-text-classification--no-code-changes-necessary","text":"","title":"Complete tutorial on how to fine-tune 73 transformer models for text classification \u2014 no code changes necessary!"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#info","text":"This notebook is designed to use a pretrained transformers model and fine-tune it on a classification task. The focus of this tutorial will be on the code itself and how to adjust it to your needs. This notebook is using the AutoClasses from transformer by Hugging Face functionality. This functionality can guess a model's configuration, tokenizer and architecture just by passing in the model's name. This allows for code reusability on a large number of transformers models!","title":"Info"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#what-should-i-know-for-this-notebook","text":"I provided enough instructions and comments to be able to follow along with minimum Python coding knowledge. Since I am using PyTorch to fine-tune our transformers models any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too.","title":"What should I know for this notebook?"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#how-to-use-this-notebook","text":"I built this notebook with reusability in mind. The way I load the dataset into the PyTorch Dataset class is pretty standard and can be easily reused for any other dataset. The only modifications needed to use your own dataset will be in reading in the dataset inside the MovieReviewsDataset class which uses PyTorch Dataset . The DataLoader will return a dictionary of batch inputs format so that it can be fed straight to the model using the statement: outputs = model(**batch) . As long as this statement holds, the rest of the code will work!","title":"How to use this notebook?"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#what-transformers-models-work-with-this-notebook","text":"There are rare cases where I use a different model than Bert when dealing with classification from text data. When there is a need to run a different transformer model architecture, which one would work with this code? Since the name of the notebooks is finetune_transformers it should work with more than one type of transformers. I ran this notebook across all the pretrained models found on Hugging Face Transformer. This way you know ahead of time if the model you plan to use works with this code without any modifications. The list of pretrained transformers models that work with this notebook can be found here . There are 73 models that worked and 33 models that failed to work with this notebook.","title":"What transformers models work with this notebook?"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#dataset","text":"This notebook will cover fine-tune transformers for binary classification task. I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with.","title":"Dataset"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#coding","text":"Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.","title":"Coding"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#downloads","text":"Download the Large Movie Review Dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz","title":"Downloads"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#installs","text":"transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. Give it a try! # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 6.7MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 48.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 49.0MB/s Building wheel for transformers (PEP 517) ... done Building wheel for sacremoses (setup.py) ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Building wheel for ml-things (setup.py) ... done Building wheel for ftfy (setup.py) ... done","title":"Installs"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#imports","text":"Import all needed libraries for this notebook. Declare parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. epochs - Number of training epochs (authors recommend between 2 and 4). batch_size - Number of batches - depending on the max sequence length and GPU memory. For 512 sequence length a batch of 10 USUALY works without cuda memory issues. For small sequence length can try batch of 32 or higher. max_length - Pad or truncate text sequences to a specific length. I will set it to 60 to speed up training. device - Look for gpu to use. Will use cpu by default if no gpu found. model_name_or_path - Name of transformers model - will use already pretrained model. Path of transformer model - will load your own model from local disk. I always like to start off with bert-base-cased : 12-layer, 768-hidden, 12-heads, 109M parameters. Trained on cased English text. labels_ids - Dictionary of labels and their id - this will be used to convert string labels to numbers. n_labels - How many labels are we using in this dataset. This is used to decide size of classification head. import io import os import torch from tqdm.notebook import tqdm from torch.utils.data import Dataset , DataLoader from ml_things import plot_dict , plot_confusion_matrix , fix_text from sklearn.metrics import classification_report , accuracy_score from transformers import ( AutoConfig , AutoModelForSequenceClassification , AutoTokenizer , AdamW , get_linear_schedule_with_warmup , set_seed , ) # Set seed for reproducibility, set_seed ( 123 ) # Number of training epochs (authors recommend between 2 and 4) epochs = 4 # Number of batches - depending on the max sequence length and GPU memory. # For 512 sequence length batch of 10 works without cuda memory issues. # For small sequence length can try batch of 32 or higher. batch_size = 32 # Pad or truncate text sequences to a specific length # if `None` it will use maximum sequence of word piece tokens allowed by model. max_length = 60 # Look for gpu to use. Will use `cpu` by default if no gpu found. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) # Name of transformers model - will use already pretrained model. # Path of transformer model - will load your own model from local disk. model_name_or_path = 'bert-base-cased' # Dicitonary of labels and their id - this will be used to convert. # String labels to number ids. labels_ids = { 'neg' : 0 , 'pos' : 1 } # How many labels are we using in training. # This is used to decide size of classification head. n_labels = len ( labels_ids )","title":"Imports"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#helper-functions","text":"I like to keep all Classes and functions that will be used in this notebook under this section to help maintain a clean look of the notebook: MovieReviewsDataset(Dataset) If you worked with PyTorch before this is pretty standard. We need this class to read in our dataset, parse it, use tokenizer that transforms text into numbers and get it into a nice format to be fed to the model. Lucky for use, Hugging Face thought of everything and made the tokenizer do all the heavy lifting (split text into tokens, padding, truncating, encode text into numbers) and is very easy to use! In this class I only need to read in the content of each file, use fix_text to fix any Unicode problems and keep track of positive and negative sentiments. I will append all texts and labels in lists that later I will feed to the tokenizer and to the label ids to transform everything into numbers. There are three main parts of this PyTorch Dataset class: init () where we read in the dataset and transform text and labels into numbers. len () where we need to return the number of examples we read in. This is used when calling len(MovieReviewsDataset()) . getitem() always takes as an input an int value that represents which example from our examples to return from our dataset. If a value of 3 is passed, we will return the example form our dataset at position 3. It needs to return an object with the format that can be fed to our model. Luckily our tokenizer does that for us and returns a dictionary of variables ready to be fed to the model in this way: model(**inputs) . class MovieReviewsDataset ( Dataset ): r \"\"\"PyTorch Dataset class for loading data. This is where the data parsing happens and where the text gets encoded using loaded tokenizer. This class is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: path (:obj:`str`): Path to the data partition. use_tokenizer (:obj:`transformers.tokenization_?`): Transformer type tokenizer used to process raw text into numbers. labels_ids (:obj:`dict`): Dictionary to encode any labels names into numbers. Keys map to labels names and Values map to number associated to those labels. max_sequence_len (:obj:`int`, `optional`) Value to indicate the maximum desired sequence to truncate or pad text sequences. If no value is passed it will used maximum sequence size supported by the tokenizer and model. \"\"\" def __init__ ( self , path , use_tokenizer , labels_ids , max_sequence_len = None ): # Check if path exists. if not os . path . isdir ( path ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) # Check max sequence length. max_sequence_len = use_tokenizer . max_len if max_sequence_len is None else max_sequence_len texts = [] labels = [] print ( 'Reading partitions...' ) # Since the labels are defined by folders with data we loop # through each label. for label , label_id , in tqdm ( labels_ids . items ()): sentiment_path = os . path . join ( path , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:10] # Sample for debugging. print ( 'Reading %s files...' % label ) # Go through each file and read its content. for file_name in tqdm ( files_names ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. texts . append ( content ) # Save encode labels. labels . append ( label_id ) # Number of exmaples. self . n_examples = len ( labels ) # Use tokenizer on texts. This can take a while. print ( 'Using tokenizer on all texts. This can take a while...' ) self . inputs = use_tokenizer ( texts , add_special_tokens = True , truncation = True , padding = True , return_tensors = 'pt' , max_length = max_sequence_len ) # Get maximum sequence length. self . sequence_len = self . inputs [ 'input_ids' ] . shape [ - 1 ] print ( 'Texts padded or truncated to %d length!' % self . sequence_len ) # Add labels. self . inputs . update ({ 'labels' : torch . tensor ( labels )}) print ( 'Finished! \\n ' ) return def __len__ ( self ): r \"\"\"When used `len` return the number of examples. \"\"\" return self . n_examples def __getitem__ ( self , item ): r \"\"\"Given an index return an example from the position. Arguments: item (:obj:`int`): Index position to pick an example to return. Returns: :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model. It holddes the statement `model(**Returned Dictionary)`. \"\"\" return { key : self . inputs [ key ][ item ] for key in self . inputs . keys ()} train(dataloader, optimizer_, scheduler_, device_) I created this function to perform a full pass through the DataLoader object (the DataLoader object is created from our Dataset* type object using the **MovieReviewsDataset class). This is basically one epoch train through the entire dataset. The dataloader is created from PyTorch DataLoader which takes the object created from MovieReviewsDataset class and puts each example in batches. This way we can feed our model batches of data! The optimizer_ and scheduler_ are very common in PyTorch. They are required to update the parameters of our model and update our learning rate during training. There is a lot more than that but I won't go into details. This can actually be a huge rabbit hole since A LOT happens behind these functions that we don't need to worry. Thank you PyTorch! In the process we keep track of the actual labels and the predicted labels along with the loss. def train ( dataloader , optimizer_ , scheduler_ , device_ ): r \"\"\" Train pytorch model on a single pass through the data loader. It will use the global variable `model` which is the transformer model loaded on `_device` that we want to train on. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. optimizer_ (:obj:`transformers.optimization.AdamW`): Optimizer used for training. scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`): PyTorch scheduler. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss]. \"\"\" # Use global variable for model. global model # Tracking variables. predictions_labels = [] true_labels = [] # Total loss for this epoch. total_loss = 0 # Put the model into training mode. model . train () # For each batch of training data... for batch in tqdm ( dataloader , total = len ( dataloader )): # Add original labels - use later for evaluation. true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Always clear any previously calculated gradients before performing a # backward pass. model . zero_grad () # Perform a forward pass (evaluate the model on this training batch). # This will return the loss (rather than the model output) because we # have provided the `labels`. # The documentation for this a bert model function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to calculate training accuracy. loss , logits = outputs [: 2 ] # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # Perform a backward pass to calculate the gradients. loss . backward () # Clip the norm of the gradients to 1.0. # This is to help prevent the \"exploding gradients\" problem. torch . nn . utils . clip_grad_norm_ ( model . parameters (), 1.0 ) # Update parameters and take a step using the computed gradient. # The optimizer dictates the \"update rule\"--how the parameters are # modified based on their gradients, the learning rate, etc. optimizer . step () # Update the learning rate. scheduler . step () # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Convert these logits to list of predicted labels values. predictions_labels += logits . argmax ( axis =- 1 ) . flatten () . tolist () # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediction for future evaluations. return true_labels , predictions_labels , avg_epoch_loss validation(dataloader, device_) I implemented this function in a very similar way as train but without the parameters update, backward pass and gradient decent part. We don't need to do all of those VERY computationally intensive tasks because we only care about our model's predictions. I use the DataLoader in a similar way as in train to get out batches to feed to our model. In the process I keep track of the actual labels and the predicted labels along with the loss. def validation ( dataloader , device_ ): r \"\"\"Validation function to evaluate model performance on a separate set of data. This function will return the true and predicted labels so we can use later to evaluate the model's performance. This function is built with reusability in mind: it can be used as is as long as the `dataloader` outputs a batch in dictionary format that can be passed straight into the model - `model(**batch)`. Arguments: dataloader (:obj:`torch.utils.data.dataloader.DataLoader`): Parsed data into batches of tensors. device_ (:obj:`torch.device`): Device used to load tensors before feeding to model. Returns: :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss] \"\"\" # Use global variable for model. global model # Tracking variables predictions_labels = [] true_labels = [] #total loss for this epoch. total_loss = 0 # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model . eval () # Evaluate data for one epoch for batch in tqdm ( dataloader , total = len ( dataloader )): # add original labels true_labels += batch [ 'labels' ] . numpy () . flatten () . tolist () # move batch to device batch = { k : v . type ( torch . long ) . to ( device_ ) for k , v in batch . items ()} # Telling the model not to compute or store gradients, saving memory and # speeding up validation with torch . no_grad (): # Forward pass, calculate logit predictions. # This will return the logits rather than the loss because we have # not provided labels. # token_type_ids is the same as the \"segment ids\", which # differentiates sentence 1 and 2 in 2-sentence tasks. # The documentation for this `model` function is here: # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification outputs = model ( ** batch ) # The call to `model` always returns a tuple, so we need to pull the # loss value out of the tuple along with the logits. We will use logits # later to to calculate training accuracy. loss , logits = outputs [: 2 ] # Move logits and labels to CPU logits = logits . detach () . cpu () . numpy () # Accumulate the training loss over all of the batches so that we can # calculate the average loss at the end. `loss` is a Tensor containing a # single value; the `.item()` function just returns the Python value # from the tensor. total_loss += loss . item () # get predicitons to list predict_content = logits . argmax ( axis =- 1 ) . flatten () . tolist () # update list predictions_labels += predict_content # Calculate the average loss over the training data. avg_epoch_loss = total_loss / len ( dataloader ) # Return all true labels and prediciton for future evaluations. return true_labels , predictions_labels , avg_epoch_loss","title":"Helper Functions"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#load-model-and-tokenizer","text":"Loading the three essential parts of the pretrained transformers: configuration , tokenizer and model . I also need to load the model on the device I'm planning to use (GPU / CPU). Since I use the AutoClass functionality from Hugging Face I only need to worry about the model's name as input and the rest is handled by the transformers library. # Get model configuration. print ( 'Loading configuraiton...' ) model_config = AutoConfig . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , num_labels = n_labels ) # Get model's tokenizer. print ( 'Loading tokenizer...' ) tokenizer = AutoTokenizer . from_pretrained ( pretrained_model_name_or_path = model_name_or_path ) # Get the actual model. print ( 'Loading model...' ) model = AutoModelForSequenceClassification . from_pretrained ( pretrained_model_name_or_path = model_name_or_path , config = model_config ) # Load model to defined device. model . to ( device ) print ( 'Model loaded to ` %s `' % device ) Loading configuraiton... Loading tokenizer... Loading model... Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Model loaded to `cuda`","title":"Load Model and Tokenizer"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#dataset-and-dataloader","text":"This is wehere I create the PyTorch Dataset and DataLoader objects that will be used to feed data into our model. This is where I use the MovieReviewsDataset class and create the dataset variables. Since data is partitioned for both train and test I will create a PyTorch Dataset and PyTorch DataLoader object for train and test. ONLY for simplicity I will use the test as validation. In practice NEVER USE THE TEST DATA FOR VALIDATION! print ( 'Dealing with Train...' ) # Create pytorch dataset. train_dataset = MovieReviewsDataset ( path = '/content/aclImdb/train' , use_tokenizer = tokenizer , labels_ids = labels_ids , max_sequence_len = max_length ) print ( 'Created `train_dataset` with %d examples!' % len ( train_dataset )) # Move pytorch dataset into dataloader. train_dataloader = DataLoader ( train_dataset , batch_size = batch_size , shuffle = True ) print ( 'Created `train_dataloader` with %d batches!' % len ( train_dataloader )) print () print ( 'Dealing with ...' ) # Create pytorch dataset. valid_dataset = MovieReviewsDataset ( path = '/content/aclImdb/test' , use_tokenizer = tokenizer , labels_ids = labels_ids , max_sequence_len = max_length ) print ( 'Created `valid_dataset` with %d examples!' % len ( valid_dataset )) # Move pytorch dataset into dataloader. valid_dataloader = DataLoader ( valid_dataset , batch_size = batch_size , shuffle = False ) print ( 'Created `eval_dataloader` with %d batches!' % len ( valid_dataloader )) Dealing with Train... Reading partitions... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|2/2 [00:34<00:00, 17.28s/it] Reading neg files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:34<00:00, 362.01it/s] Reading pos files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:23<00:00, 534.34it/s] Using tokenizer on all texts. This can take a while... Texts padded or truncated to 40 length! Finished! Created `train_dataset` with 25000 examples! Created `train_dataloader` with 25000 batches! Dealing with ... Reading partitions... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|2/2 [01:28<00:00, 44.13s/it] Reading neg files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [01:28<00:00, 141.71it/s] Reading pos files... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [01:17<00:00, 161.60it/s] Using tokenizer on all texts. This can take a while... Texts padded or truncated to 40 length! Finished! Created `valid_dataset` with 25000 examples! Created `eval_dataloader` with 25000 batches!","title":"Dataset and DataLoader"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#train","text":"I create an optimizer and scheduler that will be used by PyTorch in training. I loop through the number of defined epochs and call the train and validation functions. I will output similar info after each epoch as in Keras: train_loss: - val_loss: - train_acc: - valid_acc . After training, I plot the train and validation loss and accuracy curves to check how the training went. # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the 'W' stands for 'Weight Decay fix\" optimizer = AdamW ( model . parameters (), lr = 2e-5 , # args.learning_rate - default is 5e-5, our notebook had 2e-5 eps = 1e-8 # args.adam_epsilon - default is 1e-8. ) # Total number of training steps is number of batches * number of epochs. # `train_dataloader` contains batched data so `len(train_dataloader)` gives # us the number of batches. total_steps = len ( train_dataloader ) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup ( optimizer , num_warmup_steps = 0 , # Default value in run_glue.py num_training_steps = total_steps ) # Store the average loss after each epoch so we can plot them. all_loss = { 'train_loss' :[], 'val_loss' :[]} all_acc = { 'train_acc' :[], 'val_acc' :[]} # Loop through each epoch. print ( 'Epoch' ) for epoch in tqdm ( range ( epochs )): print () print ( 'Training on batches...' ) # Perform one full pass over the training set. train_labels , train_predict , train_loss = train ( train_dataloader , optimizer , scheduler , device ) train_acc = accuracy_score ( train_labels , train_predict ) # Get prediction form model on validation data. print ( 'Validation on batches...' ) valid_labels , valid_predict , val_loss = validation ( valid_dataloader , device ) val_acc = accuracy_score ( valid_labels , valid_predict ) # Print loss and accuracy values to see how training evolves. print ( \" train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f \" % ( train_loss , val_loss , train_acc , val_acc )) print () # Store the loss value for plotting the learning curve. all_loss [ 'train_loss' ] . append ( train_loss ) all_loss [ 'val_loss' ] . append ( val_loss ) all_acc [ 'train_acc' ] . append ( train_acc ) all_acc [ 'val_acc' ] . append ( val_acc ) # Plot loss curves. plot_dict ( all_loss , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) # Plot accuracy curves. plot_dict ( all_acc , use_xlabel = 'Epochs' , use_ylabel = 'Value' , use_linestyles = [ '-' , '--' ]) Epoch 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|4/4 [13:49<00:00, 207.37s/it] Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.86it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [00:46<00:00, 16.80it/s] train_loss: 0.44816 - val_loss: 0.38655 - train_acc: 0.78372 - valid_acc: 0.81892 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.86it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:13<00:00, 5.88it/s] train_loss: 0.29504 - val_loss: 0.43493 - train_acc: 0.87352 - valid_acc: 0.82360 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.87it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [01:43<00:00, 7.58it/s] train_loss: 0.16901 - val_loss: 0.48433 - train_acc: 0.93544 - valid_acc: 0.82624 Training on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [02:40<00:00, 4.87it/s] Validation on batches... 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [00:46<00:00, 16.79it/s] train_loss: 0.09816 - val_loss: 0.73001 - train_acc: 0.96936 - valid_acc: 0.82144 It looks like a little over one epoch is enough training for this model and dataset.","title":"Train"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#evaluate","text":"When dealing with classification it's useful to look at precision recall and f1 score. Another good thing to look at when evaluating the model is the confusion matrix. # Get prediction form model on validation data. This is where you should use # your test data. true_labels , predictions_labels , avg_epoch_loss = validation ( valid_dataloader , device ) # Create the evaluation report. evaluation_report = classification_report ( true_labels , predictions_labels , labels = list ( labels_ids . values ()), target_names = list ( labels_ids . keys ())) # Show the evaluation report. print ( evaluation_report ) # Plot confusion matrix. plot_confusion_matrix ( y_true = true_labels , y_pred = predictions_labels , classes = list ( labels_ids . keys ()), normalize = True , magnify = 3 , ); 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|782/782 [00:46<00:00, 16.77it/s] precision recall f1-score support neg 0.83 0.81 0.82 12500 pos 0.81 0.83 0.82 12500 accuracy 0.82 25000 macro avg 0.82 0.82 0.82 25000 weighted avg 0.82 0.82 0.82 25000 Normalized confusion matrix Results are not great, but for this tutorial we are not interested in performance.","title":"Evaluate"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#final-note","text":"If you made it this far Congrats and Thank you for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you have 1 minute please give me a feedback in the comments. If you see something wrong please let me know by opening an issue on my ml_things GitHub repository! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.","title":"Final Note"},{"location":"tutorial_notebooks/finetune_transformers_pytorch/#contact-","text":"\ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"Contact \ud83c\udfa3"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/","text":"Title Work in progress Info Intro to this tutorial What should I know for this notebook? Any requirements. How to use this notebook? Instructions. What ? Tutorial specific answer. Dataset I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with. Coding Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations. Downloads Download the Large Movie Review Dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Installs transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. Give it a try! Code Cell: # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Output: Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .9MB 6 .7MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 890kB 48 .9MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 .1MB 49 .0MB/s Building wheel for transformers ( PEP 517 ) ... done Building wheel for sacremoses ( setup.py ) ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 5 .2MB/s Building wheel for ml-things ( setup.py ) ... done Building wheel for ftfy ( setup.py ) ... done Imports Import all needed libraries for this notebook. Declare parameters used for this notebook: * * Code Cell: Helper Functions Class() / function() Class / function description. Code Cell: Load Model and Tokenizer Loading the three essential parts of the pretrained transformers: configuration , tokenizer and model . I also need to load the model on the device I'm planning to use (GPU / CPU). Code Cell: Output: Dataset and DataLoader Details. Code Cell: Output: Train Code Cell: Output: Use ColabImage plots straight in here Evaluate Evaluation! Code Cell: Output: Use ColabImage plots straight in here Final Note If you made it this far Congrats and Thank you for your interest in my tutorial! Other details. If you see something wrong please let me know by opening an issue on my ml_things GitHub repository! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can. Contact GitHub: gmihaila Website: gmihaila.github.io LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila Thank you! Find out more About Me .","title":"**:gear: Title**"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#title","text":"","title":"Title"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#work-in-progress","text":"","title":"Work in progress"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#info","text":"Intro to this tutorial","title":"Info"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#what-should-i-know-for-this-notebook","text":"Any requirements.","title":"What should I know for this notebook?"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#how-to-use-this-notebook","text":"Instructions.","title":"How to use this notebook?"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#what-","text":"Tutorial specific answer.","title":"What ?"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#dataset","text":"I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with.","title":"Dataset"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#coding","text":"Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.","title":"Coding"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#downloads","text":"Download the Large Movie Review Dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz","title":"Downloads"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#installs","text":"transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. Give it a try! Code Cell: # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Output: Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .9MB 6 .7MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 890kB 48 .9MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 .1MB 49 .0MB/s Building wheel for transformers ( PEP 517 ) ... done Building wheel for sacremoses ( setup.py ) ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 5 .2MB/s Building wheel for ml-things ( setup.py ) ... done Building wheel for ftfy ( setup.py ) ... done","title":"Installs"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#imports","text":"Import all needed libraries for this notebook. Declare parameters used for this notebook: * * Code Cell:","title":"Imports"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#helper-functions","text":"Class() / function() Class / function description. Code Cell:","title":"Helper Functions"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#load-model-and-tokenizer","text":"Loading the three essential parts of the pretrained transformers: configuration , tokenizer and model . I also need to load the model on the device I'm planning to use (GPU / CPU). Code Cell: Output:","title":"Load Model and Tokenizer"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#dataset-and-dataloader","text":"Details. Code Cell: Output:","title":"Dataset and DataLoader"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#train","text":"Code Cell: Output: Use ColabImage plots straight in here","title":"Train"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#evaluate","text":"Evaluation! Code Cell: Output: Use ColabImage plots straight in here","title":"Evaluate"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#final-note","text":"If you made it this far Congrats and Thank you for your interest in my tutorial! Other details. If you see something wrong please let me know by opening an issue on my ml_things GitHub repository! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.","title":"Final Note"},{"location":"tutorial_notebooks/finetune_transformers_pytorchtext/#contact","text":"GitHub: gmihaila Website: gmihaila.github.io LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila Thank you! Find out more About Me .","title":"Contact"},{"location":"tutorial_notebooks/pretrain_transformer/","text":"Pretrain Transformers Info This notebook is used to pretrain transformers models using Huggingface . This notebooks is part of my trusty notebooks for Machine Learning. Check out more similar content on my website gmihaila.github.io/useful/useful/ where I post useful notebooks like this one. This notebook is heavily inspired from the Huggingface script used for training language models: transformers/tree/master/examples/language-modeling . 'Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.' How to use this notebook? This notebooks is a code adaptation of the run_language_modeling.py . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . Parse the arguments needed that are split in TrainingArguments, ModelArguments and DataTrainingArguments. The only variables that need configuration depending on your needs are model_args , data_args and training_args in Parameters : model_args of type ModelArguments : These are the arguments for the model that you want to use such as the model_name_or_path, tokenizer_name etc. You'll need these to load the model and tokenizer. Minimum setup: model_args = ModelArguments ( model_name_or_path , model_type , tokenizer_name , ) model_name_or_path path to existing transformers model or name of transformer model to be used: bert-base-cased , roberta-base , gpt2 etc. More details here . model_type type of model used: bert , roberta , gpt2 . More details here . tokenizer_name tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased , roberta-base , gpt2 etc. data_args of type DataTrainingArguments : These are as the name suggests arguments needed for the dataset. Such as the directory name where your files are stored etc. You'll need these to load/process the dataset. Minimum setup: data_args = DataArgs ( train_data_file , eval_data_file , mlm , ) train_data_file path to your dataset. This is a plain file that contains all your text data to train a model. Use each line to separate examples: i.e. if you have a dataset composed of multiple text documents, create a single file with each line in the file associated to a text document. eval_data_file same story as train_data_file . This file is used to evaluate the model performance mlm is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta . training_args of type TrainingArguments : These are the training hyper-parameters such as learning rate, batch size, weight decay, gradient accumulation steps etc. See all possible arguments here . These are used by the Trainer. Minimum setup: model_args training_args = TrainingArguments ( output_dir , do_train , do_eval , ) output_dir path where to save the pre-trained model. do_train variable to signal if you're using train data or not. Set it to True if you mentioned train_data_file . do_eval variable to signal if you're using evaluate data or not. Set it to True if you mentioned eval_data_file . Example: Pre-train Bert In the Parameters section use arguments: # process model arguments. Check Info - Notes for more details model_args = ModelArguments ( model_name_or_path = 'bert-base-cased' , model_type = 'bert' , tokenizer_name = 'bert-base-cased' , ) # process data arguments. Check Info - Notes for more details data_args = DataArgs ( train_data_file = '/content/your_train_data' , eval_data_file = '/content/your_test_data, mlm = True , ) # process training arguments. Check Info - Notes for more details training_args = TrainingArguments ( output_dir = '/content/pretrained_bert' , do_train = True , do_eval = False ) Notes: Parameters details got from here . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . I plan on testing more models in the future. I used the The WikiText Long Term Dependency Language Modeling Dataset as an example. To reduce training time I used the evaluate split as training and test split as evaluation! .","title":":dog: Pretrain Transformers"},{"location":"tutorial_notebooks/pretrain_transformer/#pretrain-transformers","text":"","title":"Pretrain Transformers"},{"location":"tutorial_notebooks/pretrain_transformer/#info","text":"This notebook is used to pretrain transformers models using Huggingface . This notebooks is part of my trusty notebooks for Machine Learning. Check out more similar content on my website gmihaila.github.io/useful/useful/ where I post useful notebooks like this one. This notebook is heavily inspired from the Huggingface script used for training language models: transformers/tree/master/examples/language-modeling . 'Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.'","title":"Info"},{"location":"tutorial_notebooks/pretrain_transformer/#how-to-use-this-notebook","text":"This notebooks is a code adaptation of the run_language_modeling.py . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . Parse the arguments needed that are split in TrainingArguments, ModelArguments and DataTrainingArguments. The only variables that need configuration depending on your needs are model_args , data_args and training_args in Parameters : model_args of type ModelArguments : These are the arguments for the model that you want to use such as the model_name_or_path, tokenizer_name etc. You'll need these to load the model and tokenizer. Minimum setup: model_args = ModelArguments ( model_name_or_path , model_type , tokenizer_name , ) model_name_or_path path to existing transformers model or name of transformer model to be used: bert-base-cased , roberta-base , gpt2 etc. More details here . model_type type of model used: bert , roberta , gpt2 . More details here . tokenizer_name tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased , roberta-base , gpt2 etc. data_args of type DataTrainingArguments : These are as the name suggests arguments needed for the dataset. Such as the directory name where your files are stored etc. You'll need these to load/process the dataset. Minimum setup: data_args = DataArgs ( train_data_file , eval_data_file , mlm , ) train_data_file path to your dataset. This is a plain file that contains all your text data to train a model. Use each line to separate examples: i.e. if you have a dataset composed of multiple text documents, create a single file with each line in the file associated to a text document. eval_data_file same story as train_data_file . This file is used to evaluate the model performance mlm is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta . training_args of type TrainingArguments : These are the training hyper-parameters such as learning rate, batch size, weight decay, gradient accumulation steps etc. See all possible arguments here . These are used by the Trainer. Minimum setup: model_args training_args = TrainingArguments ( output_dir , do_train , do_eval , ) output_dir path where to save the pre-trained model. do_train variable to signal if you're using train data or not. Set it to True if you mentioned train_data_file . do_eval variable to signal if you're using evaluate data or not. Set it to True if you mentioned eval_data_file .","title":"How to use this notebook?"},{"location":"tutorial_notebooks/pretrain_transformer/#example","text":"","title":"Example:"},{"location":"tutorial_notebooks/pretrain_transformer/#pre-train-bert","text":"In the Parameters section use arguments: # process model arguments. Check Info - Notes for more details model_args = ModelArguments ( model_name_or_path = 'bert-base-cased' , model_type = 'bert' , tokenizer_name = 'bert-base-cased' , ) # process data arguments. Check Info - Notes for more details data_args = DataArgs ( train_data_file = '/content/your_train_data' , eval_data_file = '/content/your_test_data, mlm = True , ) # process training arguments. Check Info - Notes for more details training_args = TrainingArguments ( output_dir = '/content/pretrained_bert' , do_train = True , do_eval = False )","title":"Pre-train Bert"},{"location":"tutorial_notebooks/pretrain_transformer/#notes","text":"Parameters details got from here . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . I plan on testing more models in the future. I used the The WikiText Long Term Dependency Language Modeling Dataset as an example. To reduce training time I used the evaluate split as training and test split as evaluation! .","title":"Notes:"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/","text":"Pretrain Transformers Models in PyTorch using Hugging Face Transformers Pretrain 67 transformers models on your custom dataset. Disclaimer: The format of this tutorial notebook is very similar with my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format. This notebook is used to pretrain transformers models using Huggingface on your own custom dataset. What do I mean by pretrain transformers ? The definition of pretraining is to train in advance . That is exactly what I mean! Train a transformer model to use it as a pretrained transformers model which can be used to fine-tune it on a specific task! I also use the term fine-tune where I mean to continue training a pretrained model on a custom dataset. I know it is confusing and I hope I'm not making it worse. At the end of the day you are training a transformer model that was previously trained or not! With the AutoClasses functionality we can reuse the code on a large number of transformers models! This notebook is designed to: Use an already pretrained transformers model and fine-tune (continue training) it on your custom dataset. Train a transformer model from scratch on a custom dataset. This requires an already trained (pretrained) tokenizer. This notebook will use by default the pretrained tokenizer if an already trained tokenizer is no provided. This notebook is heavily inspired from the Hugging Face script used for training language models: transformers/tree/master/examples/language-modeling . I basically adapted that script to work nicely in a notebook with a lot more comments. Notes from transformers/tree/master/examples/language-modeling : Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss. What should I know for this notebook? Since I am using PyTorch to fine-tune our transformers models any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too. In this notebook I am using raw text data to pretrain / train / fine-tune transformers models . There is no need for labeled data since we are not doing classification. The Transformers library handles the text files in same way as the original implementation of each model did. How to use this notebook? Like with every project, I built this notebook with reusability in mind. This notebook uses a custom dataset from .txt files. Since the dataset does not come in a single .txt file I created a custom function movie_reviews_to_file that reads the dataset and creates the text file. The way I load the .txt files can be easily reused for any other dataset. The only modifications needed to use your own dataset will be in the paths provided to the train .txt file and evaluation .txt file. All parameters that need to be changed are under the Parameters Setup section. Each parameter is nicely commented and structured to be as intuitive as possible. What transformers models work with this notebook? A lot of people will probably use it for Bert. When there is a need to run a different transformer model architecture, which one would work with this code? Since the name of the notebooks is pretrain_transformers it should work with more than one type of transformers. I ran this notebook across all the pretrained models found on Hugging Face Transformer. This way you know ahead of time if the model you plan to use works with this code without any modifications. The list of pretrained transformers models that work with this notebook can be found here . There are 67 models that worked \ud83d\ude04 and 39 models that failed to work \ud83d\ude22 with this notebook. Remember these are pretrained models and fine-tuned on custom dataset. Dataset This notebook will cover pretraining transformers on a custom dataset. I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with. Coding Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant\u200a-\u200ashow the output. I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations. Downloads Download the Large Movie Review Dataset and unzip it locally. # Download the dataset. ! wget - q - nc http : // ai . stanford . edu /~ amaas / data / sentiment / aclImdb_v1 . tar . gz # Unzip the dataset. ! tar - zxf / content / aclImdb_v1 . tar . gz Installs transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. # Install transformers library. ! pip install - q git + https : // github . com / huggingface / transformers . git # Install helper functions. ! pip install - q git + https : // github . com / gmihaila / ml_things . git Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 6.7MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 48.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 49.0MB/s Building wheel for transformers (PEP 517) ... done Building wheel for sacremoses (setup.py) ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Building wheel for ml-things (setup.py) ... done Building wheel for ftfy (setup.py) ... done Imports Import all needed libraries for this notebook. Declare basic parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. device - Look for gpu to use. I will use cpu by default if no gpu found. import io import os import math import torch import warnings from tqdm.notebook import tqdm from ml_things import plot_dict , fix_text from transformers import ( CONFIG_MAPPING , MODEL_FOR_MASKED_LM_MAPPING , MODEL_FOR_CAUSAL_LM_MAPPING , PreTrainedTokenizer , TrainingArguments , AutoConfig , AutoTokenizer , AutoModelWithLMHead , AutoModelForCausalLM , AutoModelForMaskedLM , LineByLineTextDataset , TextDataset , DataCollatorForLanguageModeling , DataCollatorForWholeWordMask , DataCollatorForPermutationLanguageModeling , PretrainedConfig , Trainer , set_seed , ) # Set seed for reproducibility, set_seed ( 123 ) # Look for gpu to use. Will use `cpu` by default if no gpu found. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) Helper Functions I like to keep all Classes and functions that will be used in this notebook under this section to help maintain a clean look of the notebook: movie_reviews_to_file(path_data: str, path_texts_file: str) As I mentioned before, we will need .txt files to run this notebook. Since the Large Movie Review Dataset comes in multiple files with different labels I created this function to put together all data in a single .txt file. Examples are saved on each line of the file. The path_data points to the path where data files are present and path_texts_file will be the .txt file containing all data. ModelDataArguments This class follows similar format as the [transformers](( huggingface/transformers ) library. The main difference is the way I combined multiple types of arguments into one and used rules to make sure the arguments used are correctly set. Here are all argument detailed (they are also mentioned in the class documentation): train_data_file : Path to your .txt file dataset. If you have an example on each line of the file make sure to use line_by_line=True . If the data file contains all text data without any special grouping use line_by_line=False to move a block_size window across the text file. eval_data_file : Path to evaluation .txt file. It has the same format as train_data_file . line_by_line : If the train_data_file and eval_data_file contains separate examples on each line set line_by_line=True . If there is no separation between examples and train_data_file and eval_data_file contains continuous text then line_by_line=False and a window of block_size will be moved across the files to acquire examples. mlm : Is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta and set to False otherwise. There are functions that will raise ValueError if this argument is not set accordingly. whole_word_mask : Used as flag to determine if we decide to use whole word masking or not. Whole word masking means that whole words will be masked during training instead of tokens which can be chunks of words. mlm_probability : Used when training masked language models. Needs to have mlm=True . It represents the probability of masking tokens when training model. plm_probability : Flag to define the ratio of length of a span of masked tokens to surrounding context length for permutation language modeling. Used for XLNet. max_span_length : Flag may also be used to limit the length of a span of masked tokens used for permutation language modeling. Used for XLNet. block_size : It refers to the windows size that is moved across the text file. Set to -1 to use maximum allowed length. overwrite_cache : If there are any cached files, overwrite them. model_type : Type of model used: bert, roberta, gpt2. More details here . model_config_name : Config of model used: bert, roberta, gpt2. More details here . tokenizer_name : Tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased, roberta-base, gpt2 etc. model_name_or_path : Path to existing transformers model or name of transformer model to be used: bert-base-cased, roberta-base, gpt2 etc. More details here . model_cache_dir : Path to cache files. It helps to save time when re-running code. get_model_config(args: ModelDataArguments) Get model configuration. Using the ModelDataArguments to return the model configuration. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. Returns: Model transformers configuration. Raises: ValueError: If mlm=True and model_type is NOT in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"]. We need to use a masked language model in order to set mlm=True . get_tokenizer(args: ModelDataArguments) Get model tokenizer.Using the ModelDataArguments return the model tokenizer and change block_size form args if needed. Here are all argument detailed: args : Model and data configuration arugments needed to perform pretraining. Returns: Model transformers tokenizer. get_model(args: ModelDataArguments, model_config) Get model. Using the ModelDataArguments return the actual model. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. model_config : Model transformers configuration. Returns: PyTorch model. get_dataset(args: ModelDataArguments, tokenizer: PreTrainedTokenizer, evaluate: bool=False) Process dataset file into PyTorch Dataset. Using the ModelDataArguments return the actual model. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. tokenizer : Model transformers tokenizer. evaluate : If set to True the test / validation file is being handled. If set to False the train file is being handled. Returns: PyTorch Dataset that contains file's data. get_collator(args: ModelDataArguments, model_config: PretrainedConfig, tokenizer: PreTrainedTokenizer) Get appropriate collator function. Collator function will be used to collate a PyTorch Dataset object. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. model_config : Model transformers configuration. tokenizer : Model transformers tokenizer. Returns: Transformers specific data collator. def movie_reviews_to_file ( path_data : str , path_texts_file : str ): r \"\"\"Reading in all data from path and saving it into a single `.txt` file. In the pretraining process of our transformers model we require a text file. This function is designed to work for the Movie Reviews Dataset. You wil have to create your own function to move all examples into a text file if you don't already have a text file with all your unlabeled data. Arguments: path_data (:obj:`str`): Path to the Movie Review Dataset partition. We only have `\\train` and `test` partitions. path_texts_file (:obj:`str`): File path of the generated `.txt` file that contains one example / line. \"\"\" # Check if path exists. if not os . path . isdir ( path_data ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) # Check max sequence length. texts = [] print ( 'Reading ` %s ` partition...' % ( os . path . basename ( path_data ))) # Since the labels are defined by folders with data we loop # through each label. for label in [ 'neg' , 'pos' ]: sentiment_path = os . path . join ( path_data , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:30] # SAMPLE FOR DEBUGGING. # Go through each file and read its content. for file_name in tqdm ( files_names , desc = label , unit = 'files' ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. texts . append ( content ) # Move list to single string. all_texts = ' \\n ' . join ( texts ) # Send all texts string to single file. io . open ( file = path_texts_file , mode = 'w' , encoding = 'utf-8' ) . write ( all_texts ) # Print when done. print ( '`.txt` file saved in ` %s ` \\n ' % path_texts_file ) return class ModelDataArguments ( object ): r \"\"\"Define model and data configuration needed to perform pretraining. Eve though all arguments are optional there still needs to be a certain number of arguments that require values attributed. Arguments: train_data_file (:obj:`str`, `optional`): Path to your .txt file dataset. If you have an example on each line of the file make sure to use line_by_line=True. If the data file contains all text data without any special grouping use line_by_line=False to move a block_size window across the text file. This argument is optional and it will have a `None` value attributed inside the function. eval_data_file (:obj:`str`, `optional`): Path to evaluation .txt file. It has the same format as train_data_file. This argument is optional and it will have a `None` value attributed inside the function. line_by_line (:obj:`bool`, `optional`, defaults to :obj:`False`): If the train_data_file and eval_data_file contains separate examples on each line then line_by_line=True. If there is no separation between examples and train_data_file and eval_data_file contains continuous text then line_by_line=False and a window of block_size will be moved across the files to acquire examples. This argument is optional and it has a default value. mlm (:obj:`bool`, `optional`, defaults to :obj:`False`): Is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta and set to False otherwise. There are functions that will raise ValueError if this argument is not set accordingly. This argument is optional and it has a default value. whole_word_mask (:obj:`bool`, `optional`, defaults to :obj:`False`): Used as flag to determine if we decide to use whole word masking or not. Whole word masking means that whole words will be masked during training instead of tokens which can be chunks of words. This argument is optional and it has a default value. mlm_probability(:obj:`float`, `optional`, defaults to :obj:`0.15`): Used when training masked language models. Needs to have mlm set to True. It represents the probability of masking tokens when training model. This argument is optional and it has a default value. plm_probability (:obj:`float`, `optional`, defaults to :obj:`float(1/6)`): Flag to define the ratio of length of a span of masked tokens to surrounding context length for permutation language modeling. Used for XLNet. This argument is optional and it has a default value. max_span_length (:obj:`int`, `optional`, defaults to :obj:`5`): Flag may also be used to limit the length of a span of masked tokens used for permutation language modeling. Used for XLNet. This argument is optional and it has a default value. block_size (:obj:`int`, `optional`, defaults to :obj:`-1`): It refers to the windows size that is moved across the text file. Set to -1 to use maximum allowed length. This argument is optional and it has a default value. overwrite_cache (:obj:`bool`, `optional`, defaults to :obj:`False`): If there are any cached files, overwrite them. This argument is optional and it has a default value. model_type (:obj:`str`, `optional`): Type of model used: bert, roberta, gpt2. More details: https://huggingface.co/transformers/pretrained_models.html This argument is optional and it will have a `None` value attributed inside the function. model_config_name (:obj:`str`, `optional`): Config of model used: bert, roberta, gpt2. More details: https://huggingface.co/transformers/pretrained_models.html This argument is optional and it will have a `None` value attributed inside the function. tokenizer_name: (:obj:`str`, `optional`) Tokenizer used to process data for training the model. It usually has same name as model_name_or_path: bert-base-cased, roberta-base, gpt2 etc. This argument is optional and it will have a `None` value attributed inside the function. model_name_or_path (:obj:`str`, `optional`): Path to existing transformers model or name of transformer model to be used: bert-base-cased, roberta-base, gpt2 etc. More details: https://huggingface.co/transformers/pretrained_models.html This argument is optional and it will have a `None` value attributed inside the function. model_cache_dir (:obj:`str`, `optional`): Path to cache files to save time when re-running code. This argument is optional and it will have a `None` value attributed inside the function. Raises: ValueError: If `CONFIG_MAPPING` is not loaded in global variables. ValueError: If `model_type` is not present in `CONFIG_MAPPING.keys()`. ValueError: If `model_type`, `model_config_name` and `model_name_or_path` variables are all `None`. At least one of them needs to be set. warnings: If `model_config_name` and `model_name_or_path` are both `None`, the model will be trained from scratch. ValueError: If `tokenizer_name` and `model_name_or_path` are both `None`. We need at least one of them set to load tokenizer. \"\"\" def __init__ ( self , train_data_file = None , eval_data_file = None , line_by_line = False , mlm = False , mlm_probability = 0.15 , whole_word_mask = False , plm_probability = float ( 1 / 6 ), max_span_length = 5 , block_size =- 1 , overwrite_cache = False , model_type = None , model_config_name = None , tokenizer_name = None , model_name_or_path = None , model_cache_dir = None ): # Make sure CONFIG_MAPPING is imported from transformers module. if 'CONFIG_MAPPING' not in globals (): raise ValueError ( 'Could not find `CONFIG_MAPPING` imported! Make sure' \\ ' to import it from `transformers` module!' ) # Make sure model_type is valid. if ( model_type is not None ) and ( model_type not in CONFIG_MAPPING . keys ()): raise ValueError ( 'Invalid `model_type`! Use one of the following: %s ' % ( str ( list ( CONFIG_MAPPING . keys ())))) # Make sure that model_type, model_config_name and model_name_or_path # variables are not all `None`. if not any ([ model_type , model_config_name , model_name_or_path ]): raise ValueError ( 'You can`t have all `model_type`, `model_config_name`,' \\ ' `model_name_or_path` be `None`! You need to have' \\ 'at least one of them set!' ) # Check if a new model will be loaded from scratch. if not any ([ model_config_name , model_name_or_path ]): # Setup warning to show pretty. This is an overkill warnings . formatwarning = lambda message , category , * args , ** kwargs : \\ ' %s : %s \\n ' % ( category . __name__ , message ) # Display warning. warnings . warn ( 'You are planning to train a model from scratch! \ud83d\ude40' ) # Check if a new tokenizer wants to be loaded. # This feature is not supported! if not any ([ tokenizer_name , model_name_or_path ]): # Can't train tokenizer from scratch here! Raise error. raise ValueError ( 'You want to train tokenizer from scratch! ' \\ 'That is not possible yet! You can train your own ' \\ 'tokenizer separately and use path here to load it!' ) # Set all data related arguments. self . train_data_file = train_data_file self . eval_data_file = eval_data_file self . line_by_line = line_by_line self . mlm = mlm self . whole_word_mask = whole_word_mask self . mlm_probability = mlm_probability self . plm_probability = plm_probability self . max_span_length = max_span_length self . block_size = block_size self . overwrite_cache = overwrite_cache # Set all model and tokenizer arguments. self . model_type = model_type self . model_config_name = model_config_name self . tokenizer_name = tokenizer_name self . model_name_or_path = model_name_or_path self . model_cache_dir = model_cache_dir return def get_model_config ( args : ModelDataArguments ): r \"\"\" Get model configuration. Using the ModelDataArguments return the model configuration. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. Returns: :obj:`PretrainedConfig`: Model transformers configuration. Raises: ValueError: If `mlm=True` and `model_type` is NOT in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"]. We need to use a masked language model in order to set `mlm=True`. \"\"\" # Check model configuration. if args . model_config_name is not None : # Use model configure name if defined. model_config = AutoConfig . from_pretrained ( args . model_config_name , cache_dir = args . model_cache_dir ) elif args . model_name_or_path is not None : # Use model name or path if defined. model_config = AutoConfig . from_pretrained ( args . model_name_or_path , cache_dir = args . model_cache_dir ) else : # Use config mapping if building model from scratch. model_config = CONFIG_MAPPING [ args . model_type ]() # Make sure `mlm` flag is set for Masked Language Models (MLM). if ( model_config . model_type in [ \"bert\" , \"roberta\" , \"distilbert\" , \"camembert\" ]) and ( args . mlm is False ): raise ValueError ( 'BERT and RoBERTa-like models do not have LM heads ' \\ 'butmasked LM heads. They must be run setting `mlm=True`' ) # Adjust block size for xlnet. if model_config . model_type == \"xlnet\" : # xlnet used 512 tokens when training. args . block_size = 512 # setup memory length model_config . mem_len = 1024 return model_config def get_tokenizer ( args : ModelDataArguments ): r \"\"\" Get model tokenizer. Using the ModelDataArguments return the model tokenizer and change `block_size` form `args` if needed. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. Returns: :obj:`PreTrainedTokenizer`: Model transformers tokenizer. \"\"\" # Check tokenizer configuration. if args . tokenizer_name : # Use tokenizer name if define. tokenizer = AutoTokenizer . from_pretrained ( args . tokenizer_name , cache_dir = args . model_cache_dir ) elif args . model_name_or_path : # Use tokenizer name of path if defined. tokenizer = AutoTokenizer . from_pretrained ( args . model_name_or_path , cache_dir = args . model_cache_dir ) # Setp data block size. if args . block_size <= 0 : # Set block size to maximum length of tokenizer. # Input block size will be the max possible for the model. # Some max lengths are very large and will cause a args . block_size = tokenizer . model_max_length else : # Never go beyond tokenizer maximum length. args . block_size = min ( args . block_size , tokenizer . model_max_length ) return tokenizer def get_model ( args : ModelDataArguments , model_config ): r \"\"\" Get model. Using the ModelDataArguments return the actual model. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. model_config (:obj:`PretrainedConfig`): Model transformers configuration. Returns: :obj:`torch.nn.Module`: PyTorch model. \"\"\" # Make sure MODEL_FOR_MASKED_LM_MAPPING and MODEL_FOR_CAUSAL_LM_MAPPING are # imported from transformers module. if ( 'MODEL_FOR_MASKED_LM_MAPPING' not in globals ()) and \\ ( 'MODEL_FOR_CAUSAL_LM_MAPPING' not in globals ()): raise ValueError ( 'Could not find `MODEL_FOR_MASKED_LM_MAPPING` and' \\ ' `MODEL_FOR_MASKED_LM_MAPPING` imported! Make sure to' \\ ' import them from `transformers` module!' ) # Check if using pre-trained model or train from scratch. if args . model_name_or_path : # Use pre-trained model. if type ( model_config ) in MODEL_FOR_MASKED_LM_MAPPING . keys (): # Masked language modeling head. return AutoModelForMaskedLM . from_pretrained ( args . model_name_or_path , from_tf = bool ( \".ckpt\" in args . model_name_or_path ), config = model_config , cache_dir = args . model_cache_dir , ) elif type ( model_config ) in MODEL_FOR_CAUSAL_LM_MAPPING . keys (): # Causal language modeling head. return AutoModelForCausalLM . from_pretrained ( args . model_name_or_path , from_tf = bool ( \".ckpt\" in args . model_name_or_path ), config = model_config , cache_dir = args . model_cache_dir ) else : raise ValueError ( 'Invalid `model_name_or_path`! It should be in %s or %s !' % ( str ( MODEL_FOR_MASKED_LM_MAPPING . keys ()), str ( MODEL_FOR_CAUSAL_LM_MAPPING . keys ()))) else : # Use model from configuration - train from scratch. print ( \"Training new model from scratch!\" ) return AutoModelWithLMHead . from_config ( config ) def get_dataset ( args : ModelDataArguments , tokenizer : PreTrainedTokenizer , evaluate : bool = False ): r \"\"\" Process dataset file into PyTorch Dataset. Using the ModelDataArguments return the actual model. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. tokenizer (:obj:`PreTrainedTokenizer`): Model transformers tokenizer. evaluate (:obj:`bool`, `optional`, defaults to :obj:`False`): If set to `True` the test / validation file is being handled. If set to `False` the train file is being handled. Returns: :obj:`Dataset`: PyTorch Dataset that contains file's data. \"\"\" # Get file path for either train or evaluate. file_path = args . eval_data_file if evaluate else args . train_data_file # Check if `line_by_line` flag is set to `True`. if args . line_by_line : # Each example in data file is on each line. return LineByLineTextDataset ( tokenizer = tokenizer , file_path = file_path , block_size = args . block_size ) else : # All data in file is put together without any separation. return TextDataset ( tokenizer = tokenizer , file_path = file_path , block_size = args . block_size , overwrite_cache = args . overwrite_cache ) def get_collator ( args : ModelDataArguments , model_config : PretrainedConfig , tokenizer : PreTrainedTokenizer ): r \"\"\" Get appropriate collator function. Collator function will be used to collate a PyTorch Dataset object. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. model_config (:obj:`PretrainedConfig`): Model transformers configuration. tokenizer (:obj:`PreTrainedTokenizer`): Model transformers tokenizer. Returns: :obj:`data_collator`: Transformers specific data collator. \"\"\" # Special dataset handle depending on model type. if model_config . model_type == \"xlnet\" : # Configure collator for XLNET. return DataCollatorForPermutationLanguageModeling ( tokenizer = tokenizer , plm_probability = args . plm_probability , max_span_length = args . max_span_length , ) else : # Configure data for rest of model types. if args . mlm and args . whole_word_mask : # Use whole word masking. return DataCollatorForWholeWordMask ( tokenizer = tokenizer , mlm_probability = args . mlm_probability , ) else : # Regular language modeling. return DataCollatorForLanguageModeling ( tokenizer = tokenizer , mlm = args . mlm , mlm_probability = args . mlm_probability , ) Parameters Setup Declare the rest of the parameters used for this notebook: model_data_args contains all arguments needed to setup dataset, model configuration, model tokenizer and the actual model. This is created using the ModelDataArguments class. training_args contain all arguments needed to use the Trainer functionality from Transformers that allows us to train transformers models in PyTorch very easy. You can find the complete documentation here . There are a lot of parameters that can be set to allow multiple functionalities. I only used the following parameters (the comments are inspired from the HuggingFace documentation of TrainingArguments : output_dir : The output directory where the model predictions and checkpoints will be written. I set it up to pretrained_bert_model where the model and will be saved. overwrite_output_dir : Overwrite the content of the output directory. I set it to True in case I run the notebook multiple times I only care about the last run. do_train : Whether to run training or not. I set this parameter to True because I want to train the model on my custom dataset. do_eval : Whether to run evaluation on the evaluation files or not. I set it to True since I have test data file and I want to evaluate how well the model trains. per_device_train_batch_size : Batch size GPU/TPU core/CPU training. I set it to 2 for this example. I recommend setting it up as high as your GPU memory allows you. per_device_eval_batch_size : Batch size GPU/TPU core/CPU for evaluation.I set this value to 100 since it's not dealing with gradients. evaluation_strategy : Evaluation strategy to adopt during training: no : No evaluation during training; steps : Evaluate every eval_steps; epoch`: Evaluate every end of epoch. I set it to 'steps' since I want to evaluate model more often. logging_steps : How often to show logs. I will se this to plot history loss and calculate perplexity. I set this to 20 just as an example. If your evaluate data is large you might not want to run it that often because it will significantly slow down training time. eval_steps : Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set. Since I want to evaluate model ever logging_steps I will set this to None since it will inherit same value as logging_steps . prediction_loss_only : Set prediction loss to True in order to return loss for perplexity calculation. Since I want to calculate perplexity I set this to True since I want to monitor loss and perplexity (which is exp(loss)). learning_rate : The initial learning rate for Adam. Defaults is set to 5e-5 . weight_decay : The weight decay to apply (if not zero)Defaults is set to 0 . adam_epsilon : Epsilon for the Adam optimizer. Defaults to 1e-8 . max_grad_norm : Maximum gradient norm (for gradient clipping). Defaults to 0 . num_train_epochs : Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training). I set it to 2 at most. Since the custom dataset will be a lot smaller than the original dataset the model was trained on we don't want to overfit. save_steps : Number of updates steps before two checkpoint saves. Defaults to 500 . # Define arguments for data, tokenizer and model arguments. # See comments in `ModelDataArguments` class. model_data_args = ModelDataArguments ( train_data_file = '/content/train.txt' , eval_data_file = '/content/test.txt' , line_by_line = True , mlm = True , whole_word_mask = True , mlm_probability = 0.15 , plm_probability = float ( 1 / 6 ), max_span_length = 5 , block_size = 50 , overwrite_cache = False , model_type = 'bert' , model_config_name = 'bert-base-cased' , tokenizer_name = 'bert-base-cased' , model_name_or_path = 'bert-base-cased' , model_cache_dir = None , ) # Define arguments for training # Note: I only used the arguments I care about. `TrainingArguments` contains # a lot more arguments. For more details check the awesome documentation: # https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments training_args = TrainingArguments ( # The output directory where the model predictions # and checkpoints will be written. output_dir = 'pretrain_bert' , # Overwrite the content of the output directory. overwrite_output_dir = True , # Whether to run training or not. do_train = True , # Whether to run evaluation on the dev or not. do_eval = True , # Batch size GPU/TPU core/CPU training. per_device_train_batch_size = 10 , # Batch size GPU/TPU core/CPU for evaluation. per_device_eval_batch_size = 100 , # evaluation strategy to adopt during training # `no`: No evaluation during training. # `steps`: Evaluate every `eval_steps`. # `epoch`: Evaluate every end of epoch. evaluation_strategy = 'steps' , # How often to show logs. I will se this to # plot history loss and calculate perplexity. logging_steps = 700 , # Number of update steps between two # evaluations if evaluation_strategy=\"steps\". # Will default to the same value as l # logging_steps if not set. eval_steps = None , # Set prediction loss to `True` in order to # return loss for perplexity calculation. prediction_loss_only = True , # The initial learning rate for Adam. # Defaults to 5e-5. learning_rate = 5e-5 , # The weight decay to apply (if not zero). weight_decay = 0 , # Epsilon for the Adam optimizer. # Defaults to 1e-8 adam_epsilon = 1e-8 , # Maximum gradient norm (for gradient # clipping). Defaults to 0. max_grad_norm = 1.0 , # Total number of training epochs to perform # (if not an integer, will perform the # decimal part percents of # the last epoch before stopping training). num_train_epochs = 2 , # Number of updates steps before two checkpoint saves. # Defaults to 500 save_steps = - 1 , ) Load Configuration, Tokenizer and Model Loading the three essential parts of the pretrained transformers: configuration, tokenizer and model. Since I use the AutoClass functionality from Hugging Face I only need to worry about the model's name as input and the rest is handled by the transformers library. I will be calling each three functions created in the Helper Functions tab that help return config of the model, tokenizer of the model and the actual PyTorch model . After model is loaded is always good practice to resize the model depending on the tokenizer size. This means that the tokenizer's vocabulary will be aligned with the models embedding layer. This is very useful when we have a different tokenizer that the pretrained one or we train a transformer model from scratch. # Load model configuration. print ( 'Loading model configuration...' ) config = get_model_config ( model_data_args ) # Load model tokenizer. print ( 'Loading model`s tokenizer...' ) tokenizer = get_tokenizer ( model_data_args ) # Loading model. print ( 'Loading actual model...' ) model = get_model ( model_data_args , config ) # Resize model to fit all tokens in tokenizer. model . resize_token_embeddings ( len ( tokenizer )) Loading model configuration... Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|433/433 [00:01<00:00, 285B/s] Loading model`s tokenizer... Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|433/433 [00:01<00:00, 285B/s] Loading actual model... Downloading: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|436M/436M [00:36<00:00, 11.9MB/s] Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias'] - This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Embedding(28996, 768, padding_idx=0) Dataset and Collator This is where I create the PyTorch Dataset and data collator objects that will be used to feed data into our model. This is where I use the MovieReviewsDataset text files created with the movie_reviews_to_file function. Since data is partitioned for both train and test I will create two text files: one used for train and one used for evaluation. I strongly recommend to use a validation text file in order to determine how much training is needed in order to avoid overfitting. After you figure out what parameters yield the best results, the validation file can be incorporated in train and run a final train with the whole dataset. The data collator is used to format the PyTorch Dataset outputs to match the output of our specific transformers model: i.e. for Bert it will created the masked tokens needed to train. # Create texts file from train data. movie_reviews_to_file ( path_data = '/content/aclImdb/train' , path_texts_file = '/content/train.txt' ) # Create texts file from test data. movie_reviews_to_file ( path_data = '/content/aclImdb/test' , path_texts_file = '/content/test.txt' ) # Setup train dataset if `do_train` is set. print ( 'Creating train dataset...' ) train_dataset = get_dataset ( model_data_args , tokenizer = tokenizer , evaluate = False ) if training_args . do_train else None # Setup evaluation dataset if `do_eval` is set. print ( 'Creating evaluate dataset...' ) eval_dataset = get_dataset ( model_data_args , tokenizer = tokenizer , evaluate = True ) if training_args . do_eval else None # Get data collator to modify data format depending on type of model used. data_collator = get_collator ( model_data_args , config , tokenizer ) # Check how many logging prints you'll have. This is to avoid overflowing the # notebook with a lot of prints. Display warning to user if the logging steps # that will be displayed is larger than 100. if ( len ( train_dataset ) // training_args . per_device_train_batch_size \\ // training_args . logging_steps * training_args . num_train_epochs ) > 100 : # Display warning. warnings . warn ( 'Your `logging_steps` value will will do a lot of printing!' \\ ' Consider increasing `logging_steps` to avoid overflowing' \\ ' the notebook with a lot of prints!' ) Reading `train` partition... neg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] pos: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] `.txt` file saved in `/content/train.txt` Reading `test` partition... neg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] pos: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] `.txt` file saved in `/content/train.txt` Creating train dataset... Creating evaluate dataset... Train Hugging Face was very nice to us for creating the Trainer class. This helps make PyTorch model training of transformers very easy! We just need to make sure we loaded the proper parameters and everything else is taking care of! At the end of the training the tokenizer is saved along with the model so you can easily re-use it later or even load in on Hugging Face Models. I configured the arguments to display both train and validation loss at every logging_steps . It gives us a sense of how well the model is trained. # Initialize Trainer. print ( 'Loading `trainer`...' ) trainer = Trainer ( model = model , args = training_args , data_collator = data_collator , train_dataset = train_dataset , eval_dataset = eval_dataset , ) # Check model path to save. if training_args . do_train : print ( 'Start training...' ) # Setup model path if the model to train loaded from a local path. model_path = ( model_data_args . model_name_or_path if model_data_args . model_name_or_path is not None and os . path . isdir ( model_data_args . model_name_or_path ) else None ) # Run training. trainer . train ( model_path = model_path ) # Save model. trainer . save_model () # For convenience, we also re-save the tokenizer to the same directory, # so that you can share your model easily on huggingface.co/models =). if trainer . is_world_process_zero (): tokenizer . save_pretrained ( training_args . output_dir ) Loading `trainer`... Start training... |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|[5000/5000 09:43, Epoch 2/2] Step Training Loss Validation Loss 700 2.804672 2.600590 1400 2.666996 2.548267 2100 2.625075 2.502431 2800 2.545872 2.485056 3500 2.470102 2.444808 4200 2.464950 2.420487 4900 2.436973 2.410310 Plot Train The Trainer class is so useful that it will record the log history for us. I use this to access the train and validation losses recorded at each logging_steps during training. Since we are training / fine-tuning / extended training or pretraining (depending what terminology you use) a language model, we want to compute the perplexity. This is what Wikipedia says about perplexity: In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample. We can look at the perplexity plot in the same way we look at the loss plot: the lower the better and if the validation perplexity starts to increase we are starting to overfit the model. Note: It looks from the plots that the train loss is higher than validation loss. That means that our validation data is too easy for the model and we should use a different validation dataset. Since the purpose of this notebook is to show how to train transformers models and provide tools to evaluate such process I will leave the results as is . # Keep track of train and evaluate loss. loss_history = { 'train_loss' :[], 'eval_loss' :[]} # Keep track of train and evaluate perplexity. # This is a metric useful to track for language models. perplexity_history = { 'train_perplexity' :[], 'eval_perplexity' :[]} # Loop through each log history. for log_history in trainer . state . log_history : if 'loss' in log_history . keys (): # Deal with trianing loss. loss_history [ 'train_loss' ] . append ( log_history [ 'loss' ]) perplexity_history [ 'train_perplexity' ] . append ( math . exp ( log_history [ 'loss' ])) elif 'eval_loss' in log_history . keys (): # Deal with eval loss. loss_history [ 'eval_loss' ] . append ( log_history [ 'eval_loss' ]) perplexity_history [ 'eval_perplexity' ] . append ( math . exp ( log_history [ 'eval_loss' ])) # Plot Losses. plot_dict ( loss_history , start_step = training_args . logging_steps , step_size = training_args . logging_steps , use_title = 'Loss' , use_xlabel = 'Train Steps' , use_ylabel = 'Values' , magnify = 2 ) print () # Plot Perplexities. plot_dict ( perplexity_history , start_step = training_args . logging_steps , step_size = training_args . logging_steps , use_title = 'Perplexity' , use_xlabel = 'Train Steps' , use_ylabel = 'Values' , magnify = 2 ) Evaluate For the final evaluation we can have a separate test set that we use to do our final perplexity evaluation. For simplicity I used the same validation text file for the final evaluation. That is the reason I get the same results as the last validation perplexity plot value. # check if `do_eval` flag is set. if training_args . do_eval : # capture output if trainer evaluate. eval_output = trainer . evaluate () # compute perplexity from model loss. perplexity = math . exp ( eval_output [ \"eval_loss\" ]) print ( ' \\n Evaluate Perplexity: {:10,.2f} ' . format ( perplexity )) else : print ( 'No evaluation needed. No evaluation data provided, `do_eval=False`!' ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|[250/250 00:25] Evaluate Perplexity: 11.01 Final Note If you made it this far Congrats! \ud83c\udf8a and Thank you! \ud83d\ude4f for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you see something wrong please let me know by opening an issue on my ml_things GitHub repository ! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can. Contact \ud83c\udfa3 \ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"Pretrain Transformers"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#pretrain-transformers-models-in-pytorch-using-hugging-face-transformers","text":"","title":"Pretrain Transformers Models in PyTorch using Hugging Face Transformers"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#pretrain-67-transformers-models-on-your-custom-dataset","text":"Disclaimer: The format of this tutorial notebook is very similar with my other tutorial notebooks. This is done intentionally in order to keep readers familiar with my format. This notebook is used to pretrain transformers models using Huggingface on your own custom dataset. What do I mean by pretrain transformers ? The definition of pretraining is to train in advance . That is exactly what I mean! Train a transformer model to use it as a pretrained transformers model which can be used to fine-tune it on a specific task! I also use the term fine-tune where I mean to continue training a pretrained model on a custom dataset. I know it is confusing and I hope I'm not making it worse. At the end of the day you are training a transformer model that was previously trained or not! With the AutoClasses functionality we can reuse the code on a large number of transformers models! This notebook is designed to: Use an already pretrained transformers model and fine-tune (continue training) it on your custom dataset. Train a transformer model from scratch on a custom dataset. This requires an already trained (pretrained) tokenizer. This notebook will use by default the pretrained tokenizer if an already trained tokenizer is no provided. This notebook is heavily inspired from the Hugging Face script used for training language models: transformers/tree/master/examples/language-modeling . I basically adapted that script to work nicely in a notebook with a lot more comments. Notes from transformers/tree/master/examples/language-modeling : Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.","title":"Pretrain 67 transformers models on your custom dataset."},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#what-should-i-know-for-this-notebook","text":"Since I am using PyTorch to fine-tune our transformers models any knowledge on PyTorch is very useful. Knowing a little bit about the transformers library helps too. In this notebook I am using raw text data to pretrain / train / fine-tune transformers models . There is no need for labeled data since we are not doing classification. The Transformers library handles the text files in same way as the original implementation of each model did.","title":"What should I know for this notebook?"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#how-to-use-this-notebook","text":"Like with every project, I built this notebook with reusability in mind. This notebook uses a custom dataset from .txt files. Since the dataset does not come in a single .txt file I created a custom function movie_reviews_to_file that reads the dataset and creates the text file. The way I load the .txt files can be easily reused for any other dataset. The only modifications needed to use your own dataset will be in the paths provided to the train .txt file and evaluation .txt file. All parameters that need to be changed are under the Parameters Setup section. Each parameter is nicely commented and structured to be as intuitive as possible.","title":"How to use this notebook?"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#what-transformers-models-work-with-this-notebook","text":"A lot of people will probably use it for Bert. When there is a need to run a different transformer model architecture, which one would work with this code? Since the name of the notebooks is pretrain_transformers it should work with more than one type of transformers. I ran this notebook across all the pretrained models found on Hugging Face Transformer. This way you know ahead of time if the model you plan to use works with this code without any modifications. The list of pretrained transformers models that work with this notebook can be found here . There are 67 models that worked \ud83d\ude04 and 39 models that failed to work \ud83d\ude22 with this notebook. Remember these are pretrained models and fine-tuned on custom dataset.","title":"What transformers models work with this notebook?"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#dataset","text":"This notebook will cover pretraining transformers on a custom dataset. I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with.","title":"Dataset"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#coding","text":"Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant\u200a-\u200ashow the output. I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.","title":"Coding"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#downloads","text":"Download the Large Movie Review Dataset and unzip it locally. # Download the dataset. ! wget - q - nc http : // ai . stanford . edu /~ amaas / data / sentiment / aclImdb_v1 . tar . gz # Unzip the dataset. ! tar - zxf / content / aclImdb_v1 . tar . gz","title":"Downloads"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#installs","text":"transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. # Install transformers library. ! pip install - q git + https : // github . com / huggingface / transformers . git # Install helper functions. ! pip install - q git + https : // github . com / gmihaila / ml_things . git Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.9MB 6.7MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 890kB 48.9MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.1MB 49.0MB/s Building wheel for transformers (PEP 517) ... done Building wheel for sacremoses (setup.py) ... done |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.2MB/s Building wheel for ml-things (setup.py) ... done Building wheel for ftfy (setup.py) ... done","title":"Installs"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#imports","text":"Import all needed libraries for this notebook. Declare basic parameters used for this notebook: set_seed(123) - Always good to set a fixed seed for reproducibility. device - Look for gpu to use. I will use cpu by default if no gpu found. import io import os import math import torch import warnings from tqdm.notebook import tqdm from ml_things import plot_dict , fix_text from transformers import ( CONFIG_MAPPING , MODEL_FOR_MASKED_LM_MAPPING , MODEL_FOR_CAUSAL_LM_MAPPING , PreTrainedTokenizer , TrainingArguments , AutoConfig , AutoTokenizer , AutoModelWithLMHead , AutoModelForCausalLM , AutoModelForMaskedLM , LineByLineTextDataset , TextDataset , DataCollatorForLanguageModeling , DataCollatorForWholeWordMask , DataCollatorForPermutationLanguageModeling , PretrainedConfig , Trainer , set_seed , ) # Set seed for reproducibility, set_seed ( 123 ) # Look for gpu to use. Will use `cpu` by default if no gpu found. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' )","title":"Imports"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#helper-functions","text":"I like to keep all Classes and functions that will be used in this notebook under this section to help maintain a clean look of the notebook: movie_reviews_to_file(path_data: str, path_texts_file: str) As I mentioned before, we will need .txt files to run this notebook. Since the Large Movie Review Dataset comes in multiple files with different labels I created this function to put together all data in a single .txt file. Examples are saved on each line of the file. The path_data points to the path where data files are present and path_texts_file will be the .txt file containing all data. ModelDataArguments This class follows similar format as the [transformers](( huggingface/transformers ) library. The main difference is the way I combined multiple types of arguments into one and used rules to make sure the arguments used are correctly set. Here are all argument detailed (they are also mentioned in the class documentation): train_data_file : Path to your .txt file dataset. If you have an example on each line of the file make sure to use line_by_line=True . If the data file contains all text data without any special grouping use line_by_line=False to move a block_size window across the text file. eval_data_file : Path to evaluation .txt file. It has the same format as train_data_file . line_by_line : If the train_data_file and eval_data_file contains separate examples on each line set line_by_line=True . If there is no separation between examples and train_data_file and eval_data_file contains continuous text then line_by_line=False and a window of block_size will be moved across the files to acquire examples. mlm : Is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta and set to False otherwise. There are functions that will raise ValueError if this argument is not set accordingly. whole_word_mask : Used as flag to determine if we decide to use whole word masking or not. Whole word masking means that whole words will be masked during training instead of tokens which can be chunks of words. mlm_probability : Used when training masked language models. Needs to have mlm=True . It represents the probability of masking tokens when training model. plm_probability : Flag to define the ratio of length of a span of masked tokens to surrounding context length for permutation language modeling. Used for XLNet. max_span_length : Flag may also be used to limit the length of a span of masked tokens used for permutation language modeling. Used for XLNet. block_size : It refers to the windows size that is moved across the text file. Set to -1 to use maximum allowed length. overwrite_cache : If there are any cached files, overwrite them. model_type : Type of model used: bert, roberta, gpt2. More details here . model_config_name : Config of model used: bert, roberta, gpt2. More details here . tokenizer_name : Tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased, roberta-base, gpt2 etc. model_name_or_path : Path to existing transformers model or name of transformer model to be used: bert-base-cased, roberta-base, gpt2 etc. More details here . model_cache_dir : Path to cache files. It helps to save time when re-running code. get_model_config(args: ModelDataArguments) Get model configuration. Using the ModelDataArguments to return the model configuration. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. Returns: Model transformers configuration. Raises: ValueError: If mlm=True and model_type is NOT in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"]. We need to use a masked language model in order to set mlm=True . get_tokenizer(args: ModelDataArguments) Get model tokenizer.Using the ModelDataArguments return the model tokenizer and change block_size form args if needed. Here are all argument detailed: args : Model and data configuration arugments needed to perform pretraining. Returns: Model transformers tokenizer. get_model(args: ModelDataArguments, model_config) Get model. Using the ModelDataArguments return the actual model. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. model_config : Model transformers configuration. Returns: PyTorch model. get_dataset(args: ModelDataArguments, tokenizer: PreTrainedTokenizer, evaluate: bool=False) Process dataset file into PyTorch Dataset. Using the ModelDataArguments return the actual model. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. tokenizer : Model transformers tokenizer. evaluate : If set to True the test / validation file is being handled. If set to False the train file is being handled. Returns: PyTorch Dataset that contains file's data. get_collator(args: ModelDataArguments, model_config: PretrainedConfig, tokenizer: PreTrainedTokenizer) Get appropriate collator function. Collator function will be used to collate a PyTorch Dataset object. Here are all argument detailed: args : Model and data configuration arguments needed to perform pretraining. model_config : Model transformers configuration. tokenizer : Model transformers tokenizer. Returns: Transformers specific data collator. def movie_reviews_to_file ( path_data : str , path_texts_file : str ): r \"\"\"Reading in all data from path and saving it into a single `.txt` file. In the pretraining process of our transformers model we require a text file. This function is designed to work for the Movie Reviews Dataset. You wil have to create your own function to move all examples into a text file if you don't already have a text file with all your unlabeled data. Arguments: path_data (:obj:`str`): Path to the Movie Review Dataset partition. We only have `\\train` and `test` partitions. path_texts_file (:obj:`str`): File path of the generated `.txt` file that contains one example / line. \"\"\" # Check if path exists. if not os . path . isdir ( path_data ): # Raise error if path is invalid. raise ValueError ( 'Invalid `path` variable! Needs to be a directory' ) # Check max sequence length. texts = [] print ( 'Reading ` %s ` partition...' % ( os . path . basename ( path_data ))) # Since the labels are defined by folders with data we loop # through each label. for label in [ 'neg' , 'pos' ]: sentiment_path = os . path . join ( path_data , label ) # Get all files from path. files_names = os . listdir ( sentiment_path ) #[:30] # SAMPLE FOR DEBUGGING. # Go through each file and read its content. for file_name in tqdm ( files_names , desc = label , unit = 'files' ): file_path = os . path . join ( sentiment_path , file_name ) # Read content. content = io . open ( file_path , mode = 'r' , encoding = 'utf-8' ) . read () # Fix any unicode issues. content = fix_text ( content ) # Save content. texts . append ( content ) # Move list to single string. all_texts = ' \\n ' . join ( texts ) # Send all texts string to single file. io . open ( file = path_texts_file , mode = 'w' , encoding = 'utf-8' ) . write ( all_texts ) # Print when done. print ( '`.txt` file saved in ` %s ` \\n ' % path_texts_file ) return class ModelDataArguments ( object ): r \"\"\"Define model and data configuration needed to perform pretraining. Eve though all arguments are optional there still needs to be a certain number of arguments that require values attributed. Arguments: train_data_file (:obj:`str`, `optional`): Path to your .txt file dataset. If you have an example on each line of the file make sure to use line_by_line=True. If the data file contains all text data without any special grouping use line_by_line=False to move a block_size window across the text file. This argument is optional and it will have a `None` value attributed inside the function. eval_data_file (:obj:`str`, `optional`): Path to evaluation .txt file. It has the same format as train_data_file. This argument is optional and it will have a `None` value attributed inside the function. line_by_line (:obj:`bool`, `optional`, defaults to :obj:`False`): If the train_data_file and eval_data_file contains separate examples on each line then line_by_line=True. If there is no separation between examples and train_data_file and eval_data_file contains continuous text then line_by_line=False and a window of block_size will be moved across the files to acquire examples. This argument is optional and it has a default value. mlm (:obj:`bool`, `optional`, defaults to :obj:`False`): Is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta and set to False otherwise. There are functions that will raise ValueError if this argument is not set accordingly. This argument is optional and it has a default value. whole_word_mask (:obj:`bool`, `optional`, defaults to :obj:`False`): Used as flag to determine if we decide to use whole word masking or not. Whole word masking means that whole words will be masked during training instead of tokens which can be chunks of words. This argument is optional and it has a default value. mlm_probability(:obj:`float`, `optional`, defaults to :obj:`0.15`): Used when training masked language models. Needs to have mlm set to True. It represents the probability of masking tokens when training model. This argument is optional and it has a default value. plm_probability (:obj:`float`, `optional`, defaults to :obj:`float(1/6)`): Flag to define the ratio of length of a span of masked tokens to surrounding context length for permutation language modeling. Used for XLNet. This argument is optional and it has a default value. max_span_length (:obj:`int`, `optional`, defaults to :obj:`5`): Flag may also be used to limit the length of a span of masked tokens used for permutation language modeling. Used for XLNet. This argument is optional and it has a default value. block_size (:obj:`int`, `optional`, defaults to :obj:`-1`): It refers to the windows size that is moved across the text file. Set to -1 to use maximum allowed length. This argument is optional and it has a default value. overwrite_cache (:obj:`bool`, `optional`, defaults to :obj:`False`): If there are any cached files, overwrite them. This argument is optional and it has a default value. model_type (:obj:`str`, `optional`): Type of model used: bert, roberta, gpt2. More details: https://huggingface.co/transformers/pretrained_models.html This argument is optional and it will have a `None` value attributed inside the function. model_config_name (:obj:`str`, `optional`): Config of model used: bert, roberta, gpt2. More details: https://huggingface.co/transformers/pretrained_models.html This argument is optional and it will have a `None` value attributed inside the function. tokenizer_name: (:obj:`str`, `optional`) Tokenizer used to process data for training the model. It usually has same name as model_name_or_path: bert-base-cased, roberta-base, gpt2 etc. This argument is optional and it will have a `None` value attributed inside the function. model_name_or_path (:obj:`str`, `optional`): Path to existing transformers model or name of transformer model to be used: bert-base-cased, roberta-base, gpt2 etc. More details: https://huggingface.co/transformers/pretrained_models.html This argument is optional and it will have a `None` value attributed inside the function. model_cache_dir (:obj:`str`, `optional`): Path to cache files to save time when re-running code. This argument is optional and it will have a `None` value attributed inside the function. Raises: ValueError: If `CONFIG_MAPPING` is not loaded in global variables. ValueError: If `model_type` is not present in `CONFIG_MAPPING.keys()`. ValueError: If `model_type`, `model_config_name` and `model_name_or_path` variables are all `None`. At least one of them needs to be set. warnings: If `model_config_name` and `model_name_or_path` are both `None`, the model will be trained from scratch. ValueError: If `tokenizer_name` and `model_name_or_path` are both `None`. We need at least one of them set to load tokenizer. \"\"\" def __init__ ( self , train_data_file = None , eval_data_file = None , line_by_line = False , mlm = False , mlm_probability = 0.15 , whole_word_mask = False , plm_probability = float ( 1 / 6 ), max_span_length = 5 , block_size =- 1 , overwrite_cache = False , model_type = None , model_config_name = None , tokenizer_name = None , model_name_or_path = None , model_cache_dir = None ): # Make sure CONFIG_MAPPING is imported from transformers module. if 'CONFIG_MAPPING' not in globals (): raise ValueError ( 'Could not find `CONFIG_MAPPING` imported! Make sure' \\ ' to import it from `transformers` module!' ) # Make sure model_type is valid. if ( model_type is not None ) and ( model_type not in CONFIG_MAPPING . keys ()): raise ValueError ( 'Invalid `model_type`! Use one of the following: %s ' % ( str ( list ( CONFIG_MAPPING . keys ())))) # Make sure that model_type, model_config_name and model_name_or_path # variables are not all `None`. if not any ([ model_type , model_config_name , model_name_or_path ]): raise ValueError ( 'You can`t have all `model_type`, `model_config_name`,' \\ ' `model_name_or_path` be `None`! You need to have' \\ 'at least one of them set!' ) # Check if a new model will be loaded from scratch. if not any ([ model_config_name , model_name_or_path ]): # Setup warning to show pretty. This is an overkill warnings . formatwarning = lambda message , category , * args , ** kwargs : \\ ' %s : %s \\n ' % ( category . __name__ , message ) # Display warning. warnings . warn ( 'You are planning to train a model from scratch! \ud83d\ude40' ) # Check if a new tokenizer wants to be loaded. # This feature is not supported! if not any ([ tokenizer_name , model_name_or_path ]): # Can't train tokenizer from scratch here! Raise error. raise ValueError ( 'You want to train tokenizer from scratch! ' \\ 'That is not possible yet! You can train your own ' \\ 'tokenizer separately and use path here to load it!' ) # Set all data related arguments. self . train_data_file = train_data_file self . eval_data_file = eval_data_file self . line_by_line = line_by_line self . mlm = mlm self . whole_word_mask = whole_word_mask self . mlm_probability = mlm_probability self . plm_probability = plm_probability self . max_span_length = max_span_length self . block_size = block_size self . overwrite_cache = overwrite_cache # Set all model and tokenizer arguments. self . model_type = model_type self . model_config_name = model_config_name self . tokenizer_name = tokenizer_name self . model_name_or_path = model_name_or_path self . model_cache_dir = model_cache_dir return def get_model_config ( args : ModelDataArguments ): r \"\"\" Get model configuration. Using the ModelDataArguments return the model configuration. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. Returns: :obj:`PretrainedConfig`: Model transformers configuration. Raises: ValueError: If `mlm=True` and `model_type` is NOT in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"]. We need to use a masked language model in order to set `mlm=True`. \"\"\" # Check model configuration. if args . model_config_name is not None : # Use model configure name if defined. model_config = AutoConfig . from_pretrained ( args . model_config_name , cache_dir = args . model_cache_dir ) elif args . model_name_or_path is not None : # Use model name or path if defined. model_config = AutoConfig . from_pretrained ( args . model_name_or_path , cache_dir = args . model_cache_dir ) else : # Use config mapping if building model from scratch. model_config = CONFIG_MAPPING [ args . model_type ]() # Make sure `mlm` flag is set for Masked Language Models (MLM). if ( model_config . model_type in [ \"bert\" , \"roberta\" , \"distilbert\" , \"camembert\" ]) and ( args . mlm is False ): raise ValueError ( 'BERT and RoBERTa-like models do not have LM heads ' \\ 'butmasked LM heads. They must be run setting `mlm=True`' ) # Adjust block size for xlnet. if model_config . model_type == \"xlnet\" : # xlnet used 512 tokens when training. args . block_size = 512 # setup memory length model_config . mem_len = 1024 return model_config def get_tokenizer ( args : ModelDataArguments ): r \"\"\" Get model tokenizer. Using the ModelDataArguments return the model tokenizer and change `block_size` form `args` if needed. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. Returns: :obj:`PreTrainedTokenizer`: Model transformers tokenizer. \"\"\" # Check tokenizer configuration. if args . tokenizer_name : # Use tokenizer name if define. tokenizer = AutoTokenizer . from_pretrained ( args . tokenizer_name , cache_dir = args . model_cache_dir ) elif args . model_name_or_path : # Use tokenizer name of path if defined. tokenizer = AutoTokenizer . from_pretrained ( args . model_name_or_path , cache_dir = args . model_cache_dir ) # Setp data block size. if args . block_size <= 0 : # Set block size to maximum length of tokenizer. # Input block size will be the max possible for the model. # Some max lengths are very large and will cause a args . block_size = tokenizer . model_max_length else : # Never go beyond tokenizer maximum length. args . block_size = min ( args . block_size , tokenizer . model_max_length ) return tokenizer def get_model ( args : ModelDataArguments , model_config ): r \"\"\" Get model. Using the ModelDataArguments return the actual model. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. model_config (:obj:`PretrainedConfig`): Model transformers configuration. Returns: :obj:`torch.nn.Module`: PyTorch model. \"\"\" # Make sure MODEL_FOR_MASKED_LM_MAPPING and MODEL_FOR_CAUSAL_LM_MAPPING are # imported from transformers module. if ( 'MODEL_FOR_MASKED_LM_MAPPING' not in globals ()) and \\ ( 'MODEL_FOR_CAUSAL_LM_MAPPING' not in globals ()): raise ValueError ( 'Could not find `MODEL_FOR_MASKED_LM_MAPPING` and' \\ ' `MODEL_FOR_MASKED_LM_MAPPING` imported! Make sure to' \\ ' import them from `transformers` module!' ) # Check if using pre-trained model or train from scratch. if args . model_name_or_path : # Use pre-trained model. if type ( model_config ) in MODEL_FOR_MASKED_LM_MAPPING . keys (): # Masked language modeling head. return AutoModelForMaskedLM . from_pretrained ( args . model_name_or_path , from_tf = bool ( \".ckpt\" in args . model_name_or_path ), config = model_config , cache_dir = args . model_cache_dir , ) elif type ( model_config ) in MODEL_FOR_CAUSAL_LM_MAPPING . keys (): # Causal language modeling head. return AutoModelForCausalLM . from_pretrained ( args . model_name_or_path , from_tf = bool ( \".ckpt\" in args . model_name_or_path ), config = model_config , cache_dir = args . model_cache_dir ) else : raise ValueError ( 'Invalid `model_name_or_path`! It should be in %s or %s !' % ( str ( MODEL_FOR_MASKED_LM_MAPPING . keys ()), str ( MODEL_FOR_CAUSAL_LM_MAPPING . keys ()))) else : # Use model from configuration - train from scratch. print ( \"Training new model from scratch!\" ) return AutoModelWithLMHead . from_config ( config ) def get_dataset ( args : ModelDataArguments , tokenizer : PreTrainedTokenizer , evaluate : bool = False ): r \"\"\" Process dataset file into PyTorch Dataset. Using the ModelDataArguments return the actual model. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. tokenizer (:obj:`PreTrainedTokenizer`): Model transformers tokenizer. evaluate (:obj:`bool`, `optional`, defaults to :obj:`False`): If set to `True` the test / validation file is being handled. If set to `False` the train file is being handled. Returns: :obj:`Dataset`: PyTorch Dataset that contains file's data. \"\"\" # Get file path for either train or evaluate. file_path = args . eval_data_file if evaluate else args . train_data_file # Check if `line_by_line` flag is set to `True`. if args . line_by_line : # Each example in data file is on each line. return LineByLineTextDataset ( tokenizer = tokenizer , file_path = file_path , block_size = args . block_size ) else : # All data in file is put together without any separation. return TextDataset ( tokenizer = tokenizer , file_path = file_path , block_size = args . block_size , overwrite_cache = args . overwrite_cache ) def get_collator ( args : ModelDataArguments , model_config : PretrainedConfig , tokenizer : PreTrainedTokenizer ): r \"\"\" Get appropriate collator function. Collator function will be used to collate a PyTorch Dataset object. Arguments: args (:obj:`ModelDataArguments`): Model and data configuration arguments needed to perform pretraining. model_config (:obj:`PretrainedConfig`): Model transformers configuration. tokenizer (:obj:`PreTrainedTokenizer`): Model transformers tokenizer. Returns: :obj:`data_collator`: Transformers specific data collator. \"\"\" # Special dataset handle depending on model type. if model_config . model_type == \"xlnet\" : # Configure collator for XLNET. return DataCollatorForPermutationLanguageModeling ( tokenizer = tokenizer , plm_probability = args . plm_probability , max_span_length = args . max_span_length , ) else : # Configure data for rest of model types. if args . mlm and args . whole_word_mask : # Use whole word masking. return DataCollatorForWholeWordMask ( tokenizer = tokenizer , mlm_probability = args . mlm_probability , ) else : # Regular language modeling. return DataCollatorForLanguageModeling ( tokenizer = tokenizer , mlm = args . mlm , mlm_probability = args . mlm_probability , )","title":"Helper Functions"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#parameters-setup","text":"Declare the rest of the parameters used for this notebook: model_data_args contains all arguments needed to setup dataset, model configuration, model tokenizer and the actual model. This is created using the ModelDataArguments class. training_args contain all arguments needed to use the Trainer functionality from Transformers that allows us to train transformers models in PyTorch very easy. You can find the complete documentation here . There are a lot of parameters that can be set to allow multiple functionalities. I only used the following parameters (the comments are inspired from the HuggingFace documentation of TrainingArguments : output_dir : The output directory where the model predictions and checkpoints will be written. I set it up to pretrained_bert_model where the model and will be saved. overwrite_output_dir : Overwrite the content of the output directory. I set it to True in case I run the notebook multiple times I only care about the last run. do_train : Whether to run training or not. I set this parameter to True because I want to train the model on my custom dataset. do_eval : Whether to run evaluation on the evaluation files or not. I set it to True since I have test data file and I want to evaluate how well the model trains. per_device_train_batch_size : Batch size GPU/TPU core/CPU training. I set it to 2 for this example. I recommend setting it up as high as your GPU memory allows you. per_device_eval_batch_size : Batch size GPU/TPU core/CPU for evaluation.I set this value to 100 since it's not dealing with gradients. evaluation_strategy : Evaluation strategy to adopt during training: no : No evaluation during training; steps : Evaluate every eval_steps; epoch`: Evaluate every end of epoch. I set it to 'steps' since I want to evaluate model more often. logging_steps : How often to show logs. I will se this to plot history loss and calculate perplexity. I set this to 20 just as an example. If your evaluate data is large you might not want to run it that often because it will significantly slow down training time. eval_steps : Number of update steps between two evaluations if evaluation_strategy=\"steps\". Will default to the same value as logging_steps if not set. Since I want to evaluate model ever logging_steps I will set this to None since it will inherit same value as logging_steps . prediction_loss_only : Set prediction loss to True in order to return loss for perplexity calculation. Since I want to calculate perplexity I set this to True since I want to monitor loss and perplexity (which is exp(loss)). learning_rate : The initial learning rate for Adam. Defaults is set to 5e-5 . weight_decay : The weight decay to apply (if not zero)Defaults is set to 0 . adam_epsilon : Epsilon for the Adam optimizer. Defaults to 1e-8 . max_grad_norm : Maximum gradient norm (for gradient clipping). Defaults to 0 . num_train_epochs : Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training). I set it to 2 at most. Since the custom dataset will be a lot smaller than the original dataset the model was trained on we don't want to overfit. save_steps : Number of updates steps before two checkpoint saves. Defaults to 500 . # Define arguments for data, tokenizer and model arguments. # See comments in `ModelDataArguments` class. model_data_args = ModelDataArguments ( train_data_file = '/content/train.txt' , eval_data_file = '/content/test.txt' , line_by_line = True , mlm = True , whole_word_mask = True , mlm_probability = 0.15 , plm_probability = float ( 1 / 6 ), max_span_length = 5 , block_size = 50 , overwrite_cache = False , model_type = 'bert' , model_config_name = 'bert-base-cased' , tokenizer_name = 'bert-base-cased' , model_name_or_path = 'bert-base-cased' , model_cache_dir = None , ) # Define arguments for training # Note: I only used the arguments I care about. `TrainingArguments` contains # a lot more arguments. For more details check the awesome documentation: # https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments training_args = TrainingArguments ( # The output directory where the model predictions # and checkpoints will be written. output_dir = 'pretrain_bert' , # Overwrite the content of the output directory. overwrite_output_dir = True , # Whether to run training or not. do_train = True , # Whether to run evaluation on the dev or not. do_eval = True , # Batch size GPU/TPU core/CPU training. per_device_train_batch_size = 10 , # Batch size GPU/TPU core/CPU for evaluation. per_device_eval_batch_size = 100 , # evaluation strategy to adopt during training # `no`: No evaluation during training. # `steps`: Evaluate every `eval_steps`. # `epoch`: Evaluate every end of epoch. evaluation_strategy = 'steps' , # How often to show logs. I will se this to # plot history loss and calculate perplexity. logging_steps = 700 , # Number of update steps between two # evaluations if evaluation_strategy=\"steps\". # Will default to the same value as l # logging_steps if not set. eval_steps = None , # Set prediction loss to `True` in order to # return loss for perplexity calculation. prediction_loss_only = True , # The initial learning rate for Adam. # Defaults to 5e-5. learning_rate = 5e-5 , # The weight decay to apply (if not zero). weight_decay = 0 , # Epsilon for the Adam optimizer. # Defaults to 1e-8 adam_epsilon = 1e-8 , # Maximum gradient norm (for gradient # clipping). Defaults to 0. max_grad_norm = 1.0 , # Total number of training epochs to perform # (if not an integer, will perform the # decimal part percents of # the last epoch before stopping training). num_train_epochs = 2 , # Number of updates steps before two checkpoint saves. # Defaults to 500 save_steps = - 1 , )","title":"Parameters Setup"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#load-configuration-tokenizer-and-model","text":"Loading the three essential parts of the pretrained transformers: configuration, tokenizer and model. Since I use the AutoClass functionality from Hugging Face I only need to worry about the model's name as input and the rest is handled by the transformers library. I will be calling each three functions created in the Helper Functions tab that help return config of the model, tokenizer of the model and the actual PyTorch model . After model is loaded is always good practice to resize the model depending on the tokenizer size. This means that the tokenizer's vocabulary will be aligned with the models embedding layer. This is very useful when we have a different tokenizer that the pretrained one or we train a transformer model from scratch. # Load model configuration. print ( 'Loading model configuration...' ) config = get_model_config ( model_data_args ) # Load model tokenizer. print ( 'Loading model`s tokenizer...' ) tokenizer = get_tokenizer ( model_data_args ) # Loading model. print ( 'Loading actual model...' ) model = get_model ( model_data_args , config ) # Resize model to fit all tokens in tokenizer. model . resize_token_embeddings ( len ( tokenizer )) Loading model configuration... Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|433/433 [00:01<00:00, 285B/s] Loading model`s tokenizer... Downloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|433/433 [00:01<00:00, 285B/s] Loading actual model... Downloading: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|436M/436M [00:36<00:00, 11.9MB/s] Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias'] - This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Embedding(28996, 768, padding_idx=0)","title":"Load Configuration, Tokenizer and Model"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#dataset-and-collator","text":"This is where I create the PyTorch Dataset and data collator objects that will be used to feed data into our model. This is where I use the MovieReviewsDataset text files created with the movie_reviews_to_file function. Since data is partitioned for both train and test I will create two text files: one used for train and one used for evaluation. I strongly recommend to use a validation text file in order to determine how much training is needed in order to avoid overfitting. After you figure out what parameters yield the best results, the validation file can be incorporated in train and run a final train with the whole dataset. The data collator is used to format the PyTorch Dataset outputs to match the output of our specific transformers model: i.e. for Bert it will created the masked tokens needed to train. # Create texts file from train data. movie_reviews_to_file ( path_data = '/content/aclImdb/train' , path_texts_file = '/content/train.txt' ) # Create texts file from test data. movie_reviews_to_file ( path_data = '/content/aclImdb/test' , path_texts_file = '/content/test.txt' ) # Setup train dataset if `do_train` is set. print ( 'Creating train dataset...' ) train_dataset = get_dataset ( model_data_args , tokenizer = tokenizer , evaluate = False ) if training_args . do_train else None # Setup evaluation dataset if `do_eval` is set. print ( 'Creating evaluate dataset...' ) eval_dataset = get_dataset ( model_data_args , tokenizer = tokenizer , evaluate = True ) if training_args . do_eval else None # Get data collator to modify data format depending on type of model used. data_collator = get_collator ( model_data_args , config , tokenizer ) # Check how many logging prints you'll have. This is to avoid overflowing the # notebook with a lot of prints. Display warning to user if the logging steps # that will be displayed is larger than 100. if ( len ( train_dataset ) // training_args . per_device_train_batch_size \\ // training_args . logging_steps * training_args . num_train_epochs ) > 100 : # Display warning. warnings . warn ( 'Your `logging_steps` value will will do a lot of printing!' \\ ' Consider increasing `logging_steps` to avoid overflowing' \\ ' the notebook with a lot of prints!' ) Reading `train` partition... neg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] pos: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] `.txt` file saved in `/content/train.txt` Reading `test` partition... neg: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] pos: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|12500/12500 [00:55<00:00, 224.11files/s] `.txt` file saved in `/content/train.txt` Creating train dataset... Creating evaluate dataset...","title":"Dataset and Collator"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#train","text":"Hugging Face was very nice to us for creating the Trainer class. This helps make PyTorch model training of transformers very easy! We just need to make sure we loaded the proper parameters and everything else is taking care of! At the end of the training the tokenizer is saved along with the model so you can easily re-use it later or even load in on Hugging Face Models. I configured the arguments to display both train and validation loss at every logging_steps . It gives us a sense of how well the model is trained. # Initialize Trainer. print ( 'Loading `trainer`...' ) trainer = Trainer ( model = model , args = training_args , data_collator = data_collator , train_dataset = train_dataset , eval_dataset = eval_dataset , ) # Check model path to save. if training_args . do_train : print ( 'Start training...' ) # Setup model path if the model to train loaded from a local path. model_path = ( model_data_args . model_name_or_path if model_data_args . model_name_or_path is not None and os . path . isdir ( model_data_args . model_name_or_path ) else None ) # Run training. trainer . train ( model_path = model_path ) # Save model. trainer . save_model () # For convenience, we also re-save the tokenizer to the same directory, # so that you can share your model easily on huggingface.co/models =). if trainer . is_world_process_zero (): tokenizer . save_pretrained ( training_args . output_dir ) Loading `trainer`... Start training... |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|[5000/5000 09:43, Epoch 2/2] Step Training Loss Validation Loss 700 2.804672 2.600590 1400 2.666996 2.548267 2100 2.625075 2.502431 2800 2.545872 2.485056 3500 2.470102 2.444808 4200 2.464950 2.420487 4900 2.436973 2.410310","title":"Train"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#plot-train","text":"The Trainer class is so useful that it will record the log history for us. I use this to access the train and validation losses recorded at each logging_steps during training. Since we are training / fine-tuning / extended training or pretraining (depending what terminology you use) a language model, we want to compute the perplexity. This is what Wikipedia says about perplexity: In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample. We can look at the perplexity plot in the same way we look at the loss plot: the lower the better and if the validation perplexity starts to increase we are starting to overfit the model. Note: It looks from the plots that the train loss is higher than validation loss. That means that our validation data is too easy for the model and we should use a different validation dataset. Since the purpose of this notebook is to show how to train transformers models and provide tools to evaluate such process I will leave the results as is . # Keep track of train and evaluate loss. loss_history = { 'train_loss' :[], 'eval_loss' :[]} # Keep track of train and evaluate perplexity. # This is a metric useful to track for language models. perplexity_history = { 'train_perplexity' :[], 'eval_perplexity' :[]} # Loop through each log history. for log_history in trainer . state . log_history : if 'loss' in log_history . keys (): # Deal with trianing loss. loss_history [ 'train_loss' ] . append ( log_history [ 'loss' ]) perplexity_history [ 'train_perplexity' ] . append ( math . exp ( log_history [ 'loss' ])) elif 'eval_loss' in log_history . keys (): # Deal with eval loss. loss_history [ 'eval_loss' ] . append ( log_history [ 'eval_loss' ]) perplexity_history [ 'eval_perplexity' ] . append ( math . exp ( log_history [ 'eval_loss' ])) # Plot Losses. plot_dict ( loss_history , start_step = training_args . logging_steps , step_size = training_args . logging_steps , use_title = 'Loss' , use_xlabel = 'Train Steps' , use_ylabel = 'Values' , magnify = 2 ) print () # Plot Perplexities. plot_dict ( perplexity_history , start_step = training_args . logging_steps , step_size = training_args . logging_steps , use_title = 'Perplexity' , use_xlabel = 'Train Steps' , use_ylabel = 'Values' , magnify = 2 )","title":"Plot Train"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#evaluate","text":"For the final evaluation we can have a separate test set that we use to do our final perplexity evaluation. For simplicity I used the same validation text file for the final evaluation. That is the reason I get the same results as the last validation perplexity plot value. # check if `do_eval` flag is set. if training_args . do_eval : # capture output if trainer evaluate. eval_output = trainer . evaluate () # compute perplexity from model loss. perplexity = math . exp ( eval_output [ \"eval_loss\" ]) print ( ' \\n Evaluate Perplexity: {:10,.2f} ' . format ( perplexity )) else : print ( 'No evaluation needed. No evaluation data provided, `do_eval=False`!' ) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|[250/250 00:25] Evaluate Perplexity: 11.01","title":"Evaluate"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#final-note","text":"If you made it this far Congrats! \ud83c\udf8a and Thank you! \ud83d\ude4f for your interest in my tutorial! I've been using this code for a while now and I feel it got to a point where is nicely documented and easy to follow. Of course is easy for me to follow because I built it. That is why any feedback is welcome and it helps me improve my future tutorials! If you see something wrong please let me know by opening an issue on my ml_things GitHub repository ! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.","title":"Final Note"},{"location":"tutorial_notebooks/pretrain_transformers_pytorch/#contact-","text":"\ud83e\udd8a GitHub: gmihaila \ud83c\udf10 Website: gmihaila.github.io \ud83d\udc54 LinkedIn: mihailageorge \ud83d\udcec Email: georgemihaila@my.unt.edu.com","title":"Contact \ud83c\udfa3"},{"location":"tutorial_notebooks/pytorchtext/","text":"PyTorchText Example on how to batch text sequences with BucketIterator This notebook is an example of using pytorchtext powerful BucketIterator function which allows grouping examples of similar lengths to provide the most optimal batching method. The batching problem provides a lot of gain for recurrent models (RNN, GRU, LSTM) and transformers models (bert, roBerta, gpt2, xlnet, etc.). Basically any model that can deal with variable input batches. I will not train any model in this notebook. There are other notebooks where I use this batching method to train models. The purpose is to use an example datasets and batch it using torchtext with BucketIterator and show how it groups text sequences of similar length in batches. How to use this notebook? I am using the Large Movie Review Dataset v1.0 dataset which contains positive sentiments and negative sentiments of movie review. This dataset requires using Supervised Training with Binary Classification . The code is made with reusability in mind. It can be easily adapted for other text datasets and other NLP tasks. Comments should provide enough guidance to easily adapt this notebook to your needs. This code is designed mostly for classification tasks in mind, but it can be adapted for dialogue generation tasks . Notes: * This notebooks is a code adaptation of a few sources I foudn online: torchtext_translation_tutorial , pytorch/text - GitHub , torchtext documentation and A Comprehensive Introduction to Torchtext . Downloads Download the IMDB Movie Reviews sentiment dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Output: Installs I will use ftfy to fix any bad Unicode there might be in the text data files. Since we don't know for sure of anything is wrong with the text files its safer to run all text through ftfy. Code Cell: # install ftfy to fix any text encoding issues !pip install -q ftfy Output: | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 1 .9MB/s Building wheel for ftfy ( setup.py ) ... done Imports Import python all needed python packages. Code Cell: import io import os from tqdm.notebook import tqdm from ftfy import fix_text from torchtext import data Output: Helper Functions I've created the file_tsv function in order to concatenate all text files into a single .tsv file. Since I'll use the TabularDataset from pytorch.data I need to pass tabular format file. For text data I find the Tab Separated Values format easier to deal with - don't need to import pandas for this. Code Cell: def file_tsv ( partition_path , save_path = './' ): \"\"\"Parse each file in partition and keep track of sentiments. Create a list of paris [tag, text] \"\"\" # list of all examples in format [tag, text] examples = [] print ( partition_path ) # for each sentiment for sentiment in tqdm ([ 'pos' , 'neg' ]): # find path for sentiment sentiment_path = os . path . join ( partition_path , sentiment ) # get all files from path sentiment files_names = os . listdir ( sentiment_path ) # for each file in path sentiment for file_name in files_names : # get file content file_content = io . open ( os . path . join ( sentiment_path , file_name ), mode = 'r' , encoding = 'utf-8' ) . read () # fix any format errors file_content = fix_text ( file_content ) # append sentiment and file content examples . append ([ sentiment , file_content ]) # create a TSV file with same format `sentiment text` examples = [ \" %s \\t %s \" % ( example [ 0 ], example [ 1 ]) for example in examples ] # create file name tsv_filename = os . path . basename ( partition_path ) + '_pos_neg_ %d .tsv' % len ( examples ) # write to file io . open ( os . path . join ( save_path , tsv_filename ), mode = 'w' , encoding = 'utf-8' ) . write ( ' \\n ' . join ( examples )) return tsv_filename Output: Convert data to .tsv I will call the file_tsv function for each of the two partitions train and test . The function will return the path where the .tsv file is saved so we can use it later in pytorchtext. Code Cell: # path where to save tsv file data_path = '/content' # convert train files to tsv file train_filename = file_tsv ( partition_path = '/content/aclImdb/train' , save_path = data_path ) # convert test files to tsv file test_filename = file_tsv ( partition_path = '/content/aclImdb/test' , save_path = data_path ) Output: /content/aclImdb/train 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :39< 00 :00, 19 .80s/it ] /content/aclImdb/test 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :19< 00 :00, 9 .97s/it ] PyTorchText Setup Here I will setup the dataset to be processed by PyTrochText. I will try to add as many useful comments as possible to make the code very easy to adapt to other projects. Setup data fields Here I setup data fields for PyTorchText. We have to tell the library how to handle each column of the .tsv file. For this we need to create data.Field objects for each column. The tokenizer for text column is a simple split on white-space tokenizer. Depending on the project, it can be changed to any tokenizer. It needs to take as input text and output a list. The label tokenizer is not actually a tokenizer. It just encodes the pos into 1 and neg into 0 . Since we have two .tsv files it's great that we can use the .split function from TabularDataset to handle two files at the same time one for train and the other one for test. Find more details about torchtext.data functionality here . Code Cell: # text tokenizer function - simple white-space split text_tokenizer = lambda x : x . split () # label tokenizer - encode labels to int 0:negative and 1:positive label_tokenizer = lambda x : 0 if x == 'neg' else 1 # data fiels for text column - invoke tokenizer TEXT = data . Field ( sequential = True , tokenize = text_tokenizer , lower = False ) # data field for labels - invoke tokenize label encoder LABEL = data . Field ( sequential = True , tokenize = label_tokenizer , use_vocab = False ) # create data fields at tuples of description variable and data fiels datafields = [( \"label\" , LABEL ), ( \"text\" , TEXT )] # since we have have tab separated data we use TabularDataset train_split , test_split = data . TabularDataset . splits ( path = data_path , # path to data train = train_filename , validation = test_filename , format = 'tsv' , skip_header = False , # important fields = datafields ) Output: Bucket Iterator Here is where the magic happens! We pass in the train_split and test_split TabularDatasets splits into BucketIterator to create the actual batches. It's very nice that pytorchtext can handle splits! We need to tell the BucketIterator the batch size for both our splits. The sort_key parameter is very important. It is used to order text sequences in batches. Since we want to batch sequence of text with similar length, we will use a simple function that returns the length of our text ( len(x.text) ). It is important to keep sort=False and sort_with_batch=True to sort the batches only and not the whole dataset. Find more details in the pytorchtext BucketIterator documentation here - look at the BPTTIterator because it has same parameters except the bptt_len argument. Code Cell: # batch size used for train train_batch_size = 10 # batch size used for test test_batch_size = 20 # bucket similar length text sequences together to create batches train_iterator , val_iterator = data . BucketIterator . splits ( ( train_split , test_split ), # datasets for iterator to draw data from batch_sizes = ( train_batch_size , test_batch_size ), device = 'cpu' , # if we want to load batches on specific device sort_key = lambda x : len ( x . text ), # what function should use to group batches repeat = True , # repeat the iterator for multiple epochs(DON'T TRUST) sort = False , # avoid sorting all data using sort_key shuffle = True , # if data needs to be shuffled each time batches are created sort_within_batch = True # only sort each batch using sort_key (better to use) ) Output: Sample batch Now let's see how a batch looks like! The print format is: label sequence_length tokenized_text . We see the labels as 0 and 1 values along with the length of tokens for that text sequence and along with the list of tokens from that sequence. It looks like the lengths of sequences for this batches are very close together! This actually works! Note: I would call .create_batches() after each epoch. The repeat=True in BucketIterator should allow more epochs to run but I don't trust it! Code Cell: # create batches - needs to be called after each epoch train_iterator . create_batches () # loop through each batch for batch in train_iterator . batches : print ( len ( batch )) # print each example for example in batch : print ( example . label , len ( example . text ), example . text ) print ( ' \\n ' ) break Output: 10 0 212 [ 'Sometimes' , 'a' , 'premise' ... 1 212 [ \"I've\" , 'loved' , 'all' , 'of... 0 213 [' Apparently ', ' most ', ' view... 0 214 [ 'This' , 'movie' , 'had' , 'th... 1 215 [' This ', ' is ', ' the ', ' Neil '... 0 215 [' Hanna-Barbera ', ' sucks ', ' ... ... Train Loop Example Now let's print a list of lengths of each sequence in a batch, to see if the BucketIterator works as promised. We can see how nicely examples of similar length are grouped together by length in a single batch. After each epoch, new batches of similar length are generated when shuffle=True . It looks like we have setup everything we need to train a model! Code Cell: # example of number of epochs epochs = 1 # loop through each epoch for epoch in range ( epochs ): # create batches - needs to be called after each epoch train_iterator . create_batches () # get each batch for batch in train_iterator . batches : # put all example.text of batch in single array batch_text = [ example . text for example in batch ] # put all example.label of batch in single array batch_label = [ example . label for example in batch ] # get maximum sequence length in batch - used for padding max_sequence_len = max ([ len ( text ) for text in batch_text ]) # CODE HERE TO FEED BATCHES TO MODEL print ([ len ( text ) for text in batch_text ]) Output: [ 200 , 200 , 200 , 201 , 201 , 203 , 203 , 203 , 203 , 203 ] [ 127 , 127 , 127 , 127 , 127 , 127 , 128 , 128 , 128 , 128 ] [ 213 , 214 , 214 , 214 , 216 , 216 , 216 , 216 , 217 , 217 ] [ 238 , 238 , 239 , 240 , 240 , 241 , 241 , 241 , 241 , 241 ] [ 197 , 197 , 198 , 198 , 198 , 198 , 199 , 199 , 200 , 200 ] [ 354 , 355 , 355 , 357 , 359 , 360 , 360 , 360 , 360 , 361 ] [ 251 , 252 , 256 , 257 , 258 , 259 , 260 , 260 , 260 , 260 ] [ 148 , 148 , 148 , 148 , 148 , 148 , 149 , 149 , 149 , 149 ] [ 122 , 122 , 122 , 122 , 123 , 123 , 123 , 123 , 123 , 123 ] [ 223 , 223 , 226 , 227 , 227 , 228 , 228 , 229 , 229 , 229 ] [ 89 , 89 , 90 , 91 , 92 , 93 , 94 , 94 , 94 , 94 ] [ 131 , 131 , 132 , 132 , 132 , 132 , 132 , 133 , 133 , 133 ] ...","title":"PytorchText"},{"location":"tutorial_notebooks/pytorchtext/#pytorchtext","text":"","title":"PyTorchText"},{"location":"tutorial_notebooks/pytorchtext/#example-on-how-to-batch-text-sequences-with-bucketiterator","text":"This notebook is an example of using pytorchtext powerful BucketIterator function which allows grouping examples of similar lengths to provide the most optimal batching method. The batching problem provides a lot of gain for recurrent models (RNN, GRU, LSTM) and transformers models (bert, roBerta, gpt2, xlnet, etc.). Basically any model that can deal with variable input batches. I will not train any model in this notebook. There are other notebooks where I use this batching method to train models. The purpose is to use an example datasets and batch it using torchtext with BucketIterator and show how it groups text sequences of similar length in batches.","title":"Example on how to batch text sequences with BucketIterator"},{"location":"tutorial_notebooks/pytorchtext/#how-to-use-this-notebook","text":"I am using the Large Movie Review Dataset v1.0 dataset which contains positive sentiments and negative sentiments of movie review. This dataset requires using Supervised Training with Binary Classification . The code is made with reusability in mind. It can be easily adapted for other text datasets and other NLP tasks. Comments should provide enough guidance to easily adapt this notebook to your needs. This code is designed mostly for classification tasks in mind, but it can be adapted for dialogue generation tasks . Notes: * This notebooks is a code adaptation of a few sources I foudn online: torchtext_translation_tutorial , pytorch/text - GitHub , torchtext documentation and A Comprehensive Introduction to Torchtext .","title":"How to use this notebook?"},{"location":"tutorial_notebooks/pytorchtext/#downloads","text":"Download the IMDB Movie Reviews sentiment dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Output:","title":"Downloads"},{"location":"tutorial_notebooks/pytorchtext/#installs","text":"I will use ftfy to fix any bad Unicode there might be in the text data files. Since we don't know for sure of anything is wrong with the text files its safer to run all text through ftfy. Code Cell: # install ftfy to fix any text encoding issues !pip install -q ftfy Output: | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 1 .9MB/s Building wheel for ftfy ( setup.py ) ... done","title":"Installs"},{"location":"tutorial_notebooks/pytorchtext/#imports","text":"Import python all needed python packages. Code Cell: import io import os from tqdm.notebook import tqdm from ftfy import fix_text from torchtext import data Output:","title":"Imports"},{"location":"tutorial_notebooks/pytorchtext/#helper-functions","text":"I've created the file_tsv function in order to concatenate all text files into a single .tsv file. Since I'll use the TabularDataset from pytorch.data I need to pass tabular format file. For text data I find the Tab Separated Values format easier to deal with - don't need to import pandas for this. Code Cell: def file_tsv ( partition_path , save_path = './' ): \"\"\"Parse each file in partition and keep track of sentiments. Create a list of paris [tag, text] \"\"\" # list of all examples in format [tag, text] examples = [] print ( partition_path ) # for each sentiment for sentiment in tqdm ([ 'pos' , 'neg' ]): # find path for sentiment sentiment_path = os . path . join ( partition_path , sentiment ) # get all files from path sentiment files_names = os . listdir ( sentiment_path ) # for each file in path sentiment for file_name in files_names : # get file content file_content = io . open ( os . path . join ( sentiment_path , file_name ), mode = 'r' , encoding = 'utf-8' ) . read () # fix any format errors file_content = fix_text ( file_content ) # append sentiment and file content examples . append ([ sentiment , file_content ]) # create a TSV file with same format `sentiment text` examples = [ \" %s \\t %s \" % ( example [ 0 ], example [ 1 ]) for example in examples ] # create file name tsv_filename = os . path . basename ( partition_path ) + '_pos_neg_ %d .tsv' % len ( examples ) # write to file io . open ( os . path . join ( save_path , tsv_filename ), mode = 'w' , encoding = 'utf-8' ) . write ( ' \\n ' . join ( examples )) return tsv_filename Output:","title":"Helper Functions"},{"location":"tutorial_notebooks/pytorchtext/#convert-data-to-tsv","text":"I will call the file_tsv function for each of the two partitions train and test . The function will return the path where the .tsv file is saved so we can use it later in pytorchtext. Code Cell: # path where to save tsv file data_path = '/content' # convert train files to tsv file train_filename = file_tsv ( partition_path = '/content/aclImdb/train' , save_path = data_path ) # convert test files to tsv file test_filename = file_tsv ( partition_path = '/content/aclImdb/test' , save_path = data_path ) Output: /content/aclImdb/train 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :39< 00 :00, 19 .80s/it ] /content/aclImdb/test 100 % | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 /2 [ 00 :19< 00 :00, 9 .97s/it ]","title":"Convert data to .tsv"},{"location":"tutorial_notebooks/pytorchtext/#pytorchtext-setup","text":"Here I will setup the dataset to be processed by PyTrochText. I will try to add as many useful comments as possible to make the code very easy to adapt to other projects.","title":"PyTorchText Setup"},{"location":"tutorial_notebooks/pytorchtext/#setup-data-fields","text":"Here I setup data fields for PyTorchText. We have to tell the library how to handle each column of the .tsv file. For this we need to create data.Field objects for each column. The tokenizer for text column is a simple split on white-space tokenizer. Depending on the project, it can be changed to any tokenizer. It needs to take as input text and output a list. The label tokenizer is not actually a tokenizer. It just encodes the pos into 1 and neg into 0 . Since we have two .tsv files it's great that we can use the .split function from TabularDataset to handle two files at the same time one for train and the other one for test. Find more details about torchtext.data functionality here . Code Cell: # text tokenizer function - simple white-space split text_tokenizer = lambda x : x . split () # label tokenizer - encode labels to int 0:negative and 1:positive label_tokenizer = lambda x : 0 if x == 'neg' else 1 # data fiels for text column - invoke tokenizer TEXT = data . Field ( sequential = True , tokenize = text_tokenizer , lower = False ) # data field for labels - invoke tokenize label encoder LABEL = data . Field ( sequential = True , tokenize = label_tokenizer , use_vocab = False ) # create data fields at tuples of description variable and data fiels datafields = [( \"label\" , LABEL ), ( \"text\" , TEXT )] # since we have have tab separated data we use TabularDataset train_split , test_split = data . TabularDataset . splits ( path = data_path , # path to data train = train_filename , validation = test_filename , format = 'tsv' , skip_header = False , # important fields = datafields ) Output:","title":"Setup data fields"},{"location":"tutorial_notebooks/pytorchtext/#bucket-iterator","text":"Here is where the magic happens! We pass in the train_split and test_split TabularDatasets splits into BucketIterator to create the actual batches. It's very nice that pytorchtext can handle splits! We need to tell the BucketIterator the batch size for both our splits. The sort_key parameter is very important. It is used to order text sequences in batches. Since we want to batch sequence of text with similar length, we will use a simple function that returns the length of our text ( len(x.text) ). It is important to keep sort=False and sort_with_batch=True to sort the batches only and not the whole dataset. Find more details in the pytorchtext BucketIterator documentation here - look at the BPTTIterator because it has same parameters except the bptt_len argument. Code Cell: # batch size used for train train_batch_size = 10 # batch size used for test test_batch_size = 20 # bucket similar length text sequences together to create batches train_iterator , val_iterator = data . BucketIterator . splits ( ( train_split , test_split ), # datasets for iterator to draw data from batch_sizes = ( train_batch_size , test_batch_size ), device = 'cpu' , # if we want to load batches on specific device sort_key = lambda x : len ( x . text ), # what function should use to group batches repeat = True , # repeat the iterator for multiple epochs(DON'T TRUST) sort = False , # avoid sorting all data using sort_key shuffle = True , # if data needs to be shuffled each time batches are created sort_within_batch = True # only sort each batch using sort_key (better to use) ) Output:","title":"Bucket Iterator"},{"location":"tutorial_notebooks/pytorchtext/#sample-batch","text":"Now let's see how a batch looks like! The print format is: label sequence_length tokenized_text . We see the labels as 0 and 1 values along with the length of tokens for that text sequence and along with the list of tokens from that sequence. It looks like the lengths of sequences for this batches are very close together! This actually works! Note: I would call .create_batches() after each epoch. The repeat=True in BucketIterator should allow more epochs to run but I don't trust it! Code Cell: # create batches - needs to be called after each epoch train_iterator . create_batches () # loop through each batch for batch in train_iterator . batches : print ( len ( batch )) # print each example for example in batch : print ( example . label , len ( example . text ), example . text ) print ( ' \\n ' ) break Output: 10 0 212 [ 'Sometimes' , 'a' , 'premise' ... 1 212 [ \"I've\" , 'loved' , 'all' , 'of... 0 213 [' Apparently ', ' most ', ' view... 0 214 [ 'This' , 'movie' , 'had' , 'th... 1 215 [' This ', ' is ', ' the ', ' Neil '... 0 215 [' Hanna-Barbera ', ' sucks ', ' ... ...","title":"Sample batch"},{"location":"tutorial_notebooks/pytorchtext/#train-loop-example","text":"Now let's print a list of lengths of each sequence in a batch, to see if the BucketIterator works as promised. We can see how nicely examples of similar length are grouped together by length in a single batch. After each epoch, new batches of similar length are generated when shuffle=True . It looks like we have setup everything we need to train a model! Code Cell: # example of number of epochs epochs = 1 # loop through each epoch for epoch in range ( epochs ): # create batches - needs to be called after each epoch train_iterator . create_batches () # get each batch for batch in train_iterator . batches : # put all example.text of batch in single array batch_text = [ example . text for example in batch ] # put all example.label of batch in single array batch_label = [ example . label for example in batch ] # get maximum sequence length in batch - used for padding max_sequence_len = max ([ len ( text ) for text in batch_text ]) # CODE HERE TO FEED BATCHES TO MODEL print ([ len ( text ) for text in batch_text ]) Output: [ 200 , 200 , 200 , 201 , 201 , 203 , 203 , 203 , 203 , 203 ] [ 127 , 127 , 127 , 127 , 127 , 127 , 128 , 128 , 128 , 128 ] [ 213 , 214 , 214 , 214 , 216 , 216 , 216 , 216 , 217 , 217 ] [ 238 , 238 , 239 , 240 , 240 , 241 , 241 , 241 , 241 , 241 ] [ 197 , 197 , 198 , 198 , 198 , 198 , 199 , 199 , 200 , 200 ] [ 354 , 355 , 355 , 357 , 359 , 360 , 360 , 360 , 360 , 361 ] [ 251 , 252 , 256 , 257 , 258 , 259 , 260 , 260 , 260 , 260 ] [ 148 , 148 , 148 , 148 , 148 , 148 , 149 , 149 , 149 , 149 ] [ 122 , 122 , 122 , 122 , 123 , 123 , 123 , 123 , 123 , 123 ] [ 223 , 223 , 226 , 227 , 227 , 228 , 228 , 229 , 229 , 229 ] [ 89 , 89 , 90 , 91 , 92 , 93 , 94 , 94 , 94 , 94 ] [ 131 , 131 , 132 , 132 , 132 , 132 , 132 , 133 , 133 , 133 ] ...","title":"Train Loop Example"},{"location":"tutorial_notebooks/tutorial_template_page/","text":"Title Work in progress Info Intro to this tutorial What should I know for this notebook? Any requirements. How to use this notebook? Instructions. What ? Tutorial specific answer. Dataset I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with. Coding Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations. Downloads Download the Large Movie Review Dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz Installs transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. Give it a try! Code Cell: # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Output: Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .9MB 6 .7MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 890kB 48 .9MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 .1MB 49 .0MB/s Building wheel for transformers ( PEP 517 ) ... done Building wheel for sacremoses ( setup.py ) ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 5 .2MB/s Building wheel for ml-things ( setup.py ) ... done Building wheel for ftfy ( setup.py ) ... done Imports Import all needed libraries for this notebook. Declare parameters used for this notebook: * * Code Cell: Helper Functions Class() / function() Class / function description. Code Cell: Load Model and Tokenizer Loading the three essential parts of the pretrained transformers: configuration , tokenizer and model . I also need to load the model on the device I'm planning to use (GPU / CPU). Code Cell: Output: Dataset and DataLoader Details. Code Cell: Output: Train Code Cell: Output: Use ColabImage plots straight in here Evaluate Evaluation! Code Cell: Output: Use ColabImage plots straight in here Final Note If you made it this far Congrats and Thank you for your interest in my tutorial! Other details. If you see something wrong please let me know by opening an issue on my ml_things GitHub repository! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can. Contact GitHub: gmihaila Website: gmihaila.github.io LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila Thank you! Find out more About Me .","title":"**:gear: Title**"},{"location":"tutorial_notebooks/tutorial_template_page/#title","text":"","title":"Title"},{"location":"tutorial_notebooks/tutorial_template_page/#work-in-progress","text":"","title":"Work in progress"},{"location":"tutorial_notebooks/tutorial_template_page/#info","text":"Intro to this tutorial","title":"Info"},{"location":"tutorial_notebooks/tutorial_template_page/#what-should-i-know-for-this-notebook","text":"Any requirements.","title":"What should I know for this notebook?"},{"location":"tutorial_notebooks/tutorial_template_page/#how-to-use-this-notebook","text":"Instructions.","title":"How to use this notebook?"},{"location":"tutorial_notebooks/tutorial_template_page/#what-","text":"Tutorial specific answer.","title":"What ?"},{"location":"tutorial_notebooks/tutorial_template_page/#dataset","text":"I will use the well known movies reviews positive - negative labeled Large Movie Review Dataset . The description provided on the Stanford website: This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details. Why this dataset? I believe is an easy to understand and use dataset for classification. I think sentiment data is always fun to work with.","title":"Dataset"},{"location":"tutorial_notebooks/tutorial_template_page/#coding","text":"Now let's do some coding! We will go through each coding cell in the notebook and describe what it does, what's the code, and when is relevant - show the output I made this format to be easy to follow if you decide to run each code cell in your own python notebook. When I learn from a tutorial I always try to replicate the results. I believe it's easy to follow along if you have the code next to the explanations.","title":"Coding"},{"location":"tutorial_notebooks/tutorial_template_page/#downloads","text":"Download the Large Movie Review Dataset and unzip it locally. Code Cell: # download the dataset !wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # unzip it !tar -zxf /content/aclImdb_v1.tar.gz","title":"Downloads"},{"location":"tutorial_notebooks/tutorial_template_page/#installs","text":"transformers library needs to be installed to use all the awesome code from Hugging Face. To get the latest version I will install it straight from GitHub. ml_things library used for various machine learning related tasks. I created this library to reduce the amount of code I need to write for each machine learning project. Give it a try! Code Cell: # Install transformers library. !pip install -q git+https://github.com/huggingface/transformers.git # Install helper functions. !pip install -q git+https://github.com/gmihaila/ml_things.git Output: Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 2 .9MB 6 .7MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 890kB 48 .9MB/s | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 1 .1MB 49 .0MB/s Building wheel for transformers ( PEP 517 ) ... done Building wheel for sacremoses ( setup.py ) ... done | \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 71kB 5 .2MB/s Building wheel for ml-things ( setup.py ) ... done Building wheel for ftfy ( setup.py ) ... done","title":"Installs"},{"location":"tutorial_notebooks/tutorial_template_page/#imports","text":"Import all needed libraries for this notebook. Declare parameters used for this notebook: * * Code Cell:","title":"Imports"},{"location":"tutorial_notebooks/tutorial_template_page/#helper-functions","text":"Class() / function() Class / function description. Code Cell:","title":"Helper Functions"},{"location":"tutorial_notebooks/tutorial_template_page/#load-model-and-tokenizer","text":"Loading the three essential parts of the pretrained transformers: configuration , tokenizer and model . I also need to load the model on the device I'm planning to use (GPU / CPU). Code Cell: Output:","title":"Load Model and Tokenizer"},{"location":"tutorial_notebooks/tutorial_template_page/#dataset-and-dataloader","text":"Details. Code Cell: Output:","title":"Dataset and DataLoader"},{"location":"tutorial_notebooks/tutorial_template_page/#train","text":"Code Cell: Output: Use ColabImage plots straight in here","title":"Train"},{"location":"tutorial_notebooks/tutorial_template_page/#evaluate","text":"Evaluation! Code Cell: Output: Use ColabImage plots straight in here","title":"Evaluate"},{"location":"tutorial_notebooks/tutorial_template_page/#final-note","text":"If you made it this far Congrats and Thank you for your interest in my tutorial! Other details. If you see something wrong please let me know by opening an issue on my ml_things GitHub repository! A lot of tutorials out there are mostly a one-time thing and are not being maintained. I plan on keeping my tutorials up to date as much as I can.","title":"Final Note"},{"location":"tutorial_notebooks/tutorial_template_page/#contact","text":"GitHub: gmihaila Website: gmihaila.github.io LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila Thank you! Find out more About Me .","title":"Contact"},{"location":"useful/useful/","text":"Useful Code Here is where I put Python code snippets that I use in my Machine Learning research work. I'm using this page to have code easily accessible and to be able to share it with others. Tip : use Table of contents on the top-right side of the page to avoid endless scrolling, and is a good idea to use Copy to clipboard button on the upper right corner of each code cell to get things done quickly. Read FIle One liner to read any file: io . open ( \"my_file.txt\" , mode = 'r' , encoding = 'utf-8' ) . read () Details: import io Write File One liner to write a string to a file: io . open ( \"my_file.txt\" , mode = 'w' , encoding = 'utf-8' ) . write ( \"Your text!\" ) Details: import io Debug Start debugging after this line: import pdb ; pdb . set_trace () Details: use dir() to see all current variables, locals() to see variables and their values and globals() to see all global variables with values. Pip Install GitHub Install library directly from GitHub using pip: pip install git+github_url Details: add @version_number at the end to use a certain version to install. Parse Argument Parse arguments given when running a .py file. parser = argparse . ArgumentParser ( description = 'Description' ) parser . add_argument ( '--argument' , help = 'Help me.' , type = str ) # parse arguments args = parser . parse_args () Details: import argparse and use python script.py --argument something when running script. Doctest How to run a simple unittesc using function documentaiton. Useful when need to do unittest inside notebook: # function to test def add ( a , b ): ''' >>> add(2, 2) 5 ''' return a + b # run doctest import doctest doctest . testmod ( verbose = True ) Details: ml_things Fix Text I use this package everytime I read text data from a source I don't trust. Since text data is always messy, I always use it. It is great in fixing any bad Unicode. fix_text ( text = \"Text to be fixed\" ) Details: Install it pip install ftfy and import it from ftfy import fix_text . Current Date How to get current date in Python. I use this when need to name log files: from datetime import date today = date . today () # dd/mm/YY in string format today . strftime ( \" %d /%m/%Y\" ) Details: More details here Current Time Get current time in Python: from datetime import datetime # datetime object containing current date and time now = datetime . now () # dd/mm/YY H:M:S now . strftime ( \" %d /%m/%Y %H:%M:%S\" ) Details: More details here Remove Punctuation The fastest way to remove punctuation in Python3: table = str . maketrans ( dict . fromkeys ( string . punctuation )) \"string. With. Punctuation?\" . translate ( table ) Details: Import string . Code adapted from StackOverflow Remove punctuation from Unicode formatted strings . Class Instances from Dictionary Create class instances from dictionary. Very handy when working with notebooks and need to pass arguments as class instances. # Create dictionary of arguments. my_args = dict ( argument_one = 23 , argument_two = False ) # Convert dicitonary to class instances. my_args = type ( 'allMyArguments' , ( object ,), my_args ) Details: Code adapted from StackOverflow Creating class instance properties from a dictionary? List of Lists into Flat List Given a list of lists convert it to a single flat size list. It is the fasest way to conserve each elemnt type. l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ], [ 7 ], [ 8 , 9 ], [ 'this' , 'is' ]] functools . reduce ( operator . concat , l ) Details: Import operator, functools . Code adapted from StackOverflow How to make a flat list out of list of lists? Pickle and Unpickle Save python objects into binary using pickle. Load python objects from binary files using pickle. a = { 'hello' : 'world' } with open ( 'filename.pickle' , 'wb' ) as handle : pickle . dump ( a , handle , protocol = pickle . HIGHEST_PROTOCOL ) with open ( 'filename.pickle' , 'rb' ) as handle : b = pickle . load ( handle ) Details: Import pickle . Code adapted from StackOverflow How can I use pickle to save a dict? Notebook Input Variables How to ask user for input value to a variable. In the case of a password variable how to ask for a password variable. from getpass import getpass # Populate variables from user inputs. user = input ( 'User name: ' ) password = getpass ( 'Password: ' ) Details: Code adapted from StackOverflow Methods for using Git with Google Colab Notebook Private Repo Clone How to clone a private repo. Will need to login and ask for password. This snippet can be ran multiple times because it first check if the repo was cloned already. import os from getpass import getpass # Check if repo wasn't already cloned if not os . path . isdir ( '/content/github_repo' ): # Use GitHub username. u = 'github_username' # Ask user for GitHub password. p = getpass ( 'GitHub password: ' ) # Clone repo. ! git clone https : // $ u : $ p @github . com / github_username / github_repo . git # Remove password variable. p = '' Details: Code adapted from StackOverflow Methods for using Git with Google Colab Import Module Given Path How to import a module from a local path. Make it act as a installed library. import sys # Append module path. sys . path . append ( '/path/to/module' ) Details: After that we can use import module.stuff . Code adapted from StackOverflow Adding a path to sys.path (over using imp) . PyTorch Code snippets related to PyTorch : Dataset Code sample on how to create a PyTorch Dataset. The __len__(self) function needs to return the number of examples in your dataset and _getitem__(self,item) will use the index item to select an example from your dataset: from torch.utils.data import Dataset , DataLoader class PyTorchDataset ( Dataset ): \"\"\"PyTorch Dataset. \"\"\" def __init__ ( self ,): return def __len__ ( self ): return def __getitem__ ( self , item ): return # create pytorch dataset pytorch_dataset = PyTorchDataset () # move pytorch dataset into dataloader pytorch_dataloader = DataLoader ( pytorch_dataset , batch_size = 32 , shuffle = True ) Details: Find more details here PyTorch Device How to setup device in PyTorch to detect if GPU is available. If there is no GPU available it will default to CPU. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) Details: Adapted from Stack Overflow How to check if pytorch is using the GPU? .","title":"Useful Code"},{"location":"useful/useful/#useful-code","text":"Here is where I put Python code snippets that I use in my Machine Learning research work. I'm using this page to have code easily accessible and to be able to share it with others. Tip : use Table of contents on the top-right side of the page to avoid endless scrolling, and is a good idea to use Copy to clipboard button on the upper right corner of each code cell to get things done quickly.","title":"Useful Code"},{"location":"useful/useful/#read-file","text":"One liner to read any file: io . open ( \"my_file.txt\" , mode = 'r' , encoding = 'utf-8' ) . read () Details: import io","title":"Read FIle"},{"location":"useful/useful/#write-file","text":"One liner to write a string to a file: io . open ( \"my_file.txt\" , mode = 'w' , encoding = 'utf-8' ) . write ( \"Your text!\" ) Details: import io","title":"Write File"},{"location":"useful/useful/#debug","text":"Start debugging after this line: import pdb ; pdb . set_trace () Details: use dir() to see all current variables, locals() to see variables and their values and globals() to see all global variables with values.","title":"Debug"},{"location":"useful/useful/#pip-install-github","text":"Install library directly from GitHub using pip: pip install git+github_url Details: add @version_number at the end to use a certain version to install.","title":"Pip Install GitHub"},{"location":"useful/useful/#parse-argument","text":"Parse arguments given when running a .py file. parser = argparse . ArgumentParser ( description = 'Description' ) parser . add_argument ( '--argument' , help = 'Help me.' , type = str ) # parse arguments args = parser . parse_args () Details: import argparse and use python script.py --argument something when running script.","title":"Parse Argument"},{"location":"useful/useful/#doctest","text":"How to run a simple unittesc using function documentaiton. Useful when need to do unittest inside notebook: # function to test def add ( a , b ): ''' >>> add(2, 2) 5 ''' return a + b # run doctest import doctest doctest . testmod ( verbose = True ) Details: ml_things","title":"Doctest"},{"location":"useful/useful/#fix-text","text":"I use this package everytime I read text data from a source I don't trust. Since text data is always messy, I always use it. It is great in fixing any bad Unicode. fix_text ( text = \"Text to be fixed\" ) Details: Install it pip install ftfy and import it from ftfy import fix_text .","title":"Fix Text"},{"location":"useful/useful/#current-date","text":"How to get current date in Python. I use this when need to name log files: from datetime import date today = date . today () # dd/mm/YY in string format today . strftime ( \" %d /%m/%Y\" ) Details: More details here","title":"Current Date"},{"location":"useful/useful/#current-time","text":"Get current time in Python: from datetime import datetime # datetime object containing current date and time now = datetime . now () # dd/mm/YY H:M:S now . strftime ( \" %d /%m/%Y %H:%M:%S\" ) Details: More details here","title":"Current Time"},{"location":"useful/useful/#remove-punctuation","text":"The fastest way to remove punctuation in Python3: table = str . maketrans ( dict . fromkeys ( string . punctuation )) \"string. With. Punctuation?\" . translate ( table ) Details: Import string . Code adapted from StackOverflow Remove punctuation from Unicode formatted strings .","title":"Remove Punctuation"},{"location":"useful/useful/#class-instances-from-dictionary","text":"Create class instances from dictionary. Very handy when working with notebooks and need to pass arguments as class instances. # Create dictionary of arguments. my_args = dict ( argument_one = 23 , argument_two = False ) # Convert dicitonary to class instances. my_args = type ( 'allMyArguments' , ( object ,), my_args ) Details: Code adapted from StackOverflow Creating class instance properties from a dictionary?","title":"Class Instances from Dictionary"},{"location":"useful/useful/#list-of-lists-into-flat-list","text":"Given a list of lists convert it to a single flat size list. It is the fasest way to conserve each elemnt type. l = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ], [ 7 ], [ 8 , 9 ], [ 'this' , 'is' ]] functools . reduce ( operator . concat , l ) Details: Import operator, functools . Code adapted from StackOverflow How to make a flat list out of list of lists?","title":"List of Lists into Flat List"},{"location":"useful/useful/#pickle-and-unpickle","text":"Save python objects into binary using pickle. Load python objects from binary files using pickle. a = { 'hello' : 'world' } with open ( 'filename.pickle' , 'wb' ) as handle : pickle . dump ( a , handle , protocol = pickle . HIGHEST_PROTOCOL ) with open ( 'filename.pickle' , 'rb' ) as handle : b = pickle . load ( handle ) Details: Import pickle . Code adapted from StackOverflow How can I use pickle to save a dict?","title":"Pickle and Unpickle"},{"location":"useful/useful/#notebook-input-variables","text":"How to ask user for input value to a variable. In the case of a password variable how to ask for a password variable. from getpass import getpass # Populate variables from user inputs. user = input ( 'User name: ' ) password = getpass ( 'Password: ' ) Details: Code adapted from StackOverflow Methods for using Git with Google Colab","title":"Notebook Input Variables"},{"location":"useful/useful/#notebook-private-repo-clone","text":"How to clone a private repo. Will need to login and ask for password. This snippet can be ran multiple times because it first check if the repo was cloned already. import os from getpass import getpass # Check if repo wasn't already cloned if not os . path . isdir ( '/content/github_repo' ): # Use GitHub username. u = 'github_username' # Ask user for GitHub password. p = getpass ( 'GitHub password: ' ) # Clone repo. ! git clone https : // $ u : $ p @github . com / github_username / github_repo . git # Remove password variable. p = '' Details: Code adapted from StackOverflow Methods for using Git with Google Colab","title":"Notebook Private Repo Clone"},{"location":"useful/useful/#import-module-given-path","text":"How to import a module from a local path. Make it act as a installed library. import sys # Append module path. sys . path . append ( '/path/to/module' ) Details: After that we can use import module.stuff . Code adapted from StackOverflow Adding a path to sys.path (over using imp) .","title":"Import Module Given Path"},{"location":"useful/useful/#pytorch","text":"Code snippets related to PyTorch :","title":"PyTorch"},{"location":"useful/useful/#dataset","text":"Code sample on how to create a PyTorch Dataset. The __len__(self) function needs to return the number of examples in your dataset and _getitem__(self,item) will use the index item to select an example from your dataset: from torch.utils.data import Dataset , DataLoader class PyTorchDataset ( Dataset ): \"\"\"PyTorch Dataset. \"\"\" def __init__ ( self ,): return def __len__ ( self ): return def __getitem__ ( self , item ): return # create pytorch dataset pytorch_dataset = PyTorchDataset () # move pytorch dataset into dataloader pytorch_dataloader = DataLoader ( pytorch_dataset , batch_size = 32 , shuffle = True ) Details: Find more details here","title":"Dataset"},{"location":"useful/useful/#pytorch-device","text":"How to setup device in PyTorch to detect if GPU is available. If there is no GPU available it will default to CPU. device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) Details: Adapted from Stack Overflow How to check if pytorch is using the GPU? .","title":"PyTorch Device"}]}