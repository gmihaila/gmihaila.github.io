{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About me George is a PhD candidate at University of North Texas, in the Department of Computer Science. He received his Bachelors Degree in Electrical Engineering in his home-country, Romania, and he got his Masters Degree in Computer Science in the United States. He worked three summers for State Farm as a Data Scientist (DS) and Machine Learning (ML) Engineer, and he has more than 4 years of experience in Natural Language Processing (NLP), Computer Vision (CV), Deep Learning (DL) and Reinforcement Learning (RL). He is currently working towards his doctoral thesis in casual dialog generation with persona. Competencies: Data Science, parallel computing, Tensorflow2.0, PyTorch, Python, R, Java, Matlab. Current Position Data Scientist Intern State Farm | Enterprise Data & Analytics May 2020 \u2013 July 2020 Artificial Intelligence \u2013 Machine Learning Researcher University of North Texas (UNT) | Department of Computer Science Jan 2017 - Present Reading How Not to Be Wrong: The Power of Mathematical Thinking by Jordan Ellenberg Contact GitHub: gmihaila LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila","title":"About"},{"location":"#about-me","text":"George is a PhD candidate at University of North Texas, in the Department of Computer Science. He received his Bachelors Degree in Electrical Engineering in his home-country, Romania, and he got his Masters Degree in Computer Science in the United States. He worked three summers for State Farm as a Data Scientist (DS) and Machine Learning (ML) Engineer, and he has more than 4 years of experience in Natural Language Processing (NLP), Computer Vision (CV), Deep Learning (DL) and Reinforcement Learning (RL). He is currently working towards his doctoral thesis in casual dialog generation with persona. Competencies: Data Science, parallel computing, Tensorflow2.0, PyTorch, Python, R, Java, Matlab.","title":"About me"},{"location":"#current-position","text":"","title":"Current Position"},{"location":"#data-scientist-intern","text":"State Farm | Enterprise Data & Analytics May 2020 \u2013 July 2020","title":"Data Scientist Intern"},{"location":"#artificial-intelligence--machine-learning-researcher","text":"University of North Texas (UNT) | Department of Computer Science Jan 2017 - Present","title":"Artificial Intelligence \u2013 Machine Learning Researcher"},{"location":"#reading","text":"How Not to Be Wrong: The Power of Mathematical Thinking by Jordan Ellenberg","title":"Reading"},{"location":"#contact","text":"GitHub: gmihaila LinkedIn: mihailageorge Email: georgemihaila@my.unt.edu.com Schedule meeting: calendly.com/georgemihaila","title":"Contact"},{"location":"stuff/","text":"Welcome to Talon Hight-Performance Computing Documentation BTW this is mobile friendly and easy to deploy on GitHub Pages http://UNT-RITS.github.io/talon_sop/_build/html/index.html Task List Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Bundle code together Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } } Python Starter Content import tensorflow as tf import os print ( \"Hello world\" ) import tensorflow as tf import os print ( \"Hello world\" ) Formulas \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons \u2013 that's not all, we can also use GitHub's Octicons Foot Note Lorem ipsum 1 dolor sit amet R Starter Content Job Submit Content Tables Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Welcome to Talon Hight-Performance Computing Documentation"},{"location":"stuff/#welcome-to-talon-hight-performance-computing-documentation","text":"BTW this is mobile friendly and easy to deploy on GitHub Pages http://UNT-RITS.github.io/talon_sop/_build/html/index.html","title":"Welcome to Talon Hight-Performance Computing Documentation"},{"location":"stuff/#task-list","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Task List"},{"location":"stuff/#bundle-code-together","text":"Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello world!\" ); } }","title":"Bundle code together"},{"location":"stuff/#python-starter","text":"","title":"Python Starter"},{"location":"stuff/#content","text":"import tensorflow as tf import os print ( \"Hello world\" ) import tensorflow as tf import os print ( \"Hello world\" )","title":"Content"},{"location":"stuff/#formulas","text":"\\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \u2013 we can use Material Design icons \u2013 we can also use FontAwesome icons \u2013 that's not all, we can also use GitHub's Octicons","title":"Formulas"},{"location":"stuff/#foot-note","text":"Lorem ipsum 1 dolor sit amet","title":"Foot Note"},{"location":"stuff/#r-starter","text":"","title":"R Starter"},{"location":"stuff/#content_1","text":"","title":"Content"},{"location":"stuff/#job-submit","text":"","title":"Job Submit"},{"location":"stuff/#content_2","text":"","title":"Content"},{"location":"stuff/#tables","text":"Fruit List Apple Banana Kiwi Fruit Table Fruit Color Apple Red Banana Yellow Kiwi Green Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Tables"},{"location":"activities/activities/","text":"Activities GPU Technology Conference (GTC) Code | Slides march 20-22, 2020 | San Jose California GPUs in Natural Language Processing Invited to participate in conference Instructor-Led Training at the GPU Technology Conference (GTC). Title: T22128: GPUs in Natural Language Processing Session Type: Instructor-Led Training Duration: 1 Hour 45 Minutes Find New Sentiments in Text Data Code | Slides march 3, 2020 | University of North Texas DP F223 Using text embedding and clustering methods to find new sentiments in text data. Use imdb movie reviews dataset and see how positive-negative sentiments can be used to find other sentiments. Use Tensorflow 2, Word Embeddings, Contextual Embeddings. We'll do clustering and find best number of cluster in data we don't know anything about. We'll do pretty plots to interpret results! We'll do all this on our UNT Talon Supercomputer! Duration: 1 Hour 15 Minutes Intro to using Git Lab Info | Slides may 3, 2019 | University of North Texas DP F285 How to use UNT HPC GitLab server Ever wondered what is Git and why every software company uses it? We'll teach some basic commands in Git Lab (platform used in almost every software company). And Yes, UNT has a Git Lab! Duration: 1 Hour 15 Minutes Intro to Word Embeddings - NLP Tools on Talon Info | Slides may 2, 2019 | University of North Texas DP F223 Natural Language Processing on Talon HPC Ever wondered or curious about natural language processing? How do you load a text file, how do you 'clean' it, how do you make use of it? We will learn all that and at the end you will build a Neural Network model that can predict a sentiment of a Movie Review. All this on Talon! Duration: 1 Hour 15 Minutes Using Python and Jupyter Notebooks on Talon Info | Slides may 1, 2019 | University of North Texas DP F223 Hands on work on Talon UNT HPC This workshop will go over some basic instruction about Python and Jupyter Notebooks. We will learn how to see what libraries we have available and how to install your own.Learn how to create a virtual environment, and most exciting build a simple Neural Network model to predict diabetes! All this on Talon! Duration: 1 Hour 15 Minutes Machine Learning - Neural Networks on Talon Info | Slides november 30, 2018 | University of North Texas GAB 535 What are Neural Networks, and why everyone is talking about them? This workshop is meant to give a general overview of why Neural Networks and Deep Learning are getting so much attention and help shed some light on how they work and what applications are possible because of them. Duration: 1 Hour 15 Minutes","title":"Activities"},{"location":"activities/activities/#activities","text":"","title":"Activities"},{"location":"activities/activities/#gpu-technology-conference-gtc","text":"Code | Slides march 20-22, 2020 | San Jose California GPUs in Natural Language Processing Invited to participate in conference Instructor-Led Training at the GPU Technology Conference (GTC). Title: T22128: GPUs in Natural Language Processing Session Type: Instructor-Led Training Duration: 1 Hour 45 Minutes","title":"GPU Technology Conference (GTC)"},{"location":"activities/activities/#find-new-sentiments-in-text-data","text":"Code | Slides march 3, 2020 | University of North Texas DP F223 Using text embedding and clustering methods to find new sentiments in text data. Use imdb movie reviews dataset and see how positive-negative sentiments can be used to find other sentiments. Use Tensorflow 2, Word Embeddings, Contextual Embeddings. We'll do clustering and find best number of cluster in data we don't know anything about. We'll do pretty plots to interpret results! We'll do all this on our UNT Talon Supercomputer! Duration: 1 Hour 15 Minutes","title":"Find New Sentiments in Text Data"},{"location":"activities/activities/#intro-to-using-git-lab","text":"Info | Slides may 3, 2019 | University of North Texas DP F285 How to use UNT HPC GitLab server Ever wondered what is Git and why every software company uses it? We'll teach some basic commands in Git Lab (platform used in almost every software company). And Yes, UNT has a Git Lab! Duration: 1 Hour 15 Minutes","title":"Intro to using Git Lab"},{"location":"activities/activities/#intro-to-word-embeddings---nlp-tools-on-talon","text":"Info | Slides may 2, 2019 | University of North Texas DP F223 Natural Language Processing on Talon HPC Ever wondered or curious about natural language processing? How do you load a text file, how do you 'clean' it, how do you make use of it? We will learn all that and at the end you will build a Neural Network model that can predict a sentiment of a Movie Review. All this on Talon! Duration: 1 Hour 15 Minutes","title":"Intro to Word Embeddings - NLP Tools on Talon"},{"location":"activities/activities/#using-python-and-jupyter-notebooks-on-talon","text":"Info | Slides may 1, 2019 | University of North Texas DP F223 Hands on work on Talon UNT HPC This workshop will go over some basic instruction about Python and Jupyter Notebooks. We will learn how to see what libraries we have available and how to install your own.Learn how to create a virtual environment, and most exciting build a simple Neural Network model to predict diabetes! All this on Talon! Duration: 1 Hour 15 Minutes","title":"Using Python and Jupyter Notebooks on Talon"},{"location":"activities/activities/#machine-learning---neural-networks-on-talon","text":"Info | Slides november 30, 2018 | University of North Texas GAB 535 What are Neural Networks, and why everyone is talking about them? This workshop is meant to give a general overview of why Neural Networks and Deep Learning are getting so much attention and help shed some light on how they work and what applications are possible because of them. Duration: 1 Hour 15 Minutes","title":"Machine Learning - Neural Networks on Talon"},{"location":"resume/resume/","text":"Resume pdf Summary Interest areas: Reinforcement Learning, Computer Vision, Big Data, Scaling Machine Learning, Parallel Processing. Main Research areas: Deep Learning, Natural Language Processing Thesis project: Casual Dialog Generation with different personas . For example, imagine a chatbot from the TV Series Friends that talks like your favorite character (Joey, Chandler etc.) Key Assets: Being a PhD student set me up to work with little guidance and being self-taught . How I like to takle problems: always start small and with good understating of the problem, build something simple that works and keep adding and testing on that until reaching the goal. Experience Data Scientist Intern State Farm | Enterprise Data & Analytics Align machine learning and artificial inteligence research to bussiness needs. Created a Reinforcement Learning (RL) proof of concept application. Built a framework around RL that makes it expandable into other business areas. Created custom data environments to easily train and deploy RL models. May 2020 \u2013 July 2020 Data Scientist \u2013 Machine Learning Engineer University of North Texas (UNT) | Research Information Technology Supported research project data science and Machine Learning (ML) needs. Maintain latest ML frameworks for UNT\u2019s High Performance Computing (HPC). Created workshops and tutorials about cloud computing HPC services. Sep 2018 - May 2020 Machine Learning Engineer Intern State Farm | Enterprise Data & Analytics Automated model risk validation reducing deployment timeline by 90%. Deployed Python package for automated model building and data analysis. Built a user interface for automated model comparison with CI/CD pipeline. May 2019 \u2013 July 2019 Data Scientist Intern State Farm | Enterprise &Data Analytics Introduced GPUs in ML tutorials that speed up training by over 50%. Increased model performance used in claims process by 5% using text data. Built Deep Learning Optical Character Recognition with over 75% accuracy. May 2018 \u2013 August 2018 Teaching Assistant \u2013 Computer Science University of North Texas (UNT) | Department of Computer Science Built and debugged C/C++ coding assignments and exams. Assisted Computer Science students as a peer mentor and grader. Sep 2017 \u2013 May 2018 Artificial Intelligence \u2013 Machine Learning Researcher University of North Texas (UNT) | Department of Computer Science Built state of the art Deep Learning models in Natural Language Processing. Implement concepts from research papers related to state of the art chatbots. Jan 2017 - Present Education PhD in Computer Science University of North Texas (UNT) | Department of Computer Science Focus on Natural Language Processing, Deep Generative Modeling, Companionable Robots, Persona Detection. May 2019 - Present | GPA 4.0 Masters in Computer Science University of North Texas (UNT) | Department of Computer Science Specialized in Machine Learning, Natural Language Processing, Big Data, Data Mining, Computer Vision and Deep Reinforcement Learning. May 2019 | GPA 3.9 Skills Reference Dr. Rodney D. Nielsen Associate Professor - Director HiLT Lab University of North Texas \u2013 Computer Science Engineer Email: Rodney.Nielsen@UNT.edu","title":"Resume"},{"location":"resume/resume/#resume--pdf","text":"","title":"Resume  pdf"},{"location":"resume/resume/#summary","text":"Interest areas: Reinforcement Learning, Computer Vision, Big Data, Scaling Machine Learning, Parallel Processing. Main Research areas: Deep Learning, Natural Language Processing Thesis project: Casual Dialog Generation with different personas . For example, imagine a chatbot from the TV Series Friends that talks like your favorite character (Joey, Chandler etc.) Key Assets: Being a PhD student set me up to work with little guidance and being self-taught . How I like to takle problems: always start small and with good understating of the problem, build something simple that works and keep adding and testing on that until reaching the goal.","title":"Summary"},{"location":"resume/resume/#experience","text":"","title":"Experience"},{"location":"resume/resume/#data-scientist-intern","text":"State Farm | Enterprise Data & Analytics Align machine learning and artificial inteligence research to bussiness needs. Created a Reinforcement Learning (RL) proof of concept application. Built a framework around RL that makes it expandable into other business areas. Created custom data environments to easily train and deploy RL models. May 2020 \u2013 July 2020","title":"Data Scientist Intern"},{"location":"resume/resume/#data-scientist--machine-learning-engineer","text":"University of North Texas (UNT) | Research Information Technology Supported research project data science and Machine Learning (ML) needs. Maintain latest ML frameworks for UNT\u2019s High Performance Computing (HPC). Created workshops and tutorials about cloud computing HPC services. Sep 2018 - May 2020","title":"Data Scientist \u2013 Machine Learning Engineer"},{"location":"resume/resume/#machine-learning-engineer-intern","text":"State Farm | Enterprise Data & Analytics Automated model risk validation reducing deployment timeline by 90%. Deployed Python package for automated model building and data analysis. Built a user interface for automated model comparison with CI/CD pipeline. May 2019 \u2013 July 2019","title":"Machine Learning Engineer Intern"},{"location":"resume/resume/#data-scientist-intern_1","text":"State Farm | Enterprise &Data Analytics Introduced GPUs in ML tutorials that speed up training by over 50%. Increased model performance used in claims process by 5% using text data. Built Deep Learning Optical Character Recognition with over 75% accuracy. May 2018 \u2013 August 2018","title":"Data Scientist Intern"},{"location":"resume/resume/#teaching-assistant--computer-science","text":"University of North Texas (UNT) | Department of Computer Science Built and debugged C/C++ coding assignments and exams. Assisted Computer Science students as a peer mentor and grader. Sep 2017 \u2013 May 2018","title":"Teaching Assistant \u2013 Computer Science"},{"location":"resume/resume/#artificial-intelligence--machine-learning-researcher","text":"University of North Texas (UNT) | Department of Computer Science Built state of the art Deep Learning models in Natural Language Processing. Implement concepts from research papers related to state of the art chatbots. Jan 2017 - Present","title":"Artificial Intelligence \u2013 Machine Learning Researcher"},{"location":"resume/resume/#education","text":"","title":"Education"},{"location":"resume/resume/#phd-in-computer-science","text":"University of North Texas (UNT) | Department of Computer Science Focus on Natural Language Processing, Deep Generative Modeling, Companionable Robots, Persona Detection. May 2019 - Present | GPA 4.0","title":"PhD in Computer Science"},{"location":"resume/resume/#masters-in-computer-science","text":"University of North Texas (UNT) | Department of Computer Science Specialized in Machine Learning, Natural Language Processing, Big Data, Data Mining, Computer Vision and Deep Reinforcement Learning. May 2019 | GPA 3.9","title":"Masters in Computer Science"},{"location":"resume/resume/#skills","text":"","title":"Skills"},{"location":"resume/resume/#reference","text":"","title":"Reference"},{"location":"resume/resume/#dr-rodney-d-nielsen","text":"Associate Professor - Director HiLT Lab University of North Texas \u2013 Computer Science Engineer Email: Rodney.Nielsen@UNT.edu","title":"Dr. Rodney D. Nielsen"},{"location":"tutorial_notebooks/pretrain_transformer/","text":"Pretrain Transformers Info This notebook is used to pretrain transformers models using Huggingface . This notebooks is part of my trusty notebooks for Machine Learning. Check out more similar content on my website gmihaila.github.io/useful/useful/ where I post useful notebooks like this one. This notebook is heavily inspired from the Huggingface script used for training language models: transformers/tree/master/examples/language-modeling . 'Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.' How to use this notebook? This notebooks is a code adaptation of the run_language_modeling.py . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . Parse the arguments needed that are split in TrainingArguments, ModelArguments and DataTrainingArguments. The only variables that need configuration depending on your needs are model_args , data_args and training_args in Parameters : model_args of type ModelArguments : These are the arguments for the model that you want to use such as the model_name_or_path, tokenizer_name etc. You'll need these to load the model and tokenizer. Minimum setup: model_args = ModelArguments ( model_name_or_path , model_type , tokenizer_name , ) model_name_or_path path to existing transformers model or name of transformer model to be used: bert-base-cased , roberta-base , gpt2 etc. More details here . model_type type of model used: bert , roberta , gpt2 . More details here . tokenizer_name tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased , roberta-base , gpt2 etc. data_args of type DataTrainingArguments : These are as the name suggests arguments needed for the dataset. Such as the directory name where your files are stored etc. You'll need these to load/process the dataset. Minimum setup: data_args = DataArgs ( train_data_file , eval_data_file , mlm , ) train_data_file path to your dataset. This is a plain file that contains all your text data to train a model. Use each line to separate examples: i.e. if you have a dataset composed of multiple text documents, create a single file with each line in the file associated to a text document. eval_data_file same story as train_data_file . This file is used to evaluate the model performance mlm is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta . training_args of type TrainingArguments : These are the training hyper-parameters such as learning rate, batch size, weight decay, gradient accumulation steps etc. See all possible arguments here . These are used by the Trainer. Minimum setup: model_args training_args = TrainingArguments ( output_dir , do_train , do_eval , ) output_dir path where to save the pre-trained model. do_train variable to signal if you're using train data or not. Set it to True if you mentioned train_data_file . do_eval variable to signal if you're using evaluate data or not. Set it to True if you mentioned eval_data_file . Example: Pre-train Bert In the Parameters section use arguments: # process model arguments. Check Info - Notes for more details model_args = ModelArguments ( model_name_or_path = 'bert-base-cased' , model_type = 'bert' , tokenizer_name = 'bert-base-cased' , ) # process data arguments. Check Info - Notes for more details data_args = DataArgs ( train_data_file = '/content/your_train_data' , eval_data_file = '/content/your_test_data, mlm = True , ) # process training arguments. Check Info - Notes for more details training_args = TrainingArguments ( output_dir = '/content/pretrained_bert' , do_train = True , do_eval = False ) Notes: Parameters details got from here . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . I plan on testing more models in the future. I used the The WikiText Long Term Dependency Language Modeling Dataset as an example. To reduce training time I used the evaluate split as training and test split as evaluation! .","title":"Pretrain Transformers"},{"location":"tutorial_notebooks/pretrain_transformer/#pretrain-transformers","text":"","title":"Pretrain Transformers"},{"location":"tutorial_notebooks/pretrain_transformer/#info","text":"This notebook is used to pretrain transformers models using Huggingface . This notebooks is part of my trusty notebooks for Machine Learning. Check out more similar content on my website gmihaila.github.io/useful/useful/ where I post useful notebooks like this one. This notebook is heavily inspired from the Huggingface script used for training language models: transformers/tree/master/examples/language-modeling . 'Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet). GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.'","title":"Info"},{"location":"tutorial_notebooks/pretrain_transformer/#how-to-use-this-notebook","text":"This notebooks is a code adaptation of the run_language_modeling.py . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . Parse the arguments needed that are split in TrainingArguments, ModelArguments and DataTrainingArguments. The only variables that need configuration depending on your needs are model_args , data_args and training_args in Parameters : model_args of type ModelArguments : These are the arguments for the model that you want to use such as the model_name_or_path, tokenizer_name etc. You'll need these to load the model and tokenizer. Minimum setup: model_args = ModelArguments ( model_name_or_path , model_type , tokenizer_name , ) model_name_or_path path to existing transformers model or name of transformer model to be used: bert-base-cased , roberta-base , gpt2 etc. More details here . model_type type of model used: bert , roberta , gpt2 . More details here . tokenizer_name tokenizer used to process data for training the model. It usually has same name as model_name_or_path : bert-base-cased , roberta-base , gpt2 etc. data_args of type DataTrainingArguments : These are as the name suggests arguments needed for the dataset. Such as the directory name where your files are stored etc. You'll need these to load/process the dataset. Minimum setup: data_args = DataArgs ( train_data_file , eval_data_file , mlm , ) train_data_file path to your dataset. This is a plain file that contains all your text data to train a model. Use each line to separate examples: i.e. if you have a dataset composed of multiple text documents, create a single file with each line in the file associated to a text document. eval_data_file same story as train_data_file . This file is used to evaluate the model performance mlm is a flag that changes loss function depending on model architecture. This variable needs to be set to True when working with masked language models like bert or roberta . training_args of type TrainingArguments : These are the training hyper-parameters such as learning rate, batch size, weight decay, gradient accumulation steps etc. See all possible arguments here . These are used by the Trainer. Minimum setup: model_args training_args = TrainingArguments ( output_dir , do_train , do_eval , ) output_dir path where to save the pre-trained model. do_train variable to signal if you're using train data or not. Set it to True if you mentioned train_data_file . do_eval variable to signal if you're using evaluate data or not. Set it to True if you mentioned eval_data_file .","title":"How to use this notebook?"},{"location":"tutorial_notebooks/pretrain_transformer/#example","text":"","title":"Example:"},{"location":"tutorial_notebooks/pretrain_transformer/#pre-train-bert","text":"In the Parameters section use arguments: # process model arguments. Check Info - Notes for more details model_args = ModelArguments ( model_name_or_path = 'bert-base-cased' , model_type = 'bert' , tokenizer_name = 'bert-base-cased' , ) # process data arguments. Check Info - Notes for more details data_args = DataArgs ( train_data_file = '/content/your_train_data' , eval_data_file = '/content/your_test_data, mlm = True , ) # process training arguments. Check Info - Notes for more details training_args = TrainingArguments ( output_dir = '/content/pretrained_bert' , do_train = True , do_eval = False )","title":"Pre-train Bert"},{"location":"tutorial_notebooks/pretrain_transformer/#notes","text":"Parameters details got from here . Models that are guarantee to work: GPT , GPT-2 , BERT , DistilBERT , RoBERTa and XLNet . I plan on testing more models in the future. I used the The WikiText Long Term Dependency Language Modeling Dataset as an example. To reduce training time I used the evaluate split as training and test split as evaluation! .","title":"Notes:"},{"location":"useful/useful/","text":"Useful Code This is where I will put frequently used code of machine learning and python. Disclaimer: These are things I find useful for me in my day to day work with python and machine learning. These materials are not designed to be in any kind of specific order. I will try to order them by the frquency that I use some of them. Use: Table of contents on the top-right side of the page to be snapy. Copy to clipboard button on the upper right coerner of each code cell. Python argparse Sample: import argparse # create arguments parser = argparse . ArgumentParser ( description = 'Details' ) parser . add_argument ( '--some_argument' , help = 'Description' , type = str , default = 'value' ) # parse arguments args = parser . parse_args () [ print ( \"-- %s %s \" % ( arg , value )) for arg , value in args . __dict__ . items ()] machine_learning_things/master/arguments Quick file handling Sample: import io # Write to `my_file.txt` io . open ( \"my_file.txt\" , mode = 'w' , encoding = 'utf-8' ) . write ( \"Hello!\" )) # Read from `my_file.txt` io . open ( \"my_file.txt\" , mode = 'r' , encoding = 'utf-8' ) . read () Details: machine_learning_things/master/file_handling \u2026 Workign to add more things \u2026 Sample: Details: machine_learning_things/ Machine Learning \u2026 Workign to add more things \u2026 Sample: Details: machine_learning_things/","title":"Useful Code"},{"location":"useful/useful/#useful-code","text":"This is where I will put frequently used code of machine learning and python. Disclaimer: These are things I find useful for me in my day to day work with python and machine learning. These materials are not designed to be in any kind of specific order. I will try to order them by the frquency that I use some of them. Use: Table of contents on the top-right side of the page to be snapy. Copy to clipboard button on the upper right coerner of each code cell.","title":"Useful Code"},{"location":"useful/useful/#python","text":"","title":"Python"},{"location":"useful/useful/#argparse","text":"Sample: import argparse # create arguments parser = argparse . ArgumentParser ( description = 'Details' ) parser . add_argument ( '--some_argument' , help = 'Description' , type = str , default = 'value' ) # parse arguments args = parser . parse_args () [ print ( \"-- %s %s \" % ( arg , value )) for arg , value in args . __dict__ . items ()] machine_learning_things/master/arguments","title":"argparse"},{"location":"useful/useful/#quick-file-handling","text":"Sample: import io # Write to `my_file.txt` io . open ( \"my_file.txt\" , mode = 'w' , encoding = 'utf-8' ) . write ( \"Hello!\" )) # Read from `my_file.txt` io . open ( \"my_file.txt\" , mode = 'r' , encoding = 'utf-8' ) . read () Details: machine_learning_things/master/file_handling","title":"Quick file handling"},{"location":"useful/useful/#-workign-to-add-more-things-","text":"","title":"... Workign to add more things ..."},{"location":"useful/useful/#_1","text":"Sample: Details: machine_learning_things/","title":""},{"location":"useful/useful/#machine-learning","text":"","title":"Machine Learning"},{"location":"useful/useful/#-workign-to-add-more-things-_1","text":"","title":"... Workign to add more things ..."},{"location":"useful/useful/#_2","text":"Sample: Details: machine_learning_things/","title":""}]}