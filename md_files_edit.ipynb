{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "md_files_edit.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/gmihaila/gmihaila.github.io/blob/master/md_files_edit.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "KSrs7s27-Gv7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use to edit/create .md files"
      ]
    },
    {
      "metadata": {
        "id": "gTRt-sDJ-Lmq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "gx1hDQHjs0Mn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Meeting 07/14/2018 Goals:\n",
        "\n",
        "\n",
        "\n",
        "*   Re-code **vocabulary_embedding.ipynb** \n",
        " * The purpose should be to parse, clean all text, and create embedding matrix and Tokenizer\n",
        "  * what is usefull from the original code is to fit OOV words in embeddings - maybe need more info on how to do this or see if it's worth it. If too complicated, we can skip for now.\n",
        "  \n",
        "*   Recode **train.ipynb**\n",
        " * re-build model (is pretty easy)\n",
        " * Data Generator and how the data is fed in and all the functions to flip articles etc.\n",
        " * figure out what beam search does and why we need it\n",
        " \n",
        " * Understand the Encoder-Decoder Schematics:\n",
        " ![](https://image.slidesharecdn.com/urhmqcurwy7dgxmomwua-signature-0bea2a94f96cf5a9f536918d6ef0707d749c8f2c3c6861466fa273a7271fe4a0-poli-161114204536/95/anjuli-kannan-software-engineer-google-at-mlconf-sf-2016-26-638.jpg?cb=1479158542)\n",
        "This is how the modle looks like for this code.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "sM4SjfzyedAj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Universal Text Embedding\n",
        "\n",
        "\n",
        "\n",
        "### Deep Averaging Network Function Description\n",
        "![alt text](https://i0.wp.com/mlexplained.com/wp-content/uploads/2018/05/スクリーンショット-2018-05-10-13.29.52.png?resize=502%2C358)\n",
        "\n",
        "* ### Input:\n",
        "\n",
        "  **text**      : clean text\n",
        "  \n",
        "  **emb_size**  : length of embedding vector\n",
        "  \n",
        "  **emb_model** : Gensim embedding model dictionary type map word-embedding\n",
        "\n",
        "\n",
        "* ### Output:\n",
        "  **emb_dan**   : averaged embedding of length emb_size\n",
        "\n",
        "```python\n",
        "def Dan(text, emb_size, emb_model):\n",
        "    emb_dan = np.zeros(emb_size, dtype=float)\n",
        "    num_words = 0\n",
        "    for word in text.split():\n",
        "        if word in emb_model.vocab:\n",
        "            # add each embedding\n",
        "            emb_dan += np.array(emb_model[word], dtype=float)\n",
        "            num_words += 1\n",
        "    # average embedding\n",
        "    emb_dan = emb_dan/num_words\n",
        "    return emb_dan\n",
        "```\n",
        "\n",
        "### Universal Sentence Embedding\n",
        "![alt text](https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/images/example-similarity.png)\n",
        "\n",
        "\n",
        "*   ### hub module path if used --image \"tensorflow-18.04.0\" in MLP  \n",
        "     \n",
        "\n",
        "```python\n",
        "  path_hub = '/data/shared/embedding_tools/tf_module'\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "*   ### hub module path if used --image \"python-3.6.4\" in MLP  \n",
        "\n",
        "\n",
        "```python\n",
        "  path_hub = '/data/universal_text_embedding/tf_hub'\n",
        "```\n",
        "\n",
        "* ### load pre-trained model\n",
        "```python\n",
        "embed = hub.Module(path_hub)\n",
        "```\n",
        "\n",
        "* ### keras layer\n",
        "```python\n",
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)),\n",
        "                      signature=\"default\", as_dict=True)[\"default\"]\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qZMHYEcj2Nt1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}